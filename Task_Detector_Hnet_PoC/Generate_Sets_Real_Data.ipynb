{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import sys\n",
    "# Imports from other modules and packages in the project\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.helpers import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Chewie'\n",
    "date = '1007'\n",
    "fold = 4\n",
    "target_variable = 'vel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Helper functions for plotting (run this cell!)\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "# initialize a color palette for plotting\n",
    "palette = sns.xkcd_palette([\"windows blue\",\n",
    "                            \"red\",\n",
    "                            \"medium green\",\n",
    "                            \"dusty purple\",\n",
    "                            \"orange\",\n",
    "                            \"amber\",\n",
    "                            \"clay\",\n",
    "                            \"pink\",\n",
    "                            \"greyish\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../Data/Processed_Data/Tidy_'+name+'_'+date+'.pkl'\n",
    "\n",
    "with open(data_path, 'rb') as file:\n",
    "    tidy_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Force Adaptation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train trials 36\n",
      "Test trials  11\n",
      "Val trials 9\n",
      "We are testing the optimization method on fold  4\n",
      "Train trials 36\n",
      "Test trials  11\n",
      "Val trials 9\n",
      "We are testing the optimization method on fold  4\n",
      "Train trials 36\n",
      "Test trials  11\n",
      "Val trials 9\n",
      "We are testing the optimization method on fold  4\n",
      "Train trials 43\n",
      "Test trials  13\n",
      "Val trials 11\n",
      "We are testing the optimization method on fold  4\n",
      "Train trials 43\n",
      "Test trials  13\n",
      "Val trials 11\n",
      "We are testing the optimization method on fold  4\n",
      "Train trials 43\n",
      "Test trials  13\n",
      "Val trials 11\n",
      "We are testing the optimization method on fold  4\n",
      "Train trials 45\n",
      "Test trials  14\n",
      "Val trials 11\n",
      "We are testing the optimization method on fold  4\n",
      "Train trials 45\n",
      "Test trials  14\n",
      "Val trials 11\n",
      "We are testing the optimization method on fold  4\n",
      "Train trials 45\n",
      "Test trials  14\n",
      "Val trials 11\n",
      "We are testing the optimization method on fold  4\n"
     ]
    }
   ],
   "source": [
    "# Create filtered DataFrames\n",
    "baseline_df = tidy_df.loc[tidy_df['epoch'] == 'BL']\n",
    "force_df = tidy_df.loc[tidy_df['epoch'] == 'AD']\n",
    "wo_df = tidy_df.loc[tidy_df['epoch'] == 'WO']\n",
    "\n",
    "# Function to split IDs into 3 equal sets\n",
    "def split_ids(df):\n",
    "    unique_ids = df.id.unique()\n",
    "    set_length = len(unique_ids) // 3\n",
    "    return [unique_ids[i * set_length:(i + 1) * set_length] for i in range(3)]\n",
    "\n",
    "# Split IDs for each epoch\n",
    "ids_base = split_ids(baseline_df)\n",
    "ids_force = split_ids(force_df)\n",
    "ids_wo = split_ids(wo_df)\n",
    "\n",
    "# Function to create dataset dictionary entries\n",
    "def create_dataset_entry(df, ids, name_prefix):\n",
    "    return {\n",
    "        f'{name_prefix}{i+1}': get_dataset(df.loc[df.id.isin(ids[i])], fold, target_variable='vel', no_outliers=False, force_data=True)[:6]\n",
    "        for i in range(3)\n",
    "    }\n",
    "\n",
    "# Create datasets dictionary\n",
    "datasets_adaptation = {}\n",
    "datasets_adaptation.update(create_dataset_entry(baseline_df, ids_base, 'Baseline'))\n",
    "datasets_adaptation.update(create_dataset_entry(force_df, ids_force, 'Force'))\n",
    "datasets_adaptation.update(create_dataset_entry(wo_df, ids_wo, 'Washout'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = list(datasets_adaptation.keys())\n",
    "random.shuffle(dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Force3',\n",
       " 'Washout1',\n",
       " 'Baseline2',\n",
       " 'Washout2',\n",
       " 'Baseline1',\n",
       " 'Washout3',\n",
       " 'Force1',\n",
       " 'Force2',\n",
       " 'Baseline3']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_adaptation_v3 = ['Baseline1',\n",
    "                       'Baseline2',\n",
    "                       'Force1',\n",
    "                       'Force2',\n",
    "                       'Washout1',\n",
    "                       'Washout2',\n",
    "                       'Baseline3',\n",
    "                        'Force3',\n",
    "                        'Washout3',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder dictionary\n",
    "ordered_datasets_adaptation = {key: datasets_adaptation[key] for key in order_adaptation_v3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './Data'\n",
    "path_to_save_data = os.path.join(data_dir, 'Real_Data_'+'Adaptation_v2'+'.pkl')\n",
    "\n",
    "# Pickle the data and save it to file\n",
    "with open(path_to_save_data, 'wb') as handle:\n",
    "    pickle.dump(ordered_datasets_adaptation, handle, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Stimulation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../Data/Processed_Data/Tidy_Sansa_0504.pkl'\n",
    "\n",
    "with open(data_path, 'rb') as file:\n",
    "    df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df_s = df.loc[df.type == 'BASELINE'].reset_index()\n",
    "tonic_df_s = df.loc[df.type == 'TONIC'].reset_index()\n",
    "stim_df_s = df.loc[df.type == 'BC-REACH'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trials for baseline : 29\n",
      "Total number of trials for tonic stimulation : 35\n",
      "Total number of trials for triggered stimulation : 69\n"
     ]
    }
   ],
   "source": [
    "print('Total number of trials for baseline :', baseline_df_s.id.nunique())\n",
    "print('Total number of trials for tonic stimulation :', tonic_df_s.id.nunique())\n",
    "print('Total number of trials for triggered stimulation :', stim_df_s.id.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train trials 18\n",
      "Test trials  6\n",
      "Val trials 5\n",
      "We are testing the optimization method on fold  4\n",
      "Train trials 22\n",
      "Test trials  7\n",
      "Val trials 6\n",
      "We are testing the optimization method on fold  4\n",
      "Train trials 29\n",
      "Test trials  9\n",
      "Val trials 7\n",
      "We are testing the optimization method on fold  4\n",
      "Train trials 15\n",
      "Test trials  5\n",
      "Val trials 4\n",
      "We are testing the optimization method on fold  4\n"
     ]
    }
   ],
   "source": [
    "# Function to create dataset dictionary entries\n",
    "def create_dataset_entry_stim(df, ids, name_prefix, num_sets):\n",
    "    return {\n",
    "        f'{name_prefix}{i+1}': get_dataset(df.loc[df.id.isin(ids[i])], fold, target_variable='target_vel', no_outliers=False, force_data=False)[:6]\n",
    "        for i in range(num_sets)\n",
    "    }\n",
    "\n",
    "\n",
    "ids_base_s = [baseline_df_s.id.unique()]\n",
    "ids_tonic_s = [tonic_df_s.id.unique()]\n",
    "shuffled_ids = stim_df_s.id.unique()\n",
    "random.shuffle(shuffled_ids)\n",
    "ids_stim_s = [shuffled_ids[:45], shuffled_ids[45:]]\n",
    "\n",
    "# Create datasets dictionary\n",
    "datasets_stimulation = {}\n",
    "datasets_stimulation.update(create_dataset_entry_stim(baseline_df_s, ids_base_s, 'Baseline', 1))\n",
    "datasets_stimulation.update(create_dataset_entry_stim(tonic_df_s, ids_tonic_s, 'Tonic', 1))\n",
    "datasets_stimulation.update(create_dataset_entry_stim(stim_df_s, ids_stim_s, 'Triggered', 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired key order\n",
    "desired_order = ['Baseline1', 'Triggered1', 'Tonic1','Triggered2']\n",
    "desired_order_2 = ['Triggered1', 'Baseline1', 'Triggered2', 'Tonic1']\n",
    "# Reorder dictionary\n",
    "ordered_datasets = {key: datasets_stimulation[key] for key in desired_order}\n",
    "ordered_datasets_2 = {key: datasets_stimulation[key] for key in desired_order_2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './Data'\n",
    "path_to_save_data = os.path.join(data_dir, 'Real_Data_'+'Stimulation_v3'+'.pkl')\n",
    "\n",
    "# Pickle the data and save it to file\n",
    "with open(path_to_save_data, 'wb') as handle:\n",
    "    pickle.dump(ordered_datasets_2, handle, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sinthlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
