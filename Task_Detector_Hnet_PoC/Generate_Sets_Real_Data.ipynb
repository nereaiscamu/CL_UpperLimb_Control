{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nerea/anaconda3/envs/sinthlab/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import sys\n",
    "# Imports from other modules and packages in the project\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Chewie'\n",
    "date = '1007'\n",
    "fold = 4\n",
    "target_variable = 'vel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Helper functions for plotting (run this cell!)\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "# initialize a color palette for plotting\n",
    "palette = sns.xkcd_palette([\"windows blue\",\n",
    "                            \"red\",\n",
    "                            \"medium green\",\n",
    "                            \"dusty purple\",\n",
    "                            \"orange\",\n",
    "                            \"amber\",\n",
    "                            \"clay\",\n",
    "                            \"pink\",\n",
    "                            \"greyish\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Data/Processed_Data/Tidy_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[43mname\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mdate\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(data_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      4\u001b[0m     tidy_df \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'name' is not defined"
     ]
    }
   ],
   "source": [
    "data_path = '../Data/Processed_Data/Tidy_'+name+'_'+date+'.pkl'\n",
    "\n",
    "with open(data_path, 'rb') as file:\n",
    "    tidy_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Force Adaptation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train trials 36\n",
      "Test trials  11\n",
      "Val trials 9\n",
      "We are testing the optimization method on fold  4\n",
      "Train trials 36\n",
      "Test trials  11\n",
      "Val trials 9\n",
      "We are testing the optimization method on fold  4\n",
      "Train trials 36\n",
      "Test trials  11\n",
      "Val trials 9\n",
      "We are testing the optimization method on fold  4\n",
      "Train trials 43\n",
      "Test trials  13\n",
      "Val trials 11\n",
      "We are testing the optimization method on fold  4\n",
      "Train trials 43\n",
      "Test trials  13\n",
      "Val trials 11\n",
      "We are testing the optimization method on fold  4\n",
      "Train trials 43\n",
      "Test trials  13\n",
      "Val trials 11\n",
      "We are testing the optimization method on fold  4\n",
      "Train trials 45\n",
      "Test trials  14\n",
      "Val trials 11\n",
      "We are testing the optimization method on fold  4\n",
      "Train trials 45\n",
      "Test trials  14\n",
      "Val trials 11\n",
      "We are testing the optimization method on fold  4\n",
      "Train trials 45\n",
      "Test trials  14\n",
      "Val trials 11\n",
      "We are testing the optimization method on fold  4\n"
     ]
    }
   ],
   "source": [
    "# Create filtered DataFrames\n",
    "baseline_df = tidy_df.loc[tidy_df['epoch'] == 'BL']\n",
    "force_df = tidy_df.loc[tidy_df['epoch'] == 'AD']\n",
    "wo_df = tidy_df.loc[tidy_df['epoch'] == 'WO']\n",
    "\n",
    "# Function to split IDs into 3 equal sets\n",
    "def split_ids(df):\n",
    "    unique_ids = df.id.unique()\n",
    "    set_length = len(unique_ids) // 3\n",
    "    return [unique_ids[i * set_length:(i + 1) * set_length] for i in range(3)]\n",
    "\n",
    "# Split IDs for each epoch\n",
    "ids_base = split_ids(baseline_df)\n",
    "ids_force = split_ids(force_df)\n",
    "ids_wo = split_ids(wo_df)\n",
    "\n",
    "# Function to create dataset dictionary entries\n",
    "def create_dataset_entry(df, ids, name_prefix):\n",
    "    return {\n",
    "        f'{name_prefix}{i+1}': get_dataset(df.loc[df.id.isin(ids[i])], fold, target_variable='vel', no_outliers=False, force_data=True)[:6]\n",
    "        for i in range(3)\n",
    "    }\n",
    "\n",
    "# Create datasets dictionary\n",
    "datasets_adaptation = {}\n",
    "datasets_adaptation.update(create_dataset_entry(baseline_df, ids_base, 'Baseline'))\n",
    "datasets_adaptation.update(create_dataset_entry(force_df, ids_force, 'Force'))\n",
    "datasets_adaptation.update(create_dataset_entry(wo_df, ids_wo, 'Washout'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './Data'\n",
    "path_to_save_data = os.path.join(data_dir, 'Real_Data_'+'Adaptation'+'.pkl')\n",
    "\n",
    "# Pickle the data and save it to file\n",
    "with open(path_to_save_data, 'wb') as handle:\n",
    "    pickle.dump(datasets_adaptation, handle, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Stimulation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../Data/Processed_Data/Tidy_Sansa_0504.pkl'\n",
    "\n",
    "with open(data_path, 'rb') as file:\n",
    "    df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df_s = df.loc[df.type == 'BASELINE'].reset_index()\n",
    "tonic_df_s = df.loc[df.type == 'TONIC'].reset_index()\n",
    "stim_df_s = df.loc[df.type == 'BC-REACH'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_df_s.id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tonic_df_s.id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stim_df_s.id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train trials 18\n",
      "Test trials  6\n",
      "Val trials 5\n",
      "We are testing the optimization method on fold  4\n",
      "Train trials 22\n",
      "Test trials  7\n",
      "Val trials 6\n",
      "We are testing the optimization method on fold  4\n",
      "Train trials 22\n",
      "Test trials  7\n",
      "Val trials 6\n",
      "We are testing the optimization method on fold  4\n",
      "Train trials 22\n",
      "Test trials  7\n",
      "Val trials 5\n",
      "We are testing the optimization method on fold  4\n"
     ]
    }
   ],
   "source": [
    "# Function to create dataset dictionary entries\n",
    "def create_dataset_entry_stim(df, ids, name_prefix, num_sets):\n",
    "    return {\n",
    "        f'{name_prefix}{i+1}': get_dataset(df.loc[df.id.isin(ids[i])], fold, target_variable='target_vel', no_outliers=False, force_data=False)[:6]\n",
    "        for i in range(num_sets)\n",
    "    }\n",
    "\n",
    "\n",
    "ids_base_s = [baseline_df_s.id.unique()]\n",
    "ids_tonic_s = [tonic_df_s.id.unique()]\n",
    "ids_stim_s = [stim_df_s.id.unique()[:35], stim_df_s.id.unique()[35:]]\n",
    "\n",
    "# Create datasets dictionary\n",
    "datasets_stimulation = {}\n",
    "datasets_stimulation.update(create_dataset_entry_stim(baseline_df_s, ids_base_s, 'Baseline', 1))\n",
    "datasets_stimulation.update(create_dataset_entry_stim(tonic_df_s, ids_tonic_s, 'Tonic', 1))\n",
    "datasets_stimulation.update(create_dataset_entry_stim(stim_df_s, ids_stim_s, 'Triggered', 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desired key order\n",
    "desired_order = ['Baseline1', 'Triggered1', 'Tonic1','Triggered2']\n",
    "\n",
    "# Reorder dictionary\n",
    "ordered_datasets = {key: datasets_stimulation[key] for key in desired_order}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './Data'\n",
    "path_to_save_data = os.path.join(data_dir, 'Real_Data_'+'Stimulation'+'.pkl')\n",
    "\n",
    "# Pickle the data and save it to file\n",
    "with open(path_to_save_data, 'wb') as handle:\n",
    "    pickle.dump(ordered_datasets, handle, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sinthlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
