{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - EWC Notebook: Force Perturbation Dataset\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook focuses on the implementation of Elastic Weight Consolidation (EWC) for continual learning on a dynamic perturbation dataset. The primary goal is to explore the ability of neural decoders to adapt to perturbations by leveraging EWC to mitigate catastrophic forgetting. \n",
    "\n",
    "## Key Objectives\n",
    "\n",
    "1. **Target Variable**: The primary target variable in this notebook is velocity, and it will not be normalized. Outliers here are not removed. \n",
    "   \n",
    "2. **EWC Implementation**: \n",
    "   - Implement EWC to observe how well the model retains learned knowledge when trained sequentially on different tasks.\n",
    "   - Compare the performance of models trained with EWC against those without it, specifically evaluating the model's ability to retain knowledge from the baseline task while adapting to force perturbations.\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "1. **Imports and Setup**: All necessary libraries and modules are imported, and initial configurations such as device settings and paths are established.\n",
    "\n",
    "2. **Data Loading and Preprocessing**:\n",
    "   - Load pre-processed data for both baseline and force perturbation epochs.\n",
    "   - Apply Gaussian noise to input features to simulate realistic conditions.\n",
    "   - Prepare data for training, validation, and testing.\n",
    "\n",
    "3. **Baseline Model**:\n",
    "   - Train a basic RNN model on baseline data.\n",
    "   - Evaluate the model's performance on both baseline and perturbation data.\n",
    "\n",
    "4. **Full Data Training**:\n",
    "   - Train the RNN model on the combined dataset (baseline and perturbation).\n",
    "   - Save and evaluate the model's performance.\n",
    "\n",
    "5. **EWC Implementation**:\n",
    "   - Define and compute the Fisher Information Matrix.\n",
    "   - Apply EWC during training to retain knowledge from the baseline task while training on perturbation data.\n",
    "   - Compare the results with models trained without EWC.\n",
    "\n",
    "6. **Catastrophic Forgetting Evaluation**:\n",
    "   - Evaluate the impact of sequential training on catastrophic forgetting by training models on one task and then testing on another.\n",
    "   - Analyze the model's performance in retaining baseline task knowledge after training on perturbation data and vice versa.\n",
    "\n",
    "7. **Visualization and Results**:\n",
    "   - Visualize training and validation losses across epochs.\n",
    "   - Present the model's performance metrics (Explained Variance, RÂ²) for both baseline and perturbation tasks.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "The purpose of this notebook is to validate the effectiveness of EWC in mitigating catastrophic forgetting in a force adaptation task. By comparing models trained with and without EWC, we aim to demonstrate the benefits of continual learning approaches in neural decoding tasks, particularly in scenarios where the model must adapt to changing conditions while retaining previously learned information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Navigate up two levels to reach the grandparent directory (CL Control)\n",
    "parent_dir = os.path.abspath(os.path.join('..',))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from src.helpers import *\n",
    "from src.visualize import *\n",
    "from src.trainer import *\n",
    "from src.regularizers import *\n",
    "from Models.models import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import *\n",
    "from copy import deepcopy\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"c:\\\\Users\\\\nerea\\\\OneDrive\\\\Documentos\\\\EPFL_MASTER\\\\PDM\\\\Project\\\\PyalData\")\n",
    "# to change for the actual path where PyalData has been cloned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyaldata import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Chewie'\n",
    "date = '1007'\n",
    "fold = 0\n",
    "target_variable = 'vel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Helper functions for plotting (run this cell!)\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "# initialize a color palette for plotting\n",
    "palette = sns.xkcd_palette([\"windows blue\",\n",
    "                            \"red\",\n",
    "                            \"medium green\",\n",
    "                            \"dusty purple\",\n",
    "                            \"orange\",\n",
    "                            \"amber\",\n",
    "                            \"clay\",\n",
    "                            \"pink\",\n",
    "                            \"greyish\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_t_eval =  lambda array: torch.tensor(array, device=device, dtype=dtype)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../Data/Processed_Data/Tidy_'+name+'_'+date+'.pkl'\n",
    "\n",
    "with open(data_path, 'rb') as file:\n",
    "    tidy_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df = tidy_df.loc[tidy_df['epoch'] == 'BL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_df.id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_df =  tidy_df.loc[tidy_df['epoch'] == 'AD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to consider only the trials for which the monkey has already adapted to the perturbation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_keep = force_df.id.unique()[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline subset has a total of 170 trials, whereas the perturbation one contains 201 trials, we can for now try to remove the first 50 trials from the perturbation subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_df = force_df.loc[force_df.id.isin(ids_to_keep)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get train-val-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train trials 109\n",
      "Test trials  34\n",
      "Val trials 27\n",
      "We are testing the optimization method on fold  0\n"
     ]
    }
   ],
   "source": [
    "xx_train_base, yy_train_base, xx_val_base, yy_val_base,\\\n",
    "      xx_test_base, yy_test_base, info_train_base, info_val_base,\\\n",
    "          info_test_base, list_mins_base, \\\n",
    "            list_maxs_base= get_dataset(baseline_df, fold, target_variable= target_variable, no_outliers = False, force_data = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train trials 97\n",
      "Test trials  30\n",
      "Val trials 24\n",
      "We are testing the optimization method on fold  0\n"
     ]
    }
   ],
   "source": [
    "xx_train_force, yy_train_force, xx_val_force, yy_val_force,\\\n",
    "      xx_test_force, yy_test_force, info_train_force, info_val_force,\\\n",
    "          info_test_force,  list_mins_force, \\\n",
    "            list_maxs_force = get_dataset(force_df, fold, target_variable= target_variable, no_outliers = False, force_data = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train trials 211\n",
      "Test trials  66\n",
      "Val trials 53\n",
      "We are testing the optimization method on fold  0\n"
     ]
    }
   ],
   "source": [
    "xx_train_all, yy_train_all, xx_val_all, yy_val_all, \\\n",
    "    xx_test_all, yy_test_all, info_train_all, \\\n",
    "    info_val_all, info_test_all,  list_mins_all,\\\n",
    "          list_maxs_all = get_dataset(tidy_df,fold, target_variable= target_variable, no_outliers = False\n",
    "                                      , force_data = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify that we want our tensors on the GPU and in float32\n",
    "device = torch.device('cuda:0') #suposed to be cuda\n",
    "#device = torch.device('cpu') \n",
    "dtype = torch.float32\n",
    "path_to_models = '../Models/Models_Force'\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)  # If using CUDA\n",
    "\n",
    "num_dim_output = yy_train_base.shape[2]\n",
    "num_features = xx_train_base.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_reset(m):\n",
    "    reset_parameters = getattr(m, \"reset_parameters\", None)\n",
    "    if callable(reset_parameters):\n",
    "        m.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding noise to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Gaussian noise to input features\n",
    "xx_train_base = torch.tensor(xx_train_base, dtype=torch.float32) + torch.tensor(np.random.normal(loc=0, scale=0.075, size=xx_train_base.shape), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa7d41a39d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "#Hyperparameters objective and regularization\n",
    "alpha_reg = 1e-5\n",
    "l1_ratio_reg = 0.5\n",
    "\n",
    "lr = 0.00001\n",
    "loss_function = huber_loss\n",
    "delta = 8  # hyperparameter for huber loss\n",
    "\n",
    "# Hyperparameters LSTM class\n",
    "hidden_units = 300\n",
    "num_layers = 1\n",
    "input_size = 49\n",
    "dropout = 0.2\n",
    "\n",
    "#Other training hyperparameters\n",
    "\n",
    "lr_gamma= 1.37 #for scheduler\n",
    "lr_step_size = 10 #for scheduler\n",
    "\n",
    "seq_length_LSTM= 19\n",
    "batch_size_train= 25\n",
    "batch_size_val = 25\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  Causal_Simple_RNN(num_features=num_features, \n",
    "                hidden_units= hidden_units, \n",
    "                num_layers = num_layers, \n",
    "                out_dims = num_dim_output,\n",
    "                dropout = dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Causal_Simple_RNN(\n",
       "  (linear): Linear(in_features=300, out_features=2, bias=True)\n",
       "  (rnn): RNN(130, 300, batch_first=True)\n",
       "  (selu): SELU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the LSTM model\n",
    "model_base = model\n",
    "model_base.apply(weight_reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nerea/CL_UpperLimb_Control/src/trainer.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.tensor(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 Train 12.0800 Val 11.4261\n",
      "Epoch 001 Train 10.0309 Val 9.8023\n",
      "Epoch 002 Train 8.8359 Val 9.1065\n",
      "Epoch 003 Train 8.1913 Val 8.6166\n",
      "Epoch 004 Train 7.6851 Val 8.1048\n",
      "Epoch 005 Train 6.9805 Val 7.1412\n",
      "Epoch 006 Train 6.2320 Val 6.5102\n",
      "Epoch 007 Train 5.7184 Val 6.0479\n",
      "Epoch 008 Train 5.3147 Val 5.6570\n",
      "Epoch 009 Train 4.9600 Val 5.3139\n",
      "Epoch 010 Train 4.6026 Val 4.9216\n",
      "Epoch 011 Train 4.2328 Val 4.5756\n",
      "Epoch 012 Train 3.9203 Val 4.2808\n",
      "Epoch 013 Train 3.6519 Val 4.0531\n",
      "Epoch 014 Train 3.4266 Val 3.8309\n",
      "Epoch 015 Train 3.2438 Val 3.6965\n",
      "Epoch 016 Train 3.0871 Val 3.5425\n",
      "Epoch 017 Train 2.9633 Val 3.5127\n",
      "Epoch 018 Train 2.8599 Val 3.3889\n",
      "Epoch 019 Train 2.7697 Val 3.3080\n",
      "Epoch 020 Train 2.7007 Val 3.2303\n",
      "Epoch 021 Train 2.6314 Val 3.2159\n",
      "Epoch 022 Train 2.5648 Val 3.1310\n",
      "Epoch 023 Train 2.5249 Val 3.1101\n",
      "Epoch 024 Train 2.4908 Val 3.0777\n",
      "Epoch 025 Train 2.4637 Val 3.0411\n",
      "Epoch 026 Train 2.4312 Val 3.0475\n",
      "Epoch 027 Train 2.4136 Val 2.9796\n",
      "Epoch 028 Train 2.3882 Val 3.0089\n",
      "Epoch 029 Train 2.3784 Val 2.9861\n",
      "Epoch 030 Train 2.3660 Val 3.0029\n",
      "Epoch 031 Train 2.3512 Val 2.9422\n",
      "Epoch 032 Train 2.3310 Val 2.9290\n",
      "Epoch 033 Train 2.3176 Val 2.9921\n",
      "Epoch 034 Train 2.3068 Val 3.0055\n",
      "Epoch 035 Train 2.2964 Val 2.9070\n",
      "Epoch 036 Train 2.2811 Val 2.9129\n",
      "Epoch 037 Train 2.2718 Val 2.9058\n",
      "Epoch 038 Train 2.2573 Val 2.9020\n",
      "Epoch 039 Train 2.2460 Val 2.8627\n",
      "Epoch 040 Train 2.2549 Val 2.9054\n",
      "Epoch 041 Train 2.2348 Val 2.8121\n",
      "Epoch 042 Train 2.2180 Val 2.8586\n",
      "Epoch 043 Train 2.2164 Val 2.8807\n",
      "Epoch 044 Train 2.2078 Val 2.7977\n",
      "Epoch 045 Train 2.2063 Val 2.8334\n",
      "Epoch 046 Train 2.1875 Val 2.8315\n",
      "Epoch 047 Train 2.1784 Val 2.7846\n",
      "Epoch 048 Train 2.1672 Val 2.8026\n",
      "Epoch 049 Train 2.1623 Val 2.7957\n",
      "Epoch 050 Train 2.1741 Val 2.7991\n",
      "Epoch 051 Train 2.1601 Val 2.8312\n",
      "Epoch 052 Train 2.1497 Val 2.7337\n",
      "Epoch 053 Train 2.1493 Val 2.7366\n",
      "Epoch 054 Train 2.1419 Val 2.7812\n",
      "Epoch 055 Train 2.1325 Val 2.7838\n",
      "Epoch 056 Train 2.1027 Val 2.8498\n",
      "Decrease LR\n",
      "Epoch 057 Train 2.1216 Val 2.7377\n",
      "Epoch 058 Train 2.0666 Val 2.7625\n",
      "Epoch 059 Train 2.0711 Val 2.7673\n",
      "Epoch 060 Train 2.0832 Val 2.7429\n",
      "Epoch 061 Train 2.0731 Val 2.7868\n",
      "Epoch 062 Train 2.0692 Val 2.7264\n",
      "Epoch 063 Train 2.0743 Val 2.6920\n",
      "Epoch 064 Train 2.0584 Val 2.7851\n",
      "Epoch 065 Train 2.0626 Val 2.7235\n",
      "Epoch 066 Train 2.0474 Val 2.7025\n",
      "Epoch 067 Train 2.0373 Val 2.7282\n",
      "Decrease LR\n",
      "63\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = \\\n",
    "    train_model(model_base, \n",
    "                xx_train_base,yy_train_base,\n",
    "                xx_val_base, \n",
    "                yy_val_base,\n",
    "                lr= lr,\n",
    "                lr_step_size=lr_step_size,\n",
    "                lr_gamma= lr_gamma,\n",
    "                sequence_length_LSTM=seq_length_LSTM,\n",
    "                batch_size_train = batch_size_train,\n",
    "                batch_size_val = batch_size_val,\n",
    "                num_epochs=1000, \n",
    "                delta = 8,                 \n",
    "                regularizer= Regularizer_RNN, \n",
    "                l1_ratio = l1_ratio_reg,\n",
    "                alpha = alpha_reg,     \n",
    "                early_stop = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'RNN'+ name+ '_' +date+'_Baseline'\n",
    "path_base_model = os.path.join(path_to_models,experiment_name)\n",
    "if not os.path.exists(path_base_model):\n",
    "            os.makedirs(path_base_model)\n",
    "path_base_fold = os.path.join(path_base_model,'fold_{}.pth'.format(fold))\n",
    "torch.save(model_base, path_base_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Causal_Simple_RNN(\n",
       "  (linear): Linear(in_features=300, out_features=2, bias=True)\n",
       "  (rnn): RNN(130, 300, batch_first=True)\n",
       "  (selu): SELU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_baselineonly = torch.load(path_base_fold)\n",
    "model_baselineonly.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train EV: 0.94 \n",
      "Val EV: 0.90 \n",
      "Test EV: 0.87 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nerea/CL_UpperLimb_Control/src/trainer.py:220: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(x, device=device, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "y_hat, y_true, train_score, v_score, test_score = eval_model(xx_train_base, yy_train_base, xx_val_base, yy_val_base, xx_test_base, yy_test_base, model_baselineonly, metric = 'ev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2: 0.94 \n",
      "Val R2: 0.90 \n",
      "Test R2: 0.87 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nerea/CL_UpperLimb_Control/src/trainer.py:220: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(x, device=device, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "y_hat, y_true, train_score, v_score, test_score = eval_model(xx_train_base, yy_train_base, xx_val_base, yy_val_base, xx_test_base, yy_test_base, model_baselineonly, metric = 'r2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model on force data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train EV: 0.49 \n",
      "Val EV: 0.49 \n",
      "Test EV: 0.53 \n"
     ]
    }
   ],
   "source": [
    "y_hat, y_true, train_score, v_score, test_score = eval_model(xx_train_force, yy_train_force, xx_val_force, yy_val_force, xx_test_force, yy_test_force, model_baselineonly, metric = 'ev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we use all data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Causal_Simple_RNN(\n",
       "  (linear): Linear(in_features=300, out_features=2, bias=True)\n",
       "  (rnn): RNN(130, 300, batch_first=True)\n",
       "  (selu): SELU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_all = model\n",
    "model_all.apply(weight_reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 Train 9.8331 Val 7.7173\n",
      "Epoch 001 Train 6.7892 Val 5.2663\n",
      "Epoch 002 Train 4.9698 Val 4.2868\n",
      "Epoch 003 Train 4.1628 Val 3.7672\n",
      "Epoch 004 Train 3.6566 Val 3.4615\n",
      "Epoch 005 Train 3.3029 Val 3.1952\n",
      "Epoch 006 Train 3.0489 Val 3.0291\n",
      "Epoch 007 Train 2.8524 Val 2.9213\n",
      "Epoch 008 Train 2.6980 Val 2.8095\n",
      "Epoch 009 Train 2.5699 Val 2.7320\n",
      "Epoch 010 Train 2.4502 Val 2.5915\n",
      "Epoch 011 Train 2.3226 Val 2.5498\n",
      "Epoch 012 Train 2.2147 Val 2.4524\n",
      "Epoch 013 Train 2.1173 Val 2.3842\n",
      "Epoch 014 Train 2.0324 Val 2.3439\n",
      "Epoch 015 Train 1.9547 Val 2.3226\n",
      "Epoch 016 Train 1.8863 Val 2.2668\n",
      "Epoch 017 Train 1.8216 Val 2.2186\n",
      "Epoch 018 Train 1.7641 Val 2.2092\n",
      "Epoch 019 Train 1.7130 Val 2.1909\n",
      "Epoch 020 Train 1.6580 Val 2.1957\n",
      "Epoch 021 Train 1.5973 Val 2.1836\n",
      "Epoch 022 Train 1.5454 Val 2.1528\n",
      "Epoch 023 Train 1.4947 Val 2.1575\n",
      "Epoch 024 Train 1.4505 Val 2.1684\n",
      "Epoch 025 Train 1.4082 Val 2.1647\n",
      "Epoch 026 Train 1.3700 Val 2.1296\n",
      "Epoch 027 Train 1.3327 Val 2.1513\n",
      "Epoch 028 Train 1.3017 Val 2.1607\n",
      "Epoch 029 Train 1.2691 Val 2.1669\n",
      "Epoch 030 Train 1.2408 Val 2.1415\n",
      "Decrease LR\n",
      "Epoch 031 Train 1.2059 Val 2.1617\n",
      "Epoch 032 Train 1.1720 Val 2.1475\n",
      "Epoch 033 Train 1.1564 Val 2.1534\n",
      "Epoch 034 Train 1.1408 Val 2.1692\n",
      "Epoch 035 Train 1.1266 Val 2.1396\n",
      "Decrease LR\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = \\\n",
    "    train_model(model_all, xx_train_all,yy_train_all,\n",
    "                xx_val_all, \n",
    "                yy_val_all,\n",
    "                lr= lr,\n",
    "                lr_step_size=lr_step_size,\n",
    "                lr_gamma= lr_gamma,\n",
    "                sequence_length_LSTM=seq_length_LSTM,\n",
    "                batch_size_train = batch_size_train,\n",
    "                batch_size_val = batch_size_val,\n",
    "                num_epochs=1000, \n",
    "                delta = 8,                 \n",
    "                regularizer=None, #Regularizer_LSTM,\n",
    "                l1_ratio = l1_ratio_reg,\n",
    "                alpha = alpha_reg,     \n",
    "                early_stop = 5,\n",
    "                \n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'RNN'+name+'_'+date+'_Alldata'\n",
    "path_to_save_model = os.path.join(path_to_models,experiment_name)\n",
    "if not os.path.exists(path_to_save_model):\n",
    "            os.makedirs(path_to_save_model)\n",
    "path_to_save_model_fold = os.path.join(path_to_save_model,'fold_{}.pth'.format(fold))\n",
    "torch.save(model_all, path_to_save_model_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Causal_Simple_RNN(\n",
       "  (linear): Linear(in_features=300, out_features=2, bias=True)\n",
       "  (rnn): RNN(130, 300, batch_first=True)\n",
       "  (selu): SELU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_all_data = torch.load(path_to_save_model_fold)\n",
    "model_all_data.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train EV: 0.89 \n",
      "Val EV: 0.86 \n",
      "Test EV: 0.86 \n"
     ]
    }
   ],
   "source": [
    "y_hat, y_true, train_score, v_score, test_score = eval_model(xx_train_all, yy_train_all, xx_val_all, yy_val_all, xx_test_all, yy_test_all, model_all_data, metric = 'ev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing EWC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining EWC loss. For each parameter, we multiply the sqared difference between the current training parameter and the optimal one for the previous task by the importance of the parameter (extracted from the Fisher information matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ewc_loss(model, fisher, p_old):\n",
    "    loss = 0\n",
    "    for n, p in model.named_parameters():\n",
    "        _loss = fisher[n] * (p - p_old[n]) ** 2\n",
    "        loss += _loss.sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48172/2062473542.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  to_t_eval =  lambda array: torch.tensor(array, device=device, dtype=dtype)\n"
     ]
    }
   ],
   "source": [
    "x = to_t_eval(xx_train_base) \n",
    "y = to_t_eval(yy_train_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train and y_train are NumPy arrays or PyTorch tensors\n",
    "dataset = list(zip(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Causal_Simple_RNN(\n",
       "  (linear): Linear(in_features=300, out_features=2, bias=True)\n",
       "  (rnn): RNN(130, 300, batch_first=True)\n",
       "  (selu): SELU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pre_EWC = copy.deepcopy(model_baselineonly) \n",
    "# Flatten the parameters of the copied model\n",
    "for module in model_pre_EWC.modules():\n",
    "    if isinstance(module, nn.RNNBase):\n",
    "        module.flatten_parameters()\n",
    "model_pre_EWC.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the optimal parameters for the baseline task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {n: p for n, p in model_pre_EWC.named_parameters() if p.requires_grad}\n",
    "p_old = {}\n",
    "\n",
    "for n, p in deepcopy(params).items():\n",
    "    p_old[n] = p.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the Fisher matrix. \n",
    "\n",
    "Each element of the Fisher information matrix is approximated as squared gradients averaged across mini-batches during a single pass through the training dataset.\n",
    "For each input and label in the dataset the gradient goes back to 0, the model does a forward pass, the loss is computed, and a backward step is taken. From the step, each parameter's gradient is obtained and the sqared of this gradient is added to the Fisher matrix diagonal, normalized by the length of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fisher_diag(model, dataset, params, empirical=True):\n",
    " \n",
    "    fisher = {}\n",
    "    for n, p in deepcopy(params).items():\n",
    "        p.data.zero_()\n",
    "        fisher[n] = p.data\n",
    "\n",
    "    \n",
    "    for input, gt_label in dataset:\n",
    "        model.zero_grad()\n",
    "        output = model(input).view(-1)\n",
    "        if empirical:\n",
    "            label = gt_label.view(-1)\n",
    "            \n",
    "        else:\n",
    "            label = output.max(1)[1].view(-1)\n",
    "            \n",
    "\n",
    "        h_loss  = huber_loss(output, label)\n",
    "        #negloglikelihood = F.nll_loss(F.log_softmax(output, dim = -1), label)\n",
    "        #negloglikelihood.backward()\n",
    "        model.train()\n",
    "        h_loss.backward()\n",
    "\n",
    "        for n, p in model.named_parameters():\n",
    "            fisher[n].data += p.grad.data ** 2 / len(dataset)\n",
    "\n",
    "    fisher = {n: p for n, p in fisher.items()}\n",
    "    return fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_matrix = get_fisher_diag(model_pre_EWC, dataset, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_EWC(model, X,Y,\n",
    "                X_val, \n",
    "                Y_val,\n",
    "                lr=lr, # 0.0001,\n",
    "                lr_step_size= lr_step_size ,#10,\n",
    "                lr_gamma=lr_gamma,#0.9,\n",
    "                sequence_length_LSTM= seq_length_LSTM, #10,\n",
    "                batch_size_train = batch_size_train, #3,\n",
    "                batch_size_val = batch_size_val,# 3,\n",
    "                num_epochs=1000, \n",
    "                delta = 8,                 \n",
    "                regularizer=None,\n",
    "                l1_ratio = l1_ratio_reg, #0.5,\n",
    "                alpha = alpha_reg, #1e-5,     \n",
    "                early_stop = 5,\n",
    "                lambda_ewc = 0.2):\n",
    "\n",
    "    # Set up the optimizer with the specified learning rate\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Set up a learning rate scheduler\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, \n",
    "                                    step_size=lr_step_size, \n",
    "                                    gamma=lr_gamma)\n",
    "    \n",
    "    \n",
    "        \n",
    "    # Keep track of the best model's parameters and loss\n",
    "    best_model_wts = deepcopy(model.state_dict())\n",
    "    best_loss = 1e8\n",
    "\n",
    "    # Enable anomaly detection for debugging\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    # Track the train and validation loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    # Counters for early stopping\n",
    "    not_increased = 0\n",
    "    end_train = 0\n",
    "    \n",
    "    # Reshape data for the LSTM\n",
    "    train_dataset = SequenceDataset(\n",
    "    Y,    X,    sequence_length=sequence_length_LSTM)\n",
    "\n",
    "    val_dataset = SequenceDataset(\n",
    "    Y_val,    X_val,    sequence_length=sequence_length_LSTM)\n",
    "    loader_train = data.DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "    loader_val = data.DataLoader(val_dataset, batch_size=batch_size_val, shuffle=True)\n",
    "\n",
    "    # Loop through epochs\n",
    "    for epoch in np.arange(num_epochs):\n",
    "        for phase in ['train', 'val']:\n",
    "            # set model to train/validation as appropriate\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                loader = loader_train\n",
    "            else:\n",
    "                model.eval()\n",
    "                loader = loader_val\n",
    "\n",
    "            # Initialize variables to track loss and batch size\n",
    "            running_loss = 0\n",
    "            running_size = 0        \n",
    "\n",
    "            # Iterate over batches in the loader\n",
    "            for X_, y_ in loader:\n",
    "                X_ = X_.to(device)\n",
    "                y_ = y_.to(device)\n",
    "                if phase == \"train\":\n",
    "                    with torch.set_grad_enabled(True):\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        output_t = model(X_)\n",
    "                        output_t = torch.squeeze(output_t)\n",
    "\n",
    "\n",
    "                        loss_t = huber_loss(output_t, y_, delta = delta)\n",
    "                        \n",
    "                        \n",
    "                        # Add regularization to the loss in the training phase\n",
    "                        if regularizer is not None:\n",
    "                            ewc_loss = get_ewc_loss(model, fisher_matrix, p_old)\n",
    "                            loss_t += lambda_ewc * ewc_loss\n",
    "                        #     loss += regularizer(model, l1_ratio, alpha)\n",
    "                        # Compute gradients and perform an optimization step\n",
    "                        loss_t.backward(retain_graph=True)\n",
    "                        optimizer.step()\n",
    "                else:\n",
    "                    # just compute the loss in validation phase\n",
    "                    output_t = model(X_)\n",
    "                    output_t = torch.squeeze(output_t)\n",
    "\n",
    "                    loss_t = huber_loss(output_t, y_, delta = delta)\n",
    "                    \n",
    "\n",
    "                # Ensure the loss is finite\n",
    "                assert torch.isfinite(loss_t)\n",
    "                running_loss += loss_t.item()\n",
    "                running_size += 1\n",
    "\n",
    "            # compute the train/validation loss and update the best\n",
    "            # model parameters if this is the lowest validation loss yet\n",
    "            running_loss /= running_size\n",
    "            if phase == \"train\":\n",
    "                train_losses.append(running_loss)\n",
    "            else:\n",
    "                val_losses.append(running_loss)\n",
    "                # Update best model parameters if validation loss improves\n",
    "                if running_loss < best_loss:\n",
    "                    best_loss = running_loss\n",
    "                    best_epoch = epoch\n",
    "                    best_model_wts = deepcopy(model.state_dict())\n",
    "                    not_increased = 0\n",
    "                else:\n",
    "                    # Perform early stopping if validation loss doesn't improve\n",
    "                    if epoch > 10:\n",
    "                        not_increased += 1\n",
    "                        # print('Not increased : {}/5'.format(not_increased))\n",
    "                        if not_increased == early_stop:\n",
    "                            print('Decrease LR')\n",
    "                            for g in optimizer.param_groups:\n",
    "                                g['lr'] = g['lr'] / 2\n",
    "                            not_increased = 0\n",
    "                            end_train += 1\n",
    "                        \n",
    "                        if end_train == 2:\n",
    "                            model.load_state_dict(best_model_wts)\n",
    "                            print(best_epoch)\n",
    "                            return np.array(train_losses), np.array(val_losses), best_epoch\n",
    "\n",
    "        # Update learning rate with the scheduler\n",
    "        scheduler.step()\n",
    "        print(\"Epoch {:03} Train {:.4f} Val {:.4f}\".format(epoch, train_losses[-1], val_losses[-1]))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print(best_epoch)\n",
    "\n",
    "    return np.array(train_losses), np.array(val_losses), best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note` Grid search for lambda (strength of regularization in EWC) not included here. Best lambda value obtained was 1900."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 Train 4.7855 Val 4.3033\n",
      "Epoch 001 Train 4.2282 Val 4.0862\n",
      "Epoch 002 Train 4.0822 Val 3.9902\n",
      "Epoch 003 Train 4.0116 Val 3.9420\n",
      "Epoch 004 Train 3.9586 Val 3.9015\n",
      "Epoch 005 Train 3.9242 Val 3.8929\n",
      "Epoch 006 Train 3.9145 Val 3.8723\n",
      "Epoch 007 Train 3.8887 Val 3.7913\n",
      "Epoch 008 Train 3.8580 Val 3.8048\n",
      "Epoch 009 Train 3.8530 Val 3.8045\n",
      "Epoch 010 Train 3.8525 Val 3.8050\n",
      "Epoch 011 Train 3.8487 Val 3.7751\n",
      "Epoch 012 Train 3.8276 Val 3.7280\n",
      "Epoch 013 Train 3.8093 Val 3.7748\n",
      "Epoch 014 Train 3.8203 Val 3.7291\n",
      "Epoch 015 Train 3.8050 Val 3.7287\n",
      "Epoch 016 Train 3.8049 Val 3.7542\n",
      "Epoch 017 Train 3.7935 Val 3.7012\n",
      "Epoch 018 Train 3.7919 Val 3.6921\n",
      "Epoch 019 Train 3.7831 Val 3.6760\n",
      "Epoch 020 Train 3.8054 Val 3.6759\n",
      "Epoch 021 Train 3.7998 Val 3.6991\n",
      "Epoch 022 Train 3.8320 Val 3.7129\n",
      "Epoch 023 Train 3.7888 Val 3.7168\n",
      "Epoch 024 Train 3.8162 Val 3.7269\n",
      "Decrease LR\n",
      "Epoch 025 Train 3.7919 Val 3.6831\n",
      "Epoch 026 Train 3.7535 Val 3.6957\n",
      "Epoch 027 Train 3.7554 Val 3.6736\n",
      "Epoch 028 Train 3.7482 Val 3.6548\n",
      "Epoch 029 Train 3.7445 Val 3.6933\n",
      "Epoch 030 Train 3.7624 Val 3.7153\n",
      "Epoch 031 Train 3.7624 Val 3.6845\n",
      "Epoch 032 Train 3.7610 Val 3.6467\n",
      "Epoch 033 Train 3.7651 Val 3.6859\n",
      "Epoch 034 Train 3.7553 Val 3.6309\n",
      "Epoch 035 Train 3.7639 Val 3.6772\n",
      "Epoch 036 Train 3.7622 Val 3.6874\n",
      "Epoch 037 Train 3.7491 Val 3.6800\n",
      "Epoch 038 Train 3.7481 Val 3.6964\n",
      "Epoch 039 Train 3.7590 Val 3.6208\n",
      "Epoch 040 Train 3.7651 Val 3.6960\n",
      "Epoch 041 Train 3.7803 Val 3.6263\n",
      "Epoch 042 Train 3.7804 Val 3.6792\n",
      "Epoch 043 Train 3.7827 Val 3.7263\n",
      "Decrease LR\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, best_epoch = \\\n",
    "    train_model_EWC(model_pre_EWC, xx_train_force,yy_train_force,\n",
    "                xx_val_force, \n",
    "                yy_val_force,\n",
    "                lr= 1e-5,\n",
    "                lr_step_size=lr_step_size,\n",
    "                lr_gamma= lr_gamma,\n",
    "                sequence_length_LSTM=seq_length_LSTM,\n",
    "                batch_size_train = batch_size_train,\n",
    "                batch_size_val = batch_size_val,\n",
    "                num_epochs=1000, \n",
    "                delta = 8,                 \n",
    "                regularizer= True,\n",
    "                l1_ratio = 0.5,\n",
    "                alpha = 1e-5,     \n",
    "                early_stop = 5,\n",
    "                lambda_ewc = 1900\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'RNN_'+name+'_'+date+'_EWC'\n",
    "path_to_save_model = os.path.join(path_to_models,experiment_name)\n",
    "if not os.path.exists(path_to_save_model):\n",
    "            os.makedirs(path_to_save_model)\n",
    "path_to_save_model_fold = os.path.join(path_to_save_model,'fold_{}.pth'.format(fold))\n",
    "torch.save(model_pre_EWC, path_to_save_model_fold) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Causal_Simple_RNN(\n",
       "  (linear): Linear(in_features=300, out_features=2, bias=True)\n",
       "  (rnn): RNN(130, 300, batch_first=True)\n",
       "  (selu): SELU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_EWC = torch.load(path_to_save_model_fold)\n",
    "model_EWC.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train EV: 0.78 \n",
      "Val EV: 0.72 \n",
      "Test EV: 0.77 \n"
     ]
    }
   ],
   "source": [
    "y_hat, y_true, train_score, v_score, test_score = eval_model(xx_train_force, yy_train_force, xx_val_force, yy_val_force, xx_test_force, yy_test_force, model_EWC, metric = 'ev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the performance of the model on Baseline data after EWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train EV: 0.81 \n",
      "Val EV: 0.74 \n",
      "Test EV: 0.76 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nerea/CL_UpperLimb_Control/src/trainer.py:220: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(x, device=device, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "y_hat, y_true, train_score, v_score, test_score = eval_model(xx_train_base, yy_train_base, xx_val_base, yy_val_base, xx_test_base, yy_test_base, model_EWC, metric = 'ev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model only on force data and testing on baseline data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Causal_Simple_RNN(\n",
       "  (linear): Linear(in_features=300, out_features=2, bias=True)\n",
       "  (rnn): RNN(130, 300, batch_first=True)\n",
       "  (selu): SELU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_force = model\n",
    "model_force.apply(weight_reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 Train 10.8152 Val 9.5241\n",
      "Epoch 001 Train 7.9973 Val 7.7428\n",
      "Epoch 002 Train 6.7816 Val 6.9417\n",
      "Epoch 003 Train 6.0715 Val 6.3801\n",
      "Epoch 004 Train 5.4451 Val 5.6979\n",
      "Epoch 005 Train 4.6607 Val 4.8218\n",
      "Epoch 006 Train 4.0079 Val 4.3118\n",
      "Epoch 007 Train 3.6178 Val 4.0043\n",
      "Epoch 008 Train 3.3361 Val 3.7720\n",
      "Epoch 009 Train 3.1123 Val 3.5694\n",
      "Epoch 010 Train 2.9006 Val 3.3917\n",
      "Epoch 011 Train 2.6978 Val 3.2287\n",
      "Epoch 012 Train 2.5276 Val 3.0855\n",
      "Epoch 013 Train 2.3827 Val 2.9819\n",
      "Epoch 014 Train 2.2610 Val 2.9036\n",
      "Epoch 015 Train 2.1457 Val 2.8290\n",
      "Epoch 016 Train 2.0404 Val 2.7663\n",
      "Epoch 017 Train 1.9433 Val 2.7044\n",
      "Epoch 018 Train 1.8553 Val 2.6716\n",
      "Epoch 019 Train 1.7664 Val 2.6054\n",
      "Epoch 020 Train 1.6805 Val 2.5461\n",
      "Epoch 021 Train 1.5875 Val 2.5195\n",
      "Epoch 022 Train 1.5064 Val 2.4624\n",
      "Epoch 023 Train 1.4382 Val 2.4002\n",
      "Epoch 024 Train 1.3795 Val 2.3787\n",
      "Epoch 025 Train 1.3268 Val 2.3504\n",
      "Epoch 026 Train 1.2764 Val 2.3139\n",
      "Epoch 027 Train 1.2309 Val 2.3050\n",
      "Epoch 028 Train 1.1889 Val 2.3064\n",
      "Epoch 029 Train 1.1496 Val 2.2882\n",
      "Epoch 030 Train 1.1081 Val 2.2583\n",
      "Epoch 031 Train 1.0631 Val 2.2531\n",
      "Epoch 032 Train 1.0187 Val 2.2551\n",
      "Epoch 033 Train 0.9833 Val 2.2584\n",
      "Epoch 034 Train 0.9480 Val 2.2454\n",
      "Epoch 035 Train 0.9205 Val 2.2443\n",
      "Epoch 036 Train 0.8902 Val 2.2600\n",
      "Epoch 037 Train 0.8663 Val 2.2689\n",
      "Epoch 038 Train 0.8430 Val 2.2618\n",
      "Epoch 039 Train 0.8238 Val 2.2593\n",
      "Decrease LR\n",
      "Epoch 040 Train 0.8044 Val 2.2818\n",
      "Epoch 041 Train 0.7800 Val 2.2989\n",
      "Epoch 042 Train 0.7671 Val 2.2833\n",
      "Epoch 043 Train 0.7562 Val 2.2944\n",
      "Epoch 044 Train 0.7470 Val 2.2929\n",
      "Decrease LR\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = \\\n",
    "    train_model(model_force, xx_train_force,yy_train_force,\n",
    "                xx_val_force, \n",
    "                yy_val_force,\n",
    "                lr= lr,\n",
    "                lr_step_size=lr_step_size,\n",
    "                lr_gamma= lr_gamma,\n",
    "                sequence_length_LSTM=seq_length_LSTM,\n",
    "                batch_size_train = batch_size_train,\n",
    "                batch_size_val = batch_size_val,\n",
    "                num_epochs=1000, \n",
    "                delta = 8,                 \n",
    "                regularizer= None, #Regularizer_LSTM,\n",
    "                l1_ratio = l1_ratio_reg,\n",
    "                alpha = alpha_reg,     \n",
    "                early_stop = 5,\n",
    "                \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'RNN'+name+'_'+date+'_Force'\n",
    "path_to_save_model = os.path.join(path_to_models,experiment_name)\n",
    "if not os.path.exists(path_to_save_model):\n",
    "            os.makedirs(path_to_save_model)\n",
    "path_to_save_model_fold = os.path.join(path_to_save_model,'fold_{}.pth'.format(fold))\n",
    "torch.save(model_force, path_to_save_model_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Causal_Simple_RNN(\n",
       "  (linear): Linear(in_features=300, out_features=2, bias=True)\n",
       "  (rnn): RNN(130, 300, batch_first=True)\n",
       "  (selu): SELU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_force = torch.load(path_to_save_model_fold)\n",
    "model_force.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train EV: 0.94 \n",
      "Val EV: 0.80 \n",
      "Test EV: 0.85 \n"
     ]
    }
   ],
   "source": [
    "y_hat, y_true, train_score, v_score, test_score = eval_model(xx_train_force, yy_train_force, xx_val_force, yy_val_force, xx_test_force, yy_test_force, model_force, metric = 'ev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now test on baseline data to compare to the EWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train EV: 0.31 \n",
      "Val EV: 0.30 \n",
      "Test EV: 0.34 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nerea/CL_UpperLimb_Control/src/trainer.py:220: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(x, device=device, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "y_hat, y_true, train_score, v_score, test_score = eval_model(xx_train_base, yy_train_base, xx_val_base, yy_val_base, xx_test_base, yy_test_base, model_force, metric = 'ev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Catastrophic Forgetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is to take models trained for one specific task, train them on the other task and then see how they generalize or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on stimulation data using baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Causal_Simple_RNN(\n",
       "  (linear): Linear(in_features=300, out_features=2, bias=True)\n",
       "  (rnn): RNN(130, 300, batch_first=True)\n",
       "  (selu): SELU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_force_after_base  = copy.deepcopy(model_baselineonly)\n",
    "\n",
    "# Flatten the parameters of the copied model\n",
    "for module in model_force_after_base.modules():\n",
    "    if isinstance(module, nn.RNNBase):\n",
    "        module.flatten_parameters()\n",
    "model_force_after_base.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 Train 4.3486 Val 3.9733\n",
      "Epoch 001 Train 3.3735 Val 3.5937\n",
      "Epoch 002 Train 3.0583 Val 3.3521\n",
      "Epoch 003 Train 2.8624 Val 3.2325\n",
      "Epoch 004 Train 2.7140 Val 3.1133\n",
      "Epoch 005 Train 2.6029 Val 3.0435\n",
      "Epoch 006 Train 2.5040 Val 2.9437\n",
      "Epoch 007 Train 2.4278 Val 2.9223\n",
      "Epoch 008 Train 2.3542 Val 2.8661\n",
      "Epoch 009 Train 2.2968 Val 2.8334\n",
      "Epoch 010 Train 2.2390 Val 2.8041\n",
      "Epoch 011 Train 2.1671 Val 2.8061\n",
      "Epoch 012 Train 2.1069 Val 2.7706\n",
      "Epoch 013 Train 2.0429 Val 2.7413\n",
      "Epoch 014 Train 1.9925 Val 2.7095\n",
      "Epoch 015 Train 1.9326 Val 2.7246\n",
      "Epoch 016 Train 1.8732 Val 2.6905\n",
      "Epoch 017 Train 1.8249 Val 2.7031\n",
      "Epoch 018 Train 1.7688 Val 2.6909\n",
      "Epoch 019 Train 1.7192 Val 2.7448\n",
      "Epoch 020 Train 1.7080 Val 2.6905\n",
      "Decrease LR\n",
      "Epoch 021 Train 1.6609 Val 2.7125\n",
      "Epoch 022 Train 1.6213 Val 2.6710\n",
      "Epoch 023 Train 1.5970 Val 2.6612\n",
      "Epoch 024 Train 1.5805 Val 2.6922\n",
      "Epoch 025 Train 1.5576 Val 2.6559\n",
      "Epoch 026 Train 1.5387 Val 2.6354\n",
      "Epoch 027 Train 1.5256 Val 2.6242\n",
      "Epoch 028 Train 1.4992 Val 2.6468\n",
      "Epoch 029 Train 1.4794 Val 2.6422\n",
      "Epoch 030 Train 1.4643 Val 2.5993\n",
      "Epoch 031 Train 1.4368 Val 2.6486\n",
      "Epoch 032 Train 1.4163 Val 2.6692\n",
      "Epoch 033 Train 1.3954 Val 2.6003\n",
      "Epoch 034 Train 1.3738 Val 2.6269\n",
      "Epoch 035 Train 1.3534 Val 2.5927\n",
      "Epoch 036 Train 1.3295 Val 2.6190\n",
      "Epoch 037 Train 1.3121 Val 2.5868\n",
      "Epoch 038 Train 1.2914 Val 2.5597\n",
      "Epoch 039 Train 1.2730 Val 2.5824\n",
      "Epoch 040 Train 1.2598 Val 2.5453\n",
      "Epoch 041 Train 1.2365 Val 2.5166\n",
      "Epoch 042 Train 1.2132 Val 2.5183\n",
      "Epoch 043 Train 1.1951 Val 2.5291\n",
      "Epoch 044 Train 1.1732 Val 2.5017\n",
      "Epoch 045 Train 1.1563 Val 2.4925\n",
      "Epoch 046 Train 1.1367 Val 2.5036\n",
      "Epoch 047 Train 1.1189 Val 2.5105\n",
      "Epoch 048 Train 1.1033 Val 2.4325\n",
      "Epoch 049 Train 1.0903 Val 2.4691\n",
      "Epoch 050 Train 1.0749 Val 2.4312\n",
      "Epoch 051 Train 1.0637 Val 2.4019\n",
      "Epoch 052 Train 1.0371 Val 2.4308\n",
      "Epoch 053 Train 1.0272 Val 2.4242\n",
      "Epoch 054 Train 1.0043 Val 2.4289\n",
      "Epoch 055 Train 0.9902 Val 2.3610\n",
      "Epoch 056 Train 0.9784 Val 2.4573\n",
      "Epoch 057 Train 0.9645 Val 2.4213\n",
      "Epoch 058 Train 0.9493 Val 2.3853\n",
      "Epoch 059 Train 0.9349 Val 2.4091\n",
      "Decrease LR\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = \\\n",
    "    train_model(model_force_after_base, xx_train_force,yy_train_force,\n",
    "                xx_val_force, \n",
    "                yy_val_force,\n",
    "                lr= lr,\n",
    "                lr_step_size=lr_step_size,\n",
    "                lr_gamma= lr_gamma,\n",
    "                sequence_length_LSTM=seq_length_LSTM,\n",
    "                batch_size_train = batch_size_train,\n",
    "                batch_size_val = batch_size_val,\n",
    "                num_epochs=1000, \n",
    "                delta = 8,                 \n",
    "                regularizer= None, #Regularizer_LSTM,\n",
    "                l1_ratio = l1_ratio_reg,\n",
    "                alpha = alpha_reg,     \n",
    "                early_stop = 5,\n",
    "                \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'RNN'+name+'_'+date+'_Force_after_Baseline'\n",
    "path_to_save_model = os.path.join(path_to_models,experiment_name)\n",
    "if not os.path.exists(path_to_save_model):\n",
    "            os.makedirs(path_to_save_model)\n",
    "path_to_save_model_fold = os.path.join(path_to_save_model,'fold_{}.pth'.format(fold))\n",
    "torch.save(model_force_after_base, path_to_save_model_fold) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Causal_Simple_RNN(\n",
       "  (linear): Linear(in_features=300, out_features=2, bias=True)\n",
       "  (rnn): RNN(130, 300, batch_first=True)\n",
       "  (selu): SELU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_force_after_base = torch.load(path_to_save_model_fold)\n",
    "model_force_after_base.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train EV: 0.35 \n",
      "Val EV: 0.29 \n",
      "Test EV: 0.35 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nerea/CL_UpperLimb_Control/src/trainer.py:220: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(x, device=device, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "y_hat, y_true, train_score, v_score, test_score = eval_model(xx_train_base, yy_train_base, xx_val_base, yy_val_base, xx_test_base, yy_test_base, model_force_after_base, metric = 'ev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train EV: 0.94 \n",
      "Val EV: 0.82 \n",
      "Test EV: 0.83 \n"
     ]
    }
   ],
   "source": [
    "y_hat, y_true, train_score, v_score, test_score = eval_model(xx_train_force, yy_train_force, xx_val_force, yy_val_force, xx_test_force, yy_test_force, model_force_after_base, metric = 'ev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on baseline data using force model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Causal_Simple_RNN(\n",
       "  (linear): Linear(in_features=300, out_features=2, bias=True)\n",
       "  (rnn): RNN(130, 300, batch_first=True)\n",
       "  (selu): SELU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_base_after_force = copy.deepcopy(model_force)\n",
    "\n",
    "# Flatten the parameters of the copied model\n",
    "for module in model_base_after_force.modules():\n",
    "    if isinstance(module, nn.RNNBase):\n",
    "        module.flatten_parameters()\n",
    "model_base_after_force.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nerea/CL_UpperLimb_Control/src/trainer.py:152: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.tensor(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 Train 5.5312 Val 4.0114\n",
      "Epoch 001 Train 3.6615 Val 3.2763\n",
      "Epoch 002 Train 3.1011 Val 2.9771\n",
      "Epoch 003 Train 2.7750 Val 2.8055\n",
      "Epoch 004 Train 2.5446 Val 2.6957\n",
      "Epoch 005 Train 2.3689 Val 2.5986\n",
      "Epoch 006 Train 2.2267 Val 2.5345\n",
      "Epoch 007 Train 2.1121 Val 2.4759\n",
      "Epoch 008 Train 2.0128 Val 2.4268\n",
      "Epoch 009 Train 1.9257 Val 2.3824\n",
      "Epoch 010 Train 1.8391 Val 2.3439\n",
      "Epoch 011 Train 1.7502 Val 2.3045\n",
      "Epoch 012 Train 1.6709 Val 2.2750\n",
      "Epoch 013 Train 1.6032 Val 2.2529\n",
      "Epoch 014 Train 1.5475 Val 2.2731\n",
      "Epoch 015 Train 1.4923 Val 2.2213\n",
      "Epoch 016 Train 1.4458 Val 2.2017\n",
      "Epoch 017 Train 1.4001 Val 2.1867\n",
      "Epoch 018 Train 1.3626 Val 2.1812\n",
      "Epoch 019 Train 1.3274 Val 2.1654\n",
      "Epoch 020 Train 1.2929 Val 2.1706\n",
      "Epoch 021 Train 1.2541 Val 2.1468\n",
      "Epoch 022 Train 1.2196 Val 2.1432\n",
      "Epoch 023 Train 1.1853 Val 2.1396\n",
      "Epoch 024 Train 1.1564 Val 2.1377\n",
      "Epoch 025 Train 1.1279 Val 2.1163\n",
      "Epoch 026 Train 1.1018 Val 2.1215\n",
      "Epoch 027 Train 1.0804 Val 2.1015\n",
      "Epoch 028 Train 1.0562 Val 2.1011\n",
      "Epoch 029 Train 1.0379 Val 2.0922\n",
      "Epoch 030 Train 1.0203 Val 2.0728\n",
      "Epoch 031 Train 0.9958 Val 2.0656\n",
      "Epoch 032 Train 0.9710 Val 2.0559\n",
      "Epoch 033 Train 0.9521 Val 2.0660\n",
      "Epoch 034 Train 0.9370 Val 2.0342\n",
      "Epoch 035 Train 0.9200 Val 2.0473\n",
      "Epoch 036 Train 0.9004 Val 2.0329\n",
      "Epoch 037 Train 0.8892 Val 2.0243\n",
      "Epoch 038 Train 0.8720 Val 2.0132\n",
      "Epoch 039 Train 0.8607 Val 2.0458\n",
      "Epoch 040 Train 0.8451 Val 2.0263\n",
      "Epoch 041 Train 0.8301 Val 1.9985\n",
      "Epoch 042 Train 0.8120 Val 2.0053\n",
      "Epoch 043 Train 0.7976 Val 2.0043\n",
      "Epoch 044 Train 0.7853 Val 2.0032\n",
      "Epoch 045 Train 0.7714 Val 2.0092\n",
      "Epoch 046 Train 0.7596 Val 1.9974\n",
      "Epoch 047 Train 0.7491 Val 1.9968\n",
      "Epoch 048 Train 0.7363 Val 2.0125\n",
      "Epoch 049 Train 0.7257 Val 1.9925\n",
      "Epoch 050 Train 0.7195 Val 1.9880\n",
      "Epoch 051 Train 0.7061 Val 2.0021\n",
      "Epoch 052 Train 0.6934 Val 1.9758\n",
      "Epoch 053 Train 0.6802 Val 2.0173\n",
      "Epoch 054 Train 0.6709 Val 1.9902\n",
      "Epoch 055 Train 0.6610 Val 1.9823\n",
      "Epoch 056 Train 0.6517 Val 1.9892\n",
      "Decrease LR\n",
      "Epoch 057 Train 0.6414 Val 1.9899\n",
      "Epoch 058 Train 0.6287 Val 1.9902\n",
      "Epoch 059 Train 0.6244 Val 1.9929\n",
      "Epoch 060 Train 0.6202 Val 1.9745\n",
      "Epoch 061 Train 0.6134 Val 1.9731\n",
      "Epoch 062 Train 0.6075 Val 1.9871\n",
      "Epoch 063 Train 0.6029 Val 2.0056\n",
      "Epoch 064 Train 0.5959 Val 1.9899\n",
      "Epoch 065 Train 0.5900 Val 1.9949\n",
      "Decrease LR\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = \\\n",
    "    train_model(model_base_after_force, xx_train_base,yy_train_base,\n",
    "                xx_val_base, \n",
    "                yy_val_base,\n",
    "                lr= lr,\n",
    "                lr_step_size=lr_step_size,\n",
    "                lr_gamma= lr_gamma,\n",
    "                sequence_length_LSTM=seq_length_LSTM,\n",
    "                batch_size_train = batch_size_train,\n",
    "                batch_size_val = batch_size_val,\n",
    "                num_epochs=1000, \n",
    "                delta = 8,                 \n",
    "                regularizer= None, #Regularizer_LSTM,\n",
    "                l1_ratio = l1_ratio_reg,\n",
    "                alpha = alpha_reg,     \n",
    "                early_stop = 5,\n",
    "                \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'RNN'+name+'_'+date+'_Baseline_after_Force'\n",
    "path_to_save_model = os.path.join(path_to_models,experiment_name)\n",
    "if not os.path.exists(path_to_save_model):\n",
    "            os.makedirs(path_to_save_model)\n",
    "path_to_save_model_fold = os.path.join(path_to_save_model,'fold_{}.pth'.format(fold))\n",
    "torch.save(model_base_after_force, path_to_save_model_fold) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Causal_Simple_RNN(\n",
       "  (linear): Linear(in_features=300, out_features=2, bias=True)\n",
       "  (rnn): RNN(130, 300, batch_first=True)\n",
       "  (selu): SELU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_base_after_force = torch.load(path_to_save_model_fold)\n",
    "model_base_after_force.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train EV: 0.59 \n",
      "Val EV: 0.57 \n",
      "Test EV: 0.58 \n"
     ]
    }
   ],
   "source": [
    "y_hat, y_true, train_score, v_score, test_score = eval_model(xx_train_force, yy_train_force, xx_val_force, yy_val_force, xx_test_force, yy_test_force, model_base_after_force, metric = 'ev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train EV: 0.97 \n",
      "Val EV: 0.87 \n",
      "Test EV: 0.85 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nerea/CL_UpperLimb_Control/src/trainer.py:220: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(x, device=device, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "y_hat, y_true, train_score, v_score, test_score = eval_model(xx_train_base, yy_train_base, xx_val_base, yy_val_base, xx_test_base, yy_test_base, model_base_after_force, metric = 'ev')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sinthlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
