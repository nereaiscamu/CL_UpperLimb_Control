{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Hypernetworks Sequential Learning\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this section, we explore the application of hypernetworks to manage sequential learning tasks using neural decoders. The goal is to evaluate the performance of a hypernetwork-based model across different tasksâ€”namely, a baseline task and an adaptation task that involves a force perturbation. The comparison involves assessing the explained variance across training, validation, and test datasets to determine how well the hypernetwork retains knowledge of the baseline task while learning the adaptation task.\n",
    "\n",
    "## Key Objectives\n",
    "\n",
    "- **Model Performance Analysis**:\n",
    "  - Evaluate the explained variance of the model trained with hypernetworks on both the baseline and adaptation tasks.\n",
    "  - Compare the model's performance across training, validation, and test datasets for each task.\n",
    "\n",
    "- **Task Inference and Transfer Learning**:\n",
    "  - Examine how effectively the hypernetwork generalizes when switching between tasks.\n",
    "  - Understand the impact of sequential learning on the model's ability to maintain performance across previously learned tasks.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "We first train a hypernetwork to output the parameters of a neural decoder model for each task. The tasks are presented sequentially to simulate a continual learning scenario:\n",
    "\n",
    "1. **Baseline Task**: Training the model on the baseline data.\n",
    "2. **Adaptation Task**: Training the model on force perturbation data after the baseline task.\n",
    "\n",
    "The explained variance is calculated for each task using the respective model parameters generated by the hypernetwork. This metric helps assess how well the model captures the variability in the data, providing insights into the quality of the learned representations for each task.\n",
    "\n",
    "## Visualization\n",
    "\n",
    "The results below show the explained variance scores for each dataset (training, validation, and test) when using the parameters generated for the baseline task and the adaptation task. This analysis reveals the model's ability to adapt and transfer knowledge across tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nerea/anaconda3/envs/sinthlab/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Imports from other modules and packages in the project\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.helpers import *\n",
    "from src.visualize import *\n",
    "from src.trainer import *\n",
    "from src.trainer_hnet import *\n",
    "from src.regularizers import *\n",
    "from Models.models import *\n",
    "from Models.SimpleRNN_NC import SimpleRNN_NC\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import *\n",
    "from copy import deepcopy\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "from hypnettorch.hnets import HyperNetInterface\n",
    "from hypnettorch.hnets import HMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert between numpy arrays and tensors\n",
    "to_t = lambda array: torch.tensor(array, device='cpu', dtype=dtype)  #device\n",
    "to_t_eval =  lambda array: torch.tensor(array, device='cuda', dtype=dtype)  #device\n",
    "from_t = lambda tensor: tensor.to(\"cpu\").detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Chewie'\n",
    "date = '1007'\n",
    "fold = 4\n",
    "target_variable = 'vel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Helper functions for plotting (run this cell!)\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "# initialize a color palette for plotting\n",
    "palette = sns.xkcd_palette([\"windows blue\",\n",
    "                            \"red\",\n",
    "                            \"medium green\",\n",
    "                            \"dusty purple\",\n",
    "                            \"orange\",\n",
    "                            \"amber\",\n",
    "                            \"clay\",\n",
    "                            \"pink\",\n",
    "                            \"greyish\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_t_eval =  lambda array: torch.tensor(array, device=device, dtype=dtype)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../Data/Processed_Data/Tidy_'+name+'_'+date+'.pkl'\n",
    "\n",
    "with open(data_path, 'rb') as file:\n",
    "    tidy_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df = tidy_df.loc[tidy_df['epoch'] == 'BL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_df =  tidy_df.loc[tidy_df['epoch'] == 'AD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to consider only the trials for which the monkey has already adapted to the perturbation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_keep = force_df.id.unique()[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline subset has a total of 170 trials, whereas the perturbation one contains 201 trials, we can for now try to remove the first 50 trials from the perturbation subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_df = force_df.loc[force_df.id.isin(ids_to_keep)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Get train-val-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train trials 109\n",
      "Test trials  34\n",
      "Val trials 27\n",
      "We are testing the optimization method on fold  4\n"
     ]
    }
   ],
   "source": [
    "xx_train_base, yy_train_base, xx_val_base, yy_val_base,\\\n",
    "      xx_test_base, yy_test_base, info_train_base, info_val_base,\\\n",
    "          info_test_base, list_mins_base, \\\n",
    "            list_maxs_base= get_dataset(baseline_df, fold, target_variable= target_variable, no_outliers = False, force_data = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train trials 97\n",
      "Test trials  30\n",
      "Val trials 24\n",
      "We are testing the optimization method on fold  4\n"
     ]
    }
   ],
   "source": [
    "xx_train_force, yy_train_force, xx_val_force, yy_val_force,\\\n",
    "      xx_test_force, yy_test_force, info_train_force, info_val_force,\\\n",
    "          info_test_force,  list_mins_force, \\\n",
    "            list_maxs_force = get_dataset(force_df, fold, target_variable= target_variable, no_outliers = False, force_data = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train trials 211\n",
      "Test trials  66\n",
      "Val trials 53\n",
      "We are testing the optimization method on fold  4\n"
     ]
    }
   ],
   "source": [
    "xx_train_all, yy_train_all, xx_val_all, yy_val_all, \\\n",
    "    xx_test_all, yy_test_all, info_train_all, \\\n",
    "    info_val_all, info_test_all,  list_mins_all,\\\n",
    "          list_maxs_all = get_dataset(tidy_df,fold, target_variable= target_variable, no_outliers = False\n",
    "                                      , force_data = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify that we want our tensors on the GPU and in float32\n",
    "device = torch.device('cuda:0') #suposed to be cuda\n",
    "#device = torch.device('cpu') \n",
    "dtype = torch.float32\n",
    "path_to_models = './Models/Models_Force'\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)  # If using CUDA\n",
    "\n",
    "num_dim_output = yy_train_base.shape[2]\n",
    "num_features = xx_train_base.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f93c8792b10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_dim_output = yy_train_base.shape[2]\n",
    "num_features = xx_train_base.shape[2]\n",
    "\n",
    "# Hyperparameters LSTM class (from force model without hnet)\n",
    "# Define hyperparameters\n",
    "\n",
    "#Hyperparameters objective and regularization\n",
    "alpha_reg = 1e-5\n",
    "l1_ratio_reg = 0.5\n",
    "\n",
    "loss_function = huber_loss\n",
    "delta = 8  # hyperparameter for huber loss\n",
    "\n",
    "# Hyperparameters LSTM class\n",
    "n_hidden_units = 300\n",
    "num_layers = 1\n",
    "input_size = 49\n",
    "dropout = 0.2\n",
    "\n",
    "#Other training hyperparameters\n",
    "\n",
    "lr_gamma= 1.37 #for scheduler\n",
    "lr_step_size = 10 #for scheduler\n",
    "\n",
    "seq_length_LSTM= 19\n",
    "batch_size_train= 25\n",
    "batch_size_val = 25\n",
    "\n",
    "lr = 0.001\n",
    "beta = 1e-1\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5- Use the Causal_Simple_RNN model to get the param_shapes for the hnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_m = Causal_Simple_RNN(num_features=num_features, \n",
    "                    hidden_units= n_hidden_units, \n",
    "                    num_layers = num_layers, \n",
    "                    out_dims = num_dim_output, ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_shapes = [p.shape for p in list(template_m.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 300]),\n",
       " torch.Size([2]),\n",
       " torch.Size([300, 130]),\n",
       " torch.Size([300, 300]),\n",
       " torch.Size([300]),\n",
       " torch.Size([300])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created MLP Hypernet.\n",
      "Hypernetwork with 1822961 weights and 130202 outputs (compression ratio: 14.00).\n",
      "The network consists of 1822945 unconditional weights (1822945 internally maintained) and 16 conditional weights (16 internally maintained).\n"
     ]
    }
   ],
   "source": [
    "num_conditions = 2 #here only 2 tasks\n",
    "size_task_embedding = 8 #to check if the best one\n",
    "\n",
    "hnet = HMLP(param_shapes, uncond_in_size=0,\n",
    "             cond_in_size=size_task_embedding,\n",
    "            layers=[13], \n",
    "            num_cond_embs=num_conditions).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in hnet.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_test = hnet(cond_id = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_ = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_Main_Model(num_features= num_features, hnet_output = w_test,  hidden_size = n_hidden_units,\n",
    "                            num_layers= num_layers,out_dims=num_dim_output,  \n",
    "                            dropout= dropout,  LSTM_ = LSTM_).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6- Apply initialization to the hnet following the recommendations of hypnettorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnet.apply_hyperfan_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7- Train sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task id:  0\n",
      "Train trials 109\n",
      "Test trials  34\n",
      "Val trials 27\n",
      "We are testing the optimization method on fold  4\n",
      "Epoch 000 Train 1.8553 Val 1.9242\n",
      "Epoch 001 Train 0.7885 Val 1.9127\n",
      "Epoch 002 Train 0.6880 Val 1.8850\n",
      "Epoch 003 Train 0.6238 Val 1.7833\n",
      "Epoch 004 Train 0.5926 Val 1.8386\n",
      "Epoch 005 Train 0.5823 Val 1.8115\n",
      "Epoch 006 Train 0.5543 Val 1.8373\n",
      "Epoch 007 Train 0.5512 Val 1.7546\n",
      "Epoch 008 Train 0.5390 Val 1.8162\n",
      "Epoch 009 Train 0.5284 Val 1.8448\n",
      "Epoch 010 Train 0.7514 Val 1.7675\n",
      "Epoch 011 Train 0.5463 Val 1.6786\n",
      "Epoch 012 Train 0.5265 Val 1.7282\n",
      "Epoch 013 Train 0.5154 Val 1.7785\n",
      "Epoch 014 Train 0.5063 Val 1.7583\n",
      "Epoch 015 Train 0.4970 Val 1.8964\n",
      "Decrease LR\n",
      "Epoch 016 Train 0.5035 Val 1.7811\n",
      "Epoch 017 Train 0.4530 Val 1.7924\n",
      "Epoch 018 Train 0.4458 Val 1.7349\n",
      "Epoch 019 Train 0.4461 Val 1.7606\n",
      "Epoch 020 Train 0.4591 Val 1.7650\n",
      "Decrease LR\n",
      "Task id:  1\n",
      "Train trials 97\n",
      "Test trials  30\n",
      "Val trials 24\n",
      "We are testing the optimization method on fold  4\n",
      "Epoch 000 Train 1.5235 Val 2.0802\n",
      "Epoch 001 Train 0.6523 Val 1.9881\n",
      "Epoch 002 Train 0.5364 Val 2.0632\n",
      "Epoch 003 Train 0.4778 Val 1.9879\n",
      "Epoch 004 Train 0.4484 Val 2.0459\n",
      "Epoch 005 Train 0.4399 Val 2.0774\n",
      "Epoch 006 Train 0.4177 Val 2.1130\n",
      "Epoch 007 Train 0.4111 Val 2.0690\n",
      "Epoch 008 Train 0.4042 Val 2.0525\n",
      "Epoch 009 Train 0.3936 Val 2.0806\n",
      "Epoch 010 Train 0.4398 Val 2.0158\n",
      "Epoch 011 Train 0.4024 Val 2.0024\n",
      "Epoch 012 Train 0.3929 Val 2.0423\n",
      "Epoch 013 Train 0.3841 Val 2.0323\n",
      "Epoch 014 Train 0.3809 Val 2.0067\n",
      "Decrease LR\n",
      "Epoch 015 Train 0.3808 Val 2.0700\n",
      "Epoch 016 Train 0.3374 Val 2.0302\n",
      "Epoch 017 Train 0.3314 Val 2.0028\n",
      "Epoch 018 Train 0.3340 Val 2.0779\n",
      "Epoch 019 Train 0.3314 Val 2.0806\n",
      "Decrease LR\n"
     ]
    }
   ],
   "source": [
    "task_names = ['Baseline', 'Adaptation']\n",
    "task_data = [baseline_df, force_df]\n",
    "\n",
    "calc_reg = False\n",
    "task_id = 0\n",
    "\n",
    "train_losses  = {}\n",
    "val_losses = {}\n",
    "best_W = {}\n",
    "\n",
    "for name, dataset_ in zip(task_names, task_data):\n",
    "\n",
    "    print('Task id: ', task_id)\n",
    "    if task_id >0:\n",
    "        calc_reg = True\n",
    "\n",
    "    # Set up the optimizer with the specified learning rate\n",
    "    optimizer = torch.optim.Adam(hnet.internal_params, lr=lr)\n",
    "\n",
    "    # Set up a learning rate scheduler\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, \n",
    "                                    step_size=lr_step_size, \n",
    "                                    gamma=lr_gamma)\n",
    "    \n",
    "    # Generate feature and target matrices\n",
    "    x_train, y_train, x_val, y_val, \\\n",
    "    x_test, y_test, info_train, \\\n",
    "    info_val, info_test,  list_mins,\\\n",
    "          list_maxs = get_dataset(dataset_,fold,\n",
    "                                    target_variable= target_variable,\n",
    "                                    no_outliers = False, \n",
    "                                    force_data = True)\n",
    "    \n",
    "    train_losses_, val_losses_, best_w_ =train_current_task(\n",
    "        model, \n",
    "        hnet,\n",
    "        y_train, \n",
    "        x_train,\n",
    "        y_val,\n",
    "        x_val,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        calc_reg = calc_reg,\n",
    "        cond_id = task_id,\n",
    "        lr=lr,\n",
    "        lr_step_size=5,\n",
    "        lr_gamma= lr_gamma, #0.9\n",
    "        sequence_length_LSTM = seq_length_LSTM, #15\n",
    "        batch_size_train = batch_size_train, #15\n",
    "        batch_size_val = batch_size_train, #15\n",
    "        num_epochs=1000, \n",
    "        delta = 8,\n",
    "        beta = beta,             \n",
    "        regularizer=reg_hnet,\n",
    "        l1_ratio = l1_ratio_reg, #0.5\n",
    "        alpha = alpha_reg,    \n",
    "        early_stop = 5,\n",
    "        chunks = False)\n",
    "    \n",
    "    train_losses[name] = train_losses_\n",
    "    val_losses[name] = val_losses_\n",
    "    best_W[name] = best_w_\n",
    "\n",
    "    task_id += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = ['Training', 'Validation', 'Test']\n",
    "\n",
    "data_base = [[xx_train_base, yy_train_base],\n",
    "             [xx_val_base, yy_val_base],\n",
    "             [xx_test_base, yy_test_base]]\n",
    "\n",
    "data_force = [[xx_train_force, yy_train_force],\n",
    "             [xx_val_force, yy_val_force],\n",
    "             [xx_test_force, yy_test_force]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_base = hnet(cond_id = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_force = hnet(cond_id = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance score for  Training  is :  0.9560709595680237\n",
      "Explained variance score for  Validation  is :  0.8805598616600037\n",
      "Explained variance score for  Test  is :  0.8794187903404236\n"
     ]
    }
   ],
   "source": [
    "for index, [x,y] in enumerate(data_base):\n",
    "    r2 = calc_explained_variance_mnet(x, y, W_base, model)\n",
    "    print('Explained variance score for ', subsets[index], ' is : ', r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance score for  Training  is :  0.6620354652404785\n",
      "Explained variance score for  Validation  is :  0.6564041078090668\n",
      "Explained variance score for  Test  is :  0.6413238048553467\n"
     ]
    }
   ],
   "source": [
    "for index, [x,y] in enumerate(data_force):\n",
    "    r2 = calc_explained_variance_mnet(x, y, W_base, model)\n",
    "    print('Explained variance score for ', subsets[index], ' is : ', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance score for  Training  is :  0.9389128088951111\n",
      "Explained variance score for  Validation  is :  0.8200405538082123\n",
      "Explained variance score for  Test  is :  0.849303811788559\n"
     ]
    }
   ],
   "source": [
    "for index, [x,y] in enumerate(data_force):\n",
    "    r2 = calc_explained_variance_mnet(x, y, W_force, model)\n",
    "    print('Explained variance score for ', subsets[index], ' is : ', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance score for  Training  is :  0.43957510590553284\n",
      "Explained variance score for  Validation  is :  0.3781333863735199\n",
      "Explained variance score for  Test  is :  0.35967588424682617\n"
     ]
    }
   ],
   "source": [
    "for index, [x,y] in enumerate(data_base):\n",
    "    r2 = calc_explained_variance_mnet(x, y, W_force, model)\n",
    "    print('Explained variance score for ', subsets[index], ' is : ', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base_hnet = RNN_Main_Model(num_features= num_features, hnet_output = W_base,  hidden_size = n_hidden_units,\n",
    "                            num_layers= num_layers, out_dims=num_dim_output,  \n",
    "                            dropout= dropout, LSTM_ = LSTM_).to(device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_force_hnet = RNN_Main_Model(num_features= num_features, hnet_output = W_force,  hidden_size = n_hidden_units,\n",
    "                            num_layers= num_layers, out_dims=num_dim_output,  \n",
    "                            dropout= dropout, LSTM_ = LSTM_).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_base = 'RNN_hnet_'+name+'_'+date+'_Baseline'\n",
    "# exp_force = 'RNN_hnet_'+name+'_'+date+'_Force'\n",
    "# path_base = os.path.join(path_to_models,exp_base)\n",
    "# path_force = os.path.join(path_to_models,exp_force)\n",
    "# if not os.path.exists(path_base):\n",
    "#     os.makedirs(path_base)\n",
    "# if not os.path.exists(path_force):\n",
    "#     os.makedirs(path_force)\n",
    "# path_base_fold = os.path.join(path_base,'fold_{}.pth'.format(fold))\n",
    "# path_force_fold = os.path.join(path_force,'fold_{}.pth'.format(fold))\n",
    "# torch.save(model_base_hnet, path_base_fold)\n",
    "# torch.save(model_force_hnet, path_force_fold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sinthlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
