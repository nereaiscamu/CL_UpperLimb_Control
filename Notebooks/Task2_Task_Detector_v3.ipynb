{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task detector v3\n",
    "\n",
    "This notebook aims to clean and improve the loop for training and detecting different tasks.\n",
    "\n",
    "Here the steps would be:\n",
    "- Define a set of datasets, each one from a different \"task\" using simulated data with the parameters found on the previous version of this notebook.\n",
    "- Create a loop to go through all the different sets of data, identify the task to which it belongs and either use the model already trained on the identified task to predict the velocity or keep training the hnet using a new condition id.\n",
    "\n",
    "\n",
    "Requirements:\n",
    "- Keep information about all datasets (to which task they belong to and how they have been generated)\n",
    "- Manage the storage of the different models and data used for the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nerea/anaconda3/envs/sinthlab/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "# Imports DL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import *\n",
    "from copy import deepcopy\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "from hypnettorch.hnets import HyperNetInterface\n",
    "from hypnettorch.hnets import HMLP\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "# Imports from other modules and packages in the project\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.helpers import *\n",
    "from src.visualize import *\n",
    "from src.trainer import *\n",
    "from src.trainer_hnet import * \n",
    "from src.helpers_task_detector import *\n",
    "from Models.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"c:\\\\Users\\\\nerea\\\\OneDrive\\\\Documentos\\\\EPFL_MASTER\\\\PDM\\\\Project\\\\PyalData\")\n",
    "# to change for the actual path where PyalData has been cloned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyaldata import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Chewie'\n",
    "date = '1007'\n",
    "fold = 0\n",
    "target_variable = 'vel'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data and generate the simulated datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We work on fold 0 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load pre-processed data\n",
    "data_path = '../Data/Processed_Data/Tidy_'+name+'_'+date+'.pkl'\n",
    "\n",
    "with open(data_path, 'rb') as file:\n",
    "    tidy_df = pickle.load(file)\n",
    "baseline_df = tidy_df.loc[tidy_df['epoch'] == 'BL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_data = int(baseline_df.shape[0]/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From those matrices, we will use half the data for one dataset and the other for a new one. The idea is that the model is exposed to the other half dataset and recognises the task it has already trained before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train trials 54\n",
      "Test trials  17\n",
      "Val trials 14\n",
      "We are testing the optimization method on fold  0\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_val, y_val,\\\n",
    "      x_test, y_test, info_train, info_val,\\\n",
    "          info_test, list_mins_base, \\\n",
    "            list_maxs_base= get_dataset(baseline_df.iloc[:size_data, :], \n",
    "                                            fold, \n",
    "                                            target_variable= target_variable, \n",
    "                                            no_outliers = False, \n",
    "                                            force_data = True, \n",
    "                                            std = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "datasets['Data_'+str(0)+'_1'] = x_train, y_train, x_val, y_val, x_test, y_test\n",
    "datasets['Data_'+str(0)+'_2'] = x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train trials 54\n",
      "Test trials  17\n",
      "Val trials 14\n",
      "We are testing the optimization method on fold  0\n",
      "Train trials 54\n",
      "Test trials  17\n",
      "Val trials 14\n",
      "We are testing the optimization method on fold  0\n",
      "Train trials 54\n",
      "Test trials  17\n",
      "Val trials 14\n",
      "We are testing the optimization method on fold  0\n",
      "Train trials 54\n",
      "Test trials  17\n",
      "Val trials 14\n",
      "We are testing the optimization method on fold  0\n",
      "Train trials 54\n",
      "Test trials  17\n",
      "Val trials 14\n",
      "We are testing the optimization method on fold  0\n",
      "Train trials 54\n",
      "Test trials  17\n",
      "Val trials 14\n",
      "We are testing the optimization method on fold  0\n",
      "Train trials 54\n",
      "Test trials  17\n",
      "Val trials 14\n",
      "We are testing the optimization method on fold  0\n",
      "Train trials 54\n",
      "Test trials  17\n",
      "Val trials 14\n",
      "We are testing the optimization method on fold  0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1,5):\n",
    "    data_matrix = np.vstack(baseline_df['both_rates'])\n",
    "    baseline_df_sim = baseline_df.copy()\n",
    "    if i == 1:\n",
    "        sim_data = remove_neurons(data_matrix, 30)\n",
    "    elif i==2:\n",
    "        sim_data = shuffle_neurons(data_matrix, 60)\n",
    "    elif i == 3:\n",
    "        sim_data = add_gain(data_matrix,50)\n",
    "    elif i == 4:\n",
    "        sim_data = add_gain(data_matrix,50)\n",
    "        \n",
    "    baseline_df_sim['both_rates'] = sim_data.tolist()\n",
    "    new_data = baseline_df_sim\n",
    "\n",
    "    x_train, y_train, x_val, y_val,\\\n",
    "        x_test, y_test, info_train, info_val,\\\n",
    "            info_test, list_mins_base, \\\n",
    "                list_maxs_base= get_dataset(new_data.iloc[:size_data, :], \n",
    "                                                fold, \n",
    "                                                target_variable= target_variable, \n",
    "                                                no_outliers = False, \n",
    "                                                force_data = True, \n",
    "                                                std = False)\n",
    "    datasets['Data_'+str(i)+'_1'] = [x_train, y_train, x_val, y_val, x_test, y_test,]\n",
    "\n",
    "    x_train, y_train, x_val, y_val,\\\n",
    "        x_test, y_test, info_train, info_val,\\\n",
    "            info_test, list_mins_base, \\\n",
    "                list_maxs_base= get_dataset(new_data.iloc[size_data:, :], \n",
    "                                                fold, \n",
    "                                                target_variable= target_variable, \n",
    "                                                no_outliers = False, \n",
    "                                                force_data = True, \n",
    "                                                std = False)\n",
    "    datasets['Data_'+str(i)+'_2'] = [x_train, y_train, x_val, y_val, x_test, y_test,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../Data\"\n",
    "path_to_save_data = os.path.join(data_dir, 'Processed_Data', 'Detector_Ddatasets_'+name+'_'+str(date)+'.pkl')\n",
    "\n",
    "# Pickle the data and save it to file\n",
    "with open(path_to_save_data, 'wb') as handle:\n",
    "    pickle.dump(datasets, handle, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle the datasets to have a random order for your task detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming `datasets` is your dictionary\n",
    "keys_list = list(datasets.keys())\n",
    "random.shuffle(keys_list)\n",
    "\n",
    "shuffled_sets = datasets #{key: datasets[key] for key in keys_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Data_0_1', 'Data_0_2', 'Data_1_1', 'Data_1_2', 'Data_2_1', 'Data_2_2', 'Data_3_1', 'Data_3_2', 'Data_4_1', 'Data_4_2'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_sets.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model and hyperparameters definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd808ab6fd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify that we want our tensors on the GPU and in float32\n",
    "device = torch.device('cuda:0') #suposed to be cuda\n",
    "#device = torch.device('cpu') \n",
    "dtype = torch.float32\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)  # If using CUDA\n",
    "\n",
    "# Define hyperparameters\n",
    "\n",
    "#Hyperparameters objective and regularization\n",
    "alpha_reg = 1e-5\n",
    "l1_ratio_reg = 0.5\n",
    "\n",
    "lr = 0.00001\n",
    "loss_function = huber_loss\n",
    "delta = 8  # hyperparameter for huber loss\n",
    "\n",
    "# Hyperparameters RNN class\n",
    "hidden_units = 300\n",
    "num_layers = 1\n",
    "input_size = 49\n",
    "dropout = 0.2\n",
    "\n",
    "#Other training hyperparameters\n",
    "\n",
    "lr_gamma= 1.37 #for scheduler\n",
    "lr_step_size = 10 #for scheduler\n",
    "\n",
    "seq_length_LSTM= 19\n",
    "batch_size_train= 25\n",
    "batch_size_val = 25\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Template x_train and y_train to get the dimensions of the matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = datasets['Data_0_1'][0].shape[2]\n",
    "num_dim_output = datasets['Data_0_1'][1].shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Define task detector model\n",
    "task_detector_model =  Causal_Simple_RNN(num_features=num_features, \n",
    "                hidden_units= hidden_units, \n",
    "                num_layers = num_layers, \n",
    "                out_dims = num_dim_output,\n",
    "                dropout = dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created MLP Hypernet.\n",
      "Hypernetwork with 1823425 weights and 130202 outputs (compression ratio: 14.00).\n",
      "The network consists of 1822945 unconditional weights (1822945 internally maintained) and 480 conditional weights (480 internally maintained).\n"
     ]
    }
   ],
   "source": [
    "#### Defining the template, main and hnet models and initializing them\n",
    "template_m = Causal_Simple_RNN(num_features=num_features, \n",
    "                    hidden_units= hidden_units, \n",
    "                    num_layers = num_layers, \n",
    "                    out_dims = num_dim_output, ).to(device)\n",
    "\n",
    "param_shapes = [p.shape for p in list(template_m.parameters())]\n",
    "\n",
    "num_conditions = 60 # we want more possible conditions than what we can reach\n",
    "size_task_embedding = 8 #to check if the best one\n",
    "\n",
    "hnet = HMLP(param_shapes, uncond_in_size=0,\n",
    "             cond_in_size=size_task_embedding,\n",
    "            layers=[13], \n",
    "            num_cond_embs=num_conditions).to(device)\n",
    "\n",
    "for param in hnet.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "hnet.apply_hyperfan_init()\n",
    "\n",
    "w_test = hnet(cond_id = 0)\n",
    "\n",
    "LSTM_ = False\n",
    "\n",
    "model = RNN_Main_Model(num_features= num_features, hnet_output = w_test,  hidden_size = hidden_units,\n",
    "                            num_layers= num_layers,out_dims=num_dim_output,  \n",
    "                            dropout= dropout,  LSTM_ = LSTM_).to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the optimizer with the specified learning rate\n",
    "optimizer = torch.optim.Adam(hnet.internal_params, lr=lr)\n",
    "\n",
    "# Set up a learning rate scheduler\n",
    "scheduler = lr_scheduler.StepLR(optimizer, \n",
    "                                step_size=lr_step_size, \n",
    "                                gamma=lr_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From here all in the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "thrs = 0.8\n",
    "calc_reg = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_tasks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on the first task!\n",
      "Task_id for this task is  0\n",
      "Decrease LR\n",
      "Decrease LR\n",
      "56\n",
      "Train R2: 0.96 \n",
      "Val R2: 0.86 \n",
      "Test R2: 0.86 \n",
      "R2 for the task 0  is  0.8615813205999203\n",
      "Task learned without issues.\n",
      "Training now on the hnet\n",
      "Decrease LR\n",
      "Decrease LR\n",
      "R2 for the HNET on Task  0  is  0.8127052783966064\n",
      "[['Data_0_1', 0]]\n",
      "Train R2: 0.96 \n",
      "Val R2: 0.86 \n",
      "Test R2: 0.86 \n",
      "This data comes from a known task. \n",
      "Task_id for this task is  0\n",
      "R2 for the HNET on task 0  is  0.8174643814563751\n",
      "[['Data_0_1', 0], ['Data_0_2', 0]]\n",
      "Train R2: 0.75 \n",
      "Val R2: 0.68 \n",
      "Test R2: 0.68 \n",
      "This data comes from a different task !\n",
      "max id has changed to  1\n",
      "Task_id for this task is  1\n",
      "Decrease LR\n",
      "Decrease LR\n",
      "59\n",
      "Train R2: 0.96 \n",
      "Val R2: 0.85 \n",
      "Test R2: 0.85 \n",
      "R2 for the task 1  is  0.8494051714701392\n",
      "Task learned without issues.\n",
      "Training now on the hnet\n",
      "Decrease LR\n",
      "Decrease LR\n",
      "R2 for the HNET on Task  1  is  0.7684815526008606\n",
      "[['Data_0_1', 0], ['Data_0_2', 0], ['Data_1_1', 1]]\n",
      "Train R2: 0.68 \n",
      "Val R2: 0.74 \n",
      "Test R2: 0.68 \n",
      "Train R2: 0.85 \n",
      "Val R2: 0.87 \n",
      "Test R2: 0.86 \n",
      "This data comes from a known task. \n",
      "Task_id for this task is  1\n",
      "R2 for the HNET on task 1  is  0.7915137410163879\n",
      "[['Data_0_1', 0], ['Data_0_2', 0], ['Data_1_1', 1], ['Data_1_2', 1]]\n",
      "Train R2: 0.13 \n",
      "Val R2: 0.01 \n",
      "Test R2: 0.08 \n",
      "Train R2: 0.29 \n",
      "Val R2: 0.15 \n",
      "Test R2: 0.22 \n",
      "This data comes from a different task !\n",
      "max id has changed to  2\n",
      "Task_id for this task is  2\n",
      "Decrease LR\n",
      "Decrease LR\n",
      "40\n",
      "Train R2: 0.96 \n",
      "Val R2: 0.87 \n",
      "Test R2: 0.88 \n",
      "R2 for the task 2  is  0.8699715820425425\n",
      "Task learned without issues.\n",
      "Training now on the hnet\n",
      "Decrease LR\n",
      "Decrease LR\n",
      "R2 for the HNET on Task  2  is  0.8056795001029968\n",
      "[['Data_0_1', 0], ['Data_0_2', 0], ['Data_1_1', 1], ['Data_1_2', 1], ['Data_2_1', 2]]\n",
      "Train R2: 0.13 \n",
      "Val R2: 0.05 \n",
      "Test R2: 0.06 \n",
      "Train R2: 0.29 \n",
      "Val R2: 0.29 \n",
      "Test R2: 0.25 \n",
      "Train R2: 0.86 \n",
      "Val R2: 0.86 \n",
      "Test R2: 0.88 \n",
      "This data comes from a known task. \n",
      "Task_id for this task is  2\n",
      "R2 for the HNET on task 2  is  0.8088649809360504\n",
      "[['Data_0_1', 0], ['Data_0_2', 0], ['Data_1_1', 1], ['Data_1_2', 1], ['Data_2_1', 2], ['Data_2_2', 2]]\n",
      "Train R2: 0.02 \n",
      "Val R2: 0.15 \n",
      "Test R2: -0.09 \n",
      "Train R2: 0.25 \n",
      "Val R2: 0.20 \n",
      "Test R2: 0.25 \n",
      "Train R2: -0.51 \n",
      "Val R2: -0.43 \n",
      "Test R2: -0.38 \n",
      "This data comes from a different task !\n",
      "max id has changed to  3\n",
      "Task_id for this task is  3\n",
      "Decrease LR\n",
      "Decrease LR\n",
      "62\n",
      "Train R2: 0.96 \n",
      "Val R2: 0.86 \n",
      "Test R2: 0.86 \n",
      "R2 for the task 3  is  0.8621610001084504\n",
      "Task learned without issues.\n",
      "Training now on the hnet\n",
      "Decrease LR\n",
      "Decrease LR\n",
      "R2 for the HNET on Task  3  is  0.8170817196369171\n",
      "[['Data_0_1', 0], ['Data_0_2', 0], ['Data_1_1', 1], ['Data_1_2', 1], ['Data_2_1', 2], ['Data_2_2', 2], ['Data_3_1', 3]]\n",
      "Train R2: -0.14 \n",
      "Val R2: 0.02 \n",
      "Test R2: 0.04 \n",
      "Train R2: 0.12 \n",
      "Val R2: 0.28 \n",
      "Test R2: 0.22 \n",
      "Train R2: -0.54 \n",
      "Val R2: -0.21 \n",
      "Test R2: -0.44 \n",
      "Train R2: 0.82 \n",
      "Val R2: 0.82 \n",
      "Test R2: 0.87 \n",
      "This data comes from a known task. \n",
      "Task_id for this task is  3\n",
      "R2 for the HNET on task 3  is  0.8123648166656494\n",
      "[['Data_0_1', 0], ['Data_0_2', 0], ['Data_1_1', 1], ['Data_1_2', 1], ['Data_2_1', 2], ['Data_2_2', 2], ['Data_3_1', 3], ['Data_3_2', 3]]\n",
      "Train R2: 0.73 \n",
      "Val R2: 0.63 \n",
      "Test R2: 0.69 \n",
      "Train R2: 0.38 \n",
      "Val R2: 0.18 \n",
      "Test R2: 0.34 \n",
      "Train R2: -0.88 \n",
      "Val R2: -0.51 \n",
      "Test R2: -0.81 \n",
      "Train R2: 0.03 \n",
      "Val R2: 0.00 \n",
      "Test R2: -0.15 \n",
      "This data comes from a different task !\n",
      "max id has changed to  4\n",
      "Task_id for this task is  4\n",
      "Decrease LR\n",
      "Decrease LR\n",
      "85\n",
      "Train R2: 0.96 \n",
      "Val R2: 0.85 \n",
      "Test R2: 0.85 \n",
      "R2 for the task 4  is  0.8546780199102701\n",
      "Task learned without issues.\n",
      "Training now on the hnet\n",
      "Decrease LR\n",
      "Decrease LR\n",
      "R2 for the HNET on Task  4  is  0.7747108042240143\n",
      "[['Data_0_1', 0], ['Data_0_2', 0], ['Data_1_1', 1], ['Data_1_2', 1], ['Data_2_1', 2], ['Data_2_2', 2], ['Data_3_1', 3], ['Data_3_2', 3], ['Data_4_1', 4]]\n",
      "Train R2: 0.64 \n",
      "Val R2: 0.60 \n",
      "Test R2: 0.61 \n",
      "Train R2: 0.20 \n",
      "Val R2: 0.43 \n",
      "Test R2: 0.32 \n",
      "Train R2: -0.78 \n",
      "Val R2: -0.63 \n",
      "Test R2: -1.01 \n",
      "Train R2: 0.01 \n",
      "Val R2: -0.06 \n",
      "Test R2: -0.00 \n",
      "Train R2: 0.84 \n",
      "Val R2: 0.86 \n",
      "Test R2: 0.87 \n",
      "This data comes from a known task. \n",
      "Task_id for this task is  4\n",
      "R2 for the HNET on task 4  is  0.8002957999706268\n",
      "[['Data_0_1', 0], ['Data_0_2', 0], ['Data_1_1', 1], ['Data_1_2', 1], ['Data_2_1', 2], ['Data_2_2', 2], ['Data_3_1', 3], ['Data_3_2', 3], ['Data_4_1', 4], ['Data_4_2', 4]]\n"
     ]
    }
   ],
   "source": [
    "for s in shuffled_sets.keys():\n",
    "\n",
    "    #### Load data\n",
    "    x_train, y_train, x_val, y_val, x_test, y_test = shuffled_sets[s]\n",
    "\n",
    "    #################################################\n",
    "    ####### Define max_id and model path ############\n",
    "    #################################################\n",
    "\n",
    "    path_recog_models = '../Models/Models_Task_Recognition_Loop'\n",
    "    # Check if the directory exists, if not, create it\n",
    "    if not os.path.exists(path_recog_models):\n",
    "        os.makedirs(path_recog_models)\n",
    "\n",
    "    trained_detectors = np.sort(os.listdir(path_recog_models))\n",
    "\n",
    "    r2_list = []\n",
    "    \n",
    "    for i,m in enumerate(trained_detectors):\n",
    "        model_i = torch.load(os.path.join(path_recog_models, m)).to(device)\n",
    "        model_i.eval()\n",
    "        _, _, _, r2_i,_ = eval_model(x_train, \n",
    "                                    y_train, \n",
    "                                    x_val, \n",
    "                                    y_val,\n",
    "                                    x_test, \n",
    "                                    y_test,\n",
    "                                    model_i, \n",
    "                                    metric = 'r2')\n",
    "        r2_list.append(r2_i)\n",
    "\n",
    "    if not r2_list:\n",
    "        max_id = 0\n",
    "        task_id = 0\n",
    "        predicted_tasks.append([s,task_id])\n",
    "\n",
    "        print('Training on the first task!')\n",
    "        print('Task_id for this task is ', task_id)\n",
    "        #Define the task detector model\n",
    "        task_detector_i =  Causal_Simple_RNN(num_features=num_features, \n",
    "                    hidden_units= hidden_units, \n",
    "                    num_layers = num_layers, \n",
    "                    out_dims = num_dim_output,\n",
    "                    dropout = dropout).to(device)\n",
    "        # Training the task detector model\n",
    "        train_losses, val_losses = \\\n",
    "            train_model(task_detector_i, \n",
    "                        x_train, \n",
    "                        y_train, \n",
    "                        x_val, \n",
    "                        y_val,\n",
    "                        lr=  0.00001,\n",
    "                        lr_step_size=lr_step_size,\n",
    "                        lr_gamma= lr_gamma,\n",
    "                        sequence_length_LSTM=seq_length_LSTM,\n",
    "                        batch_size_train = batch_size_train,\n",
    "                        batch_size_val = batch_size_val,\n",
    "                        num_epochs=1000, \n",
    "                        delta = 8,                 \n",
    "                        regularizer= Regularizer_RNN, \n",
    "                        l1_ratio = l1_ratio_reg,\n",
    "                        alpha = alpha_reg,     \n",
    "                        early_stop = 5)\n",
    "        # Evaluate model on first seen data\n",
    "        y_hat, y_true,train_score, v_score, test_score = eval_model( x_train, y_train,\n",
    "                                                                    x_val, y_val,\n",
    "                                                                    x_test, y_test, \n",
    "                                                                    task_detector_i, \n",
    "                                                                    metric = 'r2')\n",
    "        print('R2 for the task', task_id, ' is ', v_score)\n",
    "\n",
    "        if v_score <thrs:\n",
    "            print('ERROR, THE TASK COULD NOT BE LEARNED BY THE DETECTOR')\n",
    "            break\n",
    "        else:\n",
    "            print('Task learned without issues.')\n",
    "        # Save the trained model\n",
    "        save_model(task_detector_i, task_id, \"Models_Task_Recognition_Loop\")\n",
    "        print('Training now on the hnet')\n",
    "        train_losses_, val_losses_, best_w_ =train_current_task(\n",
    "                                                            model, \n",
    "                                                            hnet,\n",
    "                                                            y_train, \n",
    "                                                            x_train, \n",
    "                                                            y_val,\n",
    "                                                            x_val, \n",
    "                                                            optimizer,\n",
    "                                                            scheduler,\n",
    "                                                            calc_reg = calc_reg,\n",
    "                                                            cond_id = int(task_id),\n",
    "                                                            lr=0.001,\n",
    "                                                            lr_step_size=5,\n",
    "                                                            lr_gamma= lr_gamma, #0.9\n",
    "                                                            sequence_length_LSTM = seq_length_LSTM, #15\n",
    "                                                            batch_size_train = batch_size_train, #15\n",
    "                                                            batch_size_val = batch_size_train, #15\n",
    "                                                            num_epochs=1000, \n",
    "                                                            delta = 8,\n",
    "                                                            beta = 1e-1,             \n",
    "                                                            regularizer=reg_hnet,\n",
    "                                                            l1_ratio = l1_ratio_reg, #0.5\n",
    "                                                            alpha = alpha_reg,    \n",
    "                                                            early_stop = 5,\n",
    "                                                            chunks = False)\n",
    "        W_best = hnet(cond_id = task_id)\n",
    "        r2, _ = calc_explained_variance_mnet(x_val, y_val, W_best, model)\n",
    "        print('R2 for the HNET on Task ', task_id, ' is ', r2)\n",
    "        # Save the trained model\n",
    "        save_model(hnet, task_id, \"HNET_Task_Recog_Loop\")\n",
    "\n",
    "            \n",
    "    else:\n",
    "        max_id = len(trained_detectors) - 1\n",
    "        max_r2 = max(r2_list)\n",
    "\n",
    "        if max_r2 > thrs:\n",
    "\n",
    "            # Show performance on the hnet\n",
    "            print('This data comes from a known task. ')\n",
    "            task_id = np.argmax(r2_list)\n",
    "            predicted_tasks.append([s,task_id])\n",
    "            print('Task_id for this task is ', task_id)\n",
    "            W_i = hnet(cond_id = int(task_id))\n",
    "            r2,_ = calc_explained_variance_mnet(x_val, y_val, W_i, model)\n",
    "            print('R2 for the HNET on task', task_id, ' is ', r2)\n",
    "\n",
    "        else:\n",
    "            if task_id >0:\n",
    "                calc_reg = True\n",
    "            print('This data comes from a different task !')\n",
    "            max_id += 1\n",
    "            print('max id has changed to ', max_id)\n",
    "            task_id = max_id\n",
    "            predicted_tasks.append([s,task_id])\n",
    "            print('Task_id for this task is ', task_id)\n",
    "            task_detector_i =  Causal_Simple_RNN(num_features=num_features, \n",
    "                        hidden_units= hidden_units, \n",
    "                        num_layers = num_layers, \n",
    "                        out_dims = num_dim_output,\n",
    "                        dropout = dropout).to(device)\n",
    "\n",
    "            # Training the task detector model\n",
    "            train_losses, val_losses = \\\n",
    "                train_model(task_detector_i, \n",
    "                            x_train, \n",
    "                            y_train, \n",
    "                            x_val, \n",
    "                            y_val,\n",
    "                            lr=  0.00001,\n",
    "                            lr_step_size=lr_step_size,\n",
    "                            lr_gamma= lr_gamma,\n",
    "                            sequence_length_LSTM=seq_length_LSTM,\n",
    "                            batch_size_train = batch_size_train,\n",
    "                            batch_size_val = batch_size_val,\n",
    "                            num_epochs=1000, \n",
    "                            delta = 8,                 \n",
    "                            regularizer= Regularizer_RNN, \n",
    "                            l1_ratio = l1_ratio_reg,\n",
    "                            alpha = alpha_reg,     \n",
    "                            early_stop = 5)\n",
    "            # Evaluate model on first seen data\n",
    "            y_hat, y_true,train_score, v_score, test_score = eval_model( x_train, y_train,\n",
    "                                                                        x_val, y_val,\n",
    "                                                                        x_test, y_test, \n",
    "                                                                        task_detector_i, \n",
    "                                                                        metric = 'r2')\n",
    "            print('R2 for the task', task_id, ' is ', v_score)\n",
    "\n",
    "            if v_score <thrs:\n",
    "                print('ERROR, THE TASK COULD NOT BE LEARNED BY THE DETECTOR')\n",
    "                break\n",
    "            else:\n",
    "                print('Task learned without issues.')\n",
    "\n",
    "            # Save the trained model\n",
    "            save_model(task_detector_i, task_id, \"Models_Task_Recognition_Loop\")\n",
    "            print('Training now on the hnet')\n",
    "            train_losses_, val_losses_, best_w_ =train_current_task(\n",
    "                                                                model, \n",
    "                                                                hnet,\n",
    "                                                                y_train, \n",
    "                                                                x_train, \n",
    "                                                                y_val,\n",
    "                                                                x_val, \n",
    "                                                                optimizer,\n",
    "                                                                scheduler,\n",
    "                                                                calc_reg = calc_reg,\n",
    "                                                                cond_id = int(task_id),\n",
    "                                                                lr=0.001,\n",
    "                                                                lr_step_size=5,\n",
    "                                                                lr_gamma= lr_gamma, #0.9\n",
    "                                                                sequence_length_LSTM = seq_length_LSTM, #15\n",
    "                                                                batch_size_train = batch_size_train, #15\n",
    "                                                                batch_size_val = batch_size_train, #15\n",
    "                                                                num_epochs=1000, \n",
    "                                                                delta = 8,\n",
    "                                                                beta = 1e-1,             \n",
    "                                                                regularizer=reg_hnet,\n",
    "                                                                l1_ratio = l1_ratio_reg, #0.5\n",
    "                                                                alpha = alpha_reg,    \n",
    "                                                                early_stop = 5,\n",
    "                                                                chunks = False)\n",
    "            W_best = hnet(cond_id = task_id)\n",
    "            r2,_ = calc_explained_variance_mnet(x_val, y_val, W_best, model)\n",
    "            print('R2 for the HNET on Task ', task_id, ' is ', r2)\n",
    "            # Save the trained model\n",
    "            save_model(hnet, task_id, \"HNET_Task_Recog_Loop\")\n",
    "    \n",
    "    print(predicted_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for the HNET on Task  0  is  0.8013109564781189\n",
      "R2 for the HNET on Task  1  is  0.776359349489212\n",
      "R2 for the HNET on Task  2  is  0.8031349182128906\n",
      "R2 for the HNET on Task  3  is  0.7994925677776337\n",
      "R2 for the HNET on Task  4  is  0.7980213165283203\n"
     ]
    }
   ],
   "source": [
    "for cond in range(0, task_id+1):\n",
    "    W_i = hnet(cond_id = cond)\n",
    "    data = datasets['Data_'+str(cond)+'_1']\n",
    "    r2,_ = calc_explained_variance_mnet(data[4], data[5], W_i, model)\n",
    "    print('R2 for the HNET on Task ', cond, ' is ', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for the HNET on Task  0  is  0.8016993403434753\n",
      "R2 for the HNET on Task  1  is  0.7778732776641846\n",
      "R2 for the HNET on Task  2  is  0.8064682483673096\n",
      "R2 for the HNET on Task  3  is  0.8167592585086823\n",
      "R2 for the HNET on Task  4  is  0.7851686179637909\n"
     ]
    }
   ],
   "source": [
    "for cond in range(0, task_id+1):\n",
    "    W_i = hnet(cond_id = cond)\n",
    "    data = datasets['Data_'+str(cond)+'_2']\n",
    "    r2,_ = calc_explained_variance_mnet(data[4], data[5], W_i, model)\n",
    "    print('R2 for the HNET on Task ', cond, ' is ', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sinthlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
