{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task detector v2\n",
    "\n",
    "### The first version of this task had many issues:\n",
    "- The model used for the task recognition was not well suited for this. We will now directly use the LSTM decoder to see what is the R2 when trying to predict velocity.\n",
    "- We need to further explore how each perturbation affects performance of the model to understand the optimal range of perturbations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nerea\\anaconda3\\envs\\sinthlab\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "# Imports DL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import *\n",
    "from copy import deepcopy\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "# Imports from other modules and packages in the project\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.helpers import *\n",
    "from src.visualize import *\n",
    "from src.trainer import *\n",
    "from src.trainer_hnet import * \n",
    "from Models.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"c:\\\\Users\\\\nerea\\\\OneDrive\\\\Documentos\\\\EPFL_MASTER\\\\PDM\\\\Project\\\\PyalData\")\n",
    "# to change for the actual path where PyalData has been cloned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyaldata import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Chewie'\n",
    "date = '1007'\n",
    "fold = 0\n",
    "target_variable = 'vel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load pre-processed data\n",
    "data_path = '../Data/Processed_Data/Tidy_'+name+'_'+date+'.pkl'\n",
    "\n",
    "with open(data_path, 'rb') as file:\n",
    "    tidy_df = pickle.load(file)\n",
    "baseline_df = tidy_df.loc[tidy_df['epoch'] == 'BL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Train RNN decoder on Baseline data to predict velocity\n",
    "\n",
    "#### Get train-val-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train trials 109\n",
      "Test trials  34\n",
      "Val trials 27\n",
      "We are testing the optimization method on fold  0\n"
     ]
    }
   ],
   "source": [
    "xx_train_base, yy_train_base, xx_val_base, yy_val_base,\\\n",
    "      xx_test_base, yy_test_base, info_train_base, info_val_base,\\\n",
    "          info_test_base, list_mins_base, \\\n",
    "            list_maxs_base= get_dataset(baseline_df, \n",
    "                                        fold, target_variable= target_variable, \n",
    "                                        no_outliers = False, force_data = True,\n",
    "                                        std = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_train_base, yy_train_base = xx_train_base[:20,:,:], yy_train_base[:20,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify that we want our tensors on the GPU and in float32\n",
    "#device = torch.device('cuda:0') #suposed to be cuda\n",
    "device = torch.device('cpu') \n",
    "dtype = torch.float32\n",
    "path_to_models = './Models/Models_Force'\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)  # If using CUDA\n",
    "\n",
    "num_dim_output = yy_train_base.shape[2]\n",
    "num_features = xx_train_base.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "\n",
    "#Hyperparameters objective and regularization\n",
    "alpha_reg = 1e-5\n",
    "l1_ratio_reg = 0.5\n",
    "\n",
    "lr = 0.0001 # 0.00001\n",
    "loss_function = huber_loss\n",
    "delta = 8  # hyperparameter for huber loss\n",
    "\n",
    "# Hyperparameters RNN class\n",
    "hidden_units = 300\n",
    "num_layers = 1\n",
    "dropout = 0.2\n",
    "\n",
    "#Other training hyperparameters\n",
    "\n",
    "lr_gamma= 1.37 #for scheduler\n",
    "lr_step_size = 10 #for scheduler\n",
    "\n",
    "seq_length_LSTM= 19\n",
    "batch_size_train= 10\n",
    "batch_size_val = 10\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "####### Define model\n",
    "model_base =  Causal_Simple_RNN(num_features=num_features, \n",
    "                hidden_units= hidden_units, \n",
    "                num_layers = num_layers, \n",
    "                out_dims = num_dim_output,\n",
    "                dropout = dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 Train 10.1205 Val 10.2053\n",
      "Epoch 001 Train 6.5129 Val 7.3989\n",
      "Epoch 002 Train 4.0193 Val 5.6260\n",
      "Epoch 003 Train 2.4984 Val 4.7188\n",
      "Epoch 004 Train 1.7103 Val 4.1972\n",
      "Epoch 005 Train 1.3216 Val 3.7631\n",
      "Epoch 006 Train 1.1183 Val 3.5237\n",
      "Epoch 007 Train 0.9979 Val 3.4913\n",
      "Epoch 008 Train 0.9216 Val 3.7230\n",
      "Epoch 009 Train 0.8793 Val 3.3097\n",
      "Epoch 010 Train 0.9130 Val 3.2867\n",
      "Epoch 011 Train 0.8683 Val 3.2962\n",
      "Epoch 012 Train 0.8592 Val 3.4460\n",
      "Epoch 013 Train 0.8631 Val 3.8421\n",
      "Epoch 014 Train 0.8553 Val 3.4186\n",
      "Decrease LR\n",
      "Epoch 015 Train 0.8383 Val 3.6356\n",
      "Epoch 016 Train 0.7829 Val 3.2124\n",
      "Epoch 017 Train 0.7842 Val 3.1258\n",
      "Epoch 018 Train 0.7858 Val 3.3399\n",
      "Epoch 019 Train 0.7868 Val 3.2947\n",
      "Epoch 020 Train 0.8155 Val 3.0424\n",
      "Epoch 021 Train 0.8130 Val 3.1697\n",
      "Epoch 022 Train 0.8076 Val 3.2003\n",
      "Epoch 023 Train 0.7981 Val 3.2655\n",
      "Epoch 024 Train 0.7949 Val 3.3525\n",
      "Decrease LR\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = \\\n",
    "    train_model(model_base, \n",
    "                xx_train_base,yy_train_base,\n",
    "                xx_val_base, \n",
    "                yy_val_base,\n",
    "                lr= lr,\n",
    "                lr_step_size=lr_step_size,\n",
    "                lr_gamma= lr_gamma,\n",
    "                sequence_length_LSTM=seq_length_LSTM,\n",
    "                batch_size_train = batch_size_train,\n",
    "                batch_size_val = batch_size_val,\n",
    "                num_epochs=1000, \n",
    "                delta = 8,                 \n",
    "                regularizer= Regularizer_RNN, \n",
    "                l1_ratio = l1_ratio_reg,\n",
    "                alpha = alpha_reg,     \n",
    "                early_stop = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate model on baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2: 0.95 \n",
      "Val R2: 0.82 \n",
      "Test R2: 0.84 \n"
     ]
    }
   ],
   "source": [
    "y_hat, y_true,\\\n",
    "      train_score, v_score,\\\n",
    "          test_score = eval_model(xx_train_base,\n",
    "                                   yy_train_base, \n",
    "                                   xx_val_base, \n",
    "                                   yy_val_base,\n",
    "                                   xx_test_base, \n",
    "                                   yy_test_base, \n",
    "                                   model_base, \n",
    "                                   metric = 'r2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Test different ratios and std for the simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_neurons(matrix, ratio):\n",
    "    sim_data = matrix.copy()\n",
    "    num_total_neurons = matrix.shape[1]\n",
    "    num_removed = int((ratio/100)*num_total_neurons)\n",
    "    idx_removed = random.sample(list(np.arange(0,num_total_neurons)), num_removed)\n",
    "    for i in idx_removed:\n",
    "        sim_data[:,i] = 0\n",
    "    return sim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_neurons(matrix, ratio):\n",
    "    sim_data = matrix.copy()\n",
    "    num_total_neurons = matrix.shape[1]\n",
    "    num_neurons_to_shuffle = int((ratio/100)*num_total_neurons)\n",
    "    ind_to_permute = random.sample(list(np.arange(0,num_total_neurons)), num_neurons_to_shuffle)\n",
    "    ind_to_permute = np.sort(ind_to_permute)\n",
    "    permuted_indices = np.random.permutation(ind_to_permute)\n",
    "    for i, new_i in zip(ind_to_permute, permuted_indices):\n",
    "        sim_data[:,i] = matrix[:,new_i]\n",
    "    return sim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_neurons(matrix, std_gain, std_offset):\n",
    "    sim_data1 = matrix.copy()\n",
    "    sim_data2 = matrix.copy()\n",
    "    \n",
    "    ### a) Multiplying each column by a random gain from gaussian dist.\n",
    "    gains = np.random.normal(1, std_gain, size=sim_data1.shape[1])\n",
    "    # Multiply each column of the matrix by the corresponding gain value\n",
    "    sim_data1 = sim_data1 * gains[:, np.newaxis].T\n",
    "    \n",
    "    ### b) Random offsets\n",
    "    offsets = np.random.normal(0, std_offset, size=sim_data2.shape[1])\n",
    "    # Multiply each column of the matrix by the corresponding gain value\n",
    "    sim_data2 = sim_data2 + offsets[:, np.newaxis].T\n",
    "\n",
    "    return sim_data1, sim_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = np.vstack(baseline_df['both_rates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" ratios = np.arange(0,110,10)\\nv_scores_removed = []\\nv_scores_shuffled = []\\n\\nfor r in ratios:\\n  for i in range(10):\\n      v_scores_removed_i = []\\n      v_scores_shuffled_i = []\\n      # First removed neurons\\n      sim_data = remove_neurons(data_matrix, r)\\n      baseline_df_sim = baseline_df.copy()\\n      baseline_df_sim['both_rates'] = sim_data.tolist()\\n      xx_train_sim, yy_train_sim, xx_val_sim, yy_val_sim,        xx_test_sim, yy_test_sim, info_train_sim, info_val_sim,            info_test_sim, list_mins_sim,               list_maxs_base= get_dataset(baseline_df_sim, \\n                                          fold, \\n                                          target_variable= target_variable,\\n                                          no_outliers = False, \\n                                          force_data = True, \\n                                          std = False)\\n      y_hat, y_true, train_score,        v_score, test_score = eval_model(xx_train_sim, \\n                                          yy_train_sim, \\n                                          xx_val_sim, \\n                                          yy_val_sim,                                          xx_test_sim, \\n                                          yy_test_sim,\\n                                          model_base, \\n                                          metric = 'r2')\\n      v_scores_removed_i.append(v_score)\\n\\n      # Second shuffled neurons\\n      sim_data = shuffle_neurons(data_matrix, r)\\n      baseline_df_sim = baseline_df.copy()\\n      baseline_df_sim['both_rates'] = sim_data.tolist()\\n      xx_train_sim, yy_train_sim, xx_val_sim, yy_val_sim,        xx_test_sim, yy_test_sim, info_train_sim, info_val_sim,            info_test_sim, list_mins_sim,               list_maxs_base= get_dataset(baseline_df_sim, \\n                                          fold, \\n                                          target_variable= target_variable,\\n                                          no_outliers = False, \\n                                          force_data = True, \\n                                          std = False)\\n      y_hat, y_true, train_score,        v_score, test_score = eval_model(xx_train_sim, \\n                                          yy_train_sim, \\n                                          xx_val_sim, \\n                                          yy_val_sim,                                          xx_test_sim, \\n                                          yy_test_sim,\\n                                          model_base, \\n                                          metric = 'r2')\\n      v_scores_shuffled_i.append(v_score)\\n  v_scores_removed.append(np.median(v_scores_removed_i))\\n  v_scores_shuffled.append(np.median(v_scores_shuffled_i))\\n     \""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" ratios = np.arange(0,110,10)\n",
    "v_scores_removed = []\n",
    "v_scores_shuffled = []\n",
    "\n",
    "for r in ratios:\n",
    "  for i in range(10):\n",
    "      v_scores_removed_i = []\n",
    "      v_scores_shuffled_i = []\n",
    "      # First removed neurons\n",
    "      sim_data = remove_neurons(data_matrix, r)\n",
    "      baseline_df_sim = baseline_df.copy()\n",
    "      baseline_df_sim['both_rates'] = sim_data.tolist()\n",
    "      xx_train_sim, yy_train_sim, xx_val_sim, yy_val_sim,\\\n",
    "        xx_test_sim, yy_test_sim, info_train_sim, info_val_sim,\\\n",
    "            info_test_sim, list_mins_sim, \\\n",
    "              list_maxs_base= get_dataset(baseline_df_sim, \n",
    "                                          fold, \n",
    "                                          target_variable= target_variable,\n",
    "                                          no_outliers = False, \n",
    "                                          force_data = True, \n",
    "                                          std = False)\n",
    "      y_hat, y_true, train_score,\\\n",
    "        v_score, test_score = eval_model(xx_train_sim, \n",
    "                                          yy_train_sim, \n",
    "                                          xx_val_sim, \n",
    "                                          yy_val_sim,\\\n",
    "                                          xx_test_sim, \n",
    "                                          yy_test_sim,\n",
    "                                          model_base, \n",
    "                                          metric = 'r2')\n",
    "      v_scores_removed_i.append(v_score)\n",
    "\n",
    "      # Second shuffled neurons\n",
    "      sim_data = shuffle_neurons(data_matrix, r)\n",
    "      baseline_df_sim = baseline_df.copy()\n",
    "      baseline_df_sim['both_rates'] = sim_data.tolist()\n",
    "      xx_train_sim, yy_train_sim, xx_val_sim, yy_val_sim,\\\n",
    "        xx_test_sim, yy_test_sim, info_train_sim, info_val_sim,\\\n",
    "            info_test_sim, list_mins_sim, \\\n",
    "              list_maxs_base= get_dataset(baseline_df_sim, \n",
    "                                          fold, \n",
    "                                          target_variable= target_variable,\n",
    "                                          no_outliers = False, \n",
    "                                          force_data = True, \n",
    "                                          std = False)\n",
    "      y_hat, y_true, train_score,\\\n",
    "        v_score, test_score = eval_model(xx_train_sim, \n",
    "                                          yy_train_sim, \n",
    "                                          xx_val_sim, \n",
    "                                          yy_val_sim,\\\n",
    "                                          xx_test_sim, \n",
    "                                          yy_test_sim,\n",
    "                                          model_base, \n",
    "                                          metric = 'r2')\n",
    "      v_scores_shuffled_i.append(v_score)\n",
    "  v_scores_removed.append(np.median(v_scores_removed_i))\n",
    "  v_scores_shuffled.append(np.median(v_scores_shuffled_i))\n",
    "     \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" stds_gain = np.arange(0,1.4,0.1)\\nstds_offset = np.arange(0,41,3)\\n\\nv_scores_gain = []\\nv_scores_offset = []\\nfor s_g, s_o in zip(stds_gain,stds_offset) :\\n\\n    for i in range(10):\\n        v_scores_gain_i = []\\n        v_scores_offset_i = []\\n        # First removed neurons\\n        sim_data1, sim_data2 = modify_neurons(data_matrix, s_g, s_o)\\n        baseline_df_sim = baseline_df.copy()\\n        baseline_df_sim['both_rates'] = sim_data1.tolist()\\n        xx_train_sim, yy_train_sim, xx_val_sim, yy_val_sim,          xx_test_sim, yy_test_sim, info_train_sim, info_val_sim,              info_test_sim, list_mins_sim,                 list_maxs_base= get_dataset(baseline_df_sim, \\n                                            fold, \\n                                            target_variable= target_variable,\\n                                            no_outliers = False, \\n                                            force_data = True, \\n                                            std = False)\\n        y_hat, y_true, train_score,          v_score, test_score = eval_model(xx_train_sim, \\n                                            yy_train_sim, \\n                                            xx_val_sim, \\n                                            yy_val_sim,                                            xx_test_sim, \\n                                            yy_test_sim,\\n                                            model_base, \\n                                            metric = 'r2')\\n        v_scores_gain_i.append(v_score)\\n\\n        # Second shuffled neurons\\n        baseline_df_sim = baseline_df.copy()\\n        baseline_df_sim['both_rates'] = sim_data2.tolist()\\n        xx_train_sim, yy_train_sim, xx_val_sim, yy_val_sim,          xx_test_sim, yy_test_sim, info_train_sim, info_val_sim,              info_test_sim, list_mins_sim,                 list_maxs_base= get_dataset(baseline_df_sim, \\n                                            fold, \\n                                            target_variable= target_variable,\\n                                            no_outliers = False, \\n                                            force_data = True, \\n                                            std = False)\\n        y_hat, y_true, train_score,          v_score, test_score = eval_model(xx_train_sim, \\n                                            yy_train_sim, \\n                                            xx_val_sim, \\n                                            yy_val_sim,                                            xx_test_sim, \\n                                            yy_test_sim,\\n                                            model_base, \\n                                            metric = 'r2')\\n        v_scores_offset_i.append(v_score)\\n\\n    v_scores_gain.append(np.median(v_scores_gain_i))\\n    v_scores_offset.append(np.median(v_scores_offset_i)) \""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" stds_gain = np.arange(0,1.4,0.1)\n",
    "stds_offset = np.arange(0,41,3)\n",
    "\n",
    "v_scores_gain = []\n",
    "v_scores_offset = []\n",
    "for s_g, s_o in zip(stds_gain,stds_offset) :\n",
    "\n",
    "    for i in range(10):\n",
    "        v_scores_gain_i = []\n",
    "        v_scores_offset_i = []\n",
    "        # First removed neurons\n",
    "        sim_data1, sim_data2 = modify_neurons(data_matrix, s_g, s_o)\n",
    "        baseline_df_sim = baseline_df.copy()\n",
    "        baseline_df_sim['both_rates'] = sim_data1.tolist()\n",
    "        xx_train_sim, yy_train_sim, xx_val_sim, yy_val_sim,\\\n",
    "          xx_test_sim, yy_test_sim, info_train_sim, info_val_sim,\\\n",
    "              info_test_sim, list_mins_sim, \\\n",
    "                list_maxs_base= get_dataset(baseline_df_sim, \n",
    "                                            fold, \n",
    "                                            target_variable= target_variable,\n",
    "                                            no_outliers = False, \n",
    "                                            force_data = True, \n",
    "                                            std = False)\n",
    "        y_hat, y_true, train_score,\\\n",
    "          v_score, test_score = eval_model(xx_train_sim, \n",
    "                                            yy_train_sim, \n",
    "                                            xx_val_sim, \n",
    "                                            yy_val_sim,\\\n",
    "                                            xx_test_sim, \n",
    "                                            yy_test_sim,\n",
    "                                            model_base, \n",
    "                                            metric = 'r2')\n",
    "        v_scores_gain_i.append(v_score)\n",
    "\n",
    "        # Second shuffled neurons\n",
    "        baseline_df_sim = baseline_df.copy()\n",
    "        baseline_df_sim['both_rates'] = sim_data2.tolist()\n",
    "        xx_train_sim, yy_train_sim, xx_val_sim, yy_val_sim,\\\n",
    "          xx_test_sim, yy_test_sim, info_train_sim, info_val_sim,\\\n",
    "              info_test_sim, list_mins_sim, \\\n",
    "                list_maxs_base= get_dataset(baseline_df_sim, \n",
    "                                            fold, \n",
    "                                            target_variable= target_variable,\n",
    "                                            no_outliers = False, \n",
    "                                            force_data = True, \n",
    "                                            std = False)\n",
    "        y_hat, y_true, train_score,\\\n",
    "          v_score, test_score = eval_model(xx_train_sim, \n",
    "                                            yy_train_sim, \n",
    "                                            xx_val_sim, \n",
    "                                            yy_val_sim,\\\n",
    "                                            xx_test_sim, \n",
    "                                            yy_test_sim,\n",
    "                                            model_base, \n",
    "                                            metric = 'r2')\n",
    "        v_scores_offset_i.append(v_score)\n",
    "\n",
    "    v_scores_gain.append(np.median(v_scores_gain_i))\n",
    "    v_scores_offset.append(np.median(v_scores_offset_i)) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" # Plot\\nplt.figure(figsize=(8, 6))  # Adjust the figure size as needed\\nplt.plot(ratios, v_scores_removed, marker='o', color='b', linestyle='-', label = 'Removed Neurons')\\nplt.plot(ratios, v_scores_shuffled, marker='o', color='orange', linestyle='-', label = 'Shuffled Neurons')\\n\\n# Add labels and title\\nplt.xlabel('Percentage of Neurons Removed')\\nplt.ylabel('Validation R2 Score')\\nplt.title('Model Performance on Validation Set')\\nplt.legend()\\n# Add grid lines\\nplt.grid(True)\\n\\n# Customize ticks and tick labels if needed\\n# plt.xticks(ratios)\\n# plt.yticks([0.5, 0.6, 0.7, 0.8, 0.9])\\n\\n# Show plot\\nplt.show() \""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Plot\n",
    "plt.figure(figsize=(8, 6))  # Adjust the figure size as needed\n",
    "plt.plot(ratios, v_scores_removed, marker='o', color='b', linestyle='-', label = 'Removed Neurons')\n",
    "plt.plot(ratios, v_scores_shuffled, marker='o', color='orange', linestyle='-', label = 'Shuffled Neurons')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Percentage of Neurons Removed')\n",
    "plt.ylabel('Validation R2 Score')\n",
    "plt.title('Model Performance on Validation Set')\n",
    "plt.legend()\n",
    "# Add grid lines\n",
    "plt.grid(True)\n",
    "\n",
    "# Customize ticks and tick labels if needed\n",
    "# plt.xticks(ratios)\n",
    "# plt.yticks([0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "# Show plot\n",
    "plt.show() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" # Plot\\nplt.figure(figsize=(8, 6))  # Adjust the figure size as needed\\nplt.plot(stds_gain, v_scores_gain, marker='o', color='g', linestyle='-', label = 'Modified Gain Neurons')\\n\\n# Add labels and title\\nplt.xlabel('Std of the gain distribution')\\nplt.ylabel('Validation R2 Score')\\nplt.title('Model Performance on Validation Set')\\nplt.legend()\\n# Add grid lines\\nplt.grid(True)\\n\\n# Customize ticks and tick labels if needed\\n# plt.xticks(ratios)\\n# plt.yticks([0.5, 0.6, 0.7, 0.8, 0.9])\\n\\n# Show plot\\nplt.show() \""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Plot\n",
    "plt.figure(figsize=(8, 6))  # Adjust the figure size as needed\n",
    "plt.plot(stds_gain, v_scores_gain, marker='o', color='g', linestyle='-', label = 'Modified Gain Neurons')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Std of the gain distribution')\n",
    "plt.ylabel('Validation R2 Score')\n",
    "plt.title('Model Performance on Validation Set')\n",
    "plt.legend()\n",
    "# Add grid lines\n",
    "plt.grid(True)\n",
    "\n",
    "# Customize ticks and tick labels if needed\n",
    "# plt.xticks(ratios)\n",
    "# plt.yticks([0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "# Show plot\n",
    "plt.show() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" # Plot\\nplt.figure(figsize=(8, 6))  # Adjust the figure size as needed\\nplt.plot(stds_offset, v_scores_offset, marker='o', color='m', linestyle='-', label = 'Modified Offset Neurons')\\n\\n# Add labels and title\\nplt.xlabel('Std of the offset distribution')\\nplt.ylabel('Validation R2 Score')\\nplt.title('Model Performance on Validation Set')\\nplt.legend()\\n# Add grid lines\\nplt.grid(True)\\n\\n# Customize ticks and tick labels if needed\\n# plt.xticks(ratios)\\n# plt.yticks([0.5, 0.6, 0.7, 0.8, 0.9])\\n\\n# Show plot\\nplt.show() \""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Plot\n",
    "plt.figure(figsize=(8, 6))  # Adjust the figure size as needed\n",
    "plt.plot(stds_offset, v_scores_offset, marker='o', color='m', linestyle='-', label = 'Modified Offset Neurons')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Std of the offset distribution')\n",
    "plt.ylabel('Validation R2 Score')\n",
    "plt.title('Model Performance on Validation Set')\n",
    "plt.legend()\n",
    "# Add grid lines\n",
    "plt.grid(True)\n",
    "\n",
    "# Customize ticks and tick labels if needed\n",
    "# plt.xticks(ratios)\n",
    "# plt.yticks([0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "\n",
    "# Show plot\n",
    "plt.show() \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Conclusion__:\n",
    "- Single perturbations (removing neurons, shuffling neurons, adding gain or offset) for a performance of less than 60% R2 need to be:\n",
    "    - 30% shufled neurons\n",
    "    - 60% \"removed\" neurons\n",
    "    - 0.7std for gain distribution with mean 1\n",
    "    - 16-18 std for aoofset distribution with mean 0\n",
    "\n",
    "We can now either consider each perturbation a single task or mix them and create different datasets.\n",
    "\n",
    "## 3) Test algorithm with 1 of the previous perturbations\n",
    "`Reminder algorithm:`\n",
    "\n",
    "#### The following part of the code should correspond to this logic/algorithm structure:\n",
    "\n",
    "**Initialize with first task (~pretraining):**\n",
    "- 1- Load Baseline Data and set task_id to 0, and max_id to 0 too\n",
    "- 2- Train TaskDetectorRNN on Baseline Data\n",
    "- 3- Compute R2 and define threshold\n",
    "- 4- Create folder to save trained task detector models and save it with the name of the task (here 0)\n",
    "- 5- Train hnet using task_id = 0.\n",
    "\n",
    "**Continue with new data (could be the same task):**\n",
    "- 6- Load new data\n",
    "- 7- For each of the previous tasks test the trained TaskDetectorRNN and compute the R2 score. Save in a list all R2 scores.\n",
    "    - if max(r2_list) > thrs: get argmax of the list as the task_id\n",
    "        - use h_net with the task_id corresponding to the data.\n",
    "        - report results (R2,...)\n",
    "    - else:\n",
    "        - max_id += 1\n",
    "        - task_id = max_id\n",
    "        - train TaskDetectorRNN on new data and save it with its task_id\n",
    "        - check R2 > threshold --> else report ERROR.\n",
    "        - train h_net using this task_id.\n",
    "        - report results (R2,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, task_id, folder):\n",
    "    # Define the directory path\n",
    "    models_dir = \"../Models\"\n",
    "    task_models_dir = os.path.join(models_dir, folder)\n",
    "\n",
    "    # Check if the directory exists, if not, create it\n",
    "    if not os.path.exists(task_models_dir):\n",
    "        os.makedirs(task_models_dir)\n",
    "\n",
    "    # Define the file name\n",
    "    model_file_name = f\"Model_Task_{task_id}.pth\"  # Use .pth extension for PyTorch models\n",
    "\n",
    "    # Save the model\n",
    "    model_path = os.path.join(task_models_dir, model_file_name)\n",
    "\n",
    "    # Save the model using torch.save\n",
    "    torch.save(model, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = 0\n",
    "max_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_detector = copy.deepcopy(model_base)\n",
    "# Save the trained model\n",
    "save_model(task_detector, task_id, \"Models_Task_Recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2: 0.95 \n",
      "Val R2: 0.82 \n",
      "Test R2: 0.84 \n",
      "The threshold to belong to task 0 is an R2 of :  0.7404342447860889\n"
     ]
    }
   ],
   "source": [
    "y_hat, y_true,\\\n",
    "      train_score, v_score,\\\n",
    "          test_score = eval_model(xx_train_base,\n",
    "                                   yy_train_base, \n",
    "                                   xx_val_base, \n",
    "                                   yy_val_base,\n",
    "                                   xx_test_base, \n",
    "                                   yy_test_base, \n",
    "                                   task_detector, \n",
    "                                   metric = 'r2')\n",
    "r2_max = v_score\n",
    "thrs = 0.90*r2_max\n",
    "print('The threshold to belong to task 0 is an R2 of : ', thrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1- Train hypernetwork on task 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x25dee3e7a30>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "num_dim_output = yy_train_base.shape[2]\n",
    "num_features = xx_train_base.shape[2]\n",
    "\n",
    "# Hyperparameters LSTM class (from force model without hnet)\n",
    "# Define hyperparameters\n",
    "\n",
    "#Hyperparameters objective and regularization\n",
    "alpha_reg = 1e-5\n",
    "l1_ratio_reg = 0.5\n",
    "\n",
    "loss_function = huber_loss\n",
    "delta = 8  # hyperparameter for huber loss\n",
    "\n",
    "# Hyperparameters LSTM class\n",
    "n_hidden_units = 300\n",
    "num_layers = 1\n",
    "input_size = 49\n",
    "dropout = 0.2\n",
    "\n",
    "#Other training hyperparameters\n",
    "\n",
    "lr_gamma= 1.37 #for scheduler\n",
    "lr_step_size = 10 #for scheduler\n",
    "\n",
    "seq_length_LSTM= 19\n",
    "batch_size_train= 25\n",
    "batch_size_val = 25\n",
    "\n",
    "lr = 0.001\n",
    "beta = 1e-1\n",
    "\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hypnettorch.hnets import HyperNetInterface\n",
    "from hypnettorch.hnets import HMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created MLP Hypernet.\n",
      "Hypernetwork with 1823425 weights and 130202 outputs (compression ratio: 14.00).\n",
      "The network consists of 1822945 unconditional weights (1822945 internally maintained) and 480 conditional weights (480 internally maintained).\n",
      "Task id:  0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 50\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Set up a learning rate scheduler\u001b[39;00m\n\u001b[0;32m     46\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m lr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, \n\u001b[0;32m     47\u001b[0m                                 step_size\u001b[38;5;241m=\u001b[39mlr_step_size, \n\u001b[0;32m     48\u001b[0m                                 gamma\u001b[38;5;241m=\u001b[39mlr_gamma)\n\u001b[1;32m---> 50\u001b[0m train_losses_, val_losses_, best_w_ \u001b[38;5;241m=\u001b[39m\u001b[43mtrain_current_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43myy_train_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxx_train_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43myy_val_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxx_val_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcalc_reg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcalc_reg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcond_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr_step_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr_gamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr_gamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#0.9\u001b[39;49;00m\n\u001b[0;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43msequence_length_LSTM\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseq_length_LSTM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#15\u001b[39;49;00m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#15\u001b[39;49;00m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size_val\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#15\u001b[39;49;00m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreg_hnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ml1_ratio_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#0.5\u001b[39;49;00m\n\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malpha_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nerea\\OneDrive\\Documentos\\EPFL_MASTER\\PDM\\Project\\CL_UpperLimb_Control\\Notebooks\\..\\src\\trainer_hnet.py:109\u001b[0m, in \u001b[0;36mtrain_current_task\u001b[1;34m(model, hnet, y_train, x_train, y_val, x_val, optimizer, scheduler, calc_reg, cond_id, lr, lr_step_size, lr_gamma, sequence_length_LSTM, batch_size_train, batch_size_val, num_epochs, delta, beta, regularizer, l1_ratio, alpha, early_stop, LSTM_, chunks)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Iterate over batches in the loader\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_ \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m    107\u001b[0m \n\u001b[0;32m    108\u001b[0m     \u001b[38;5;66;03m# Define data for this batch\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mdata_\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     y \u001b[38;5;241m=\u001b[39m data_[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\nerea\\anaconda3\\envs\\sinthlab\\Lib\\site-packages\\torch\\cuda\\__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    291\u001b[0m     )\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    297\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "\n",
    "#### Defining the template, main and hnet models and initializing them\n",
    "template_m = Causal_Simple_RNN(num_features=num_features, \n",
    "                    hidden_units= n_hidden_units, \n",
    "                    num_layers = num_layers, \n",
    "                    out_dims = num_dim_output, ).to(device)\n",
    "\n",
    "param_shapes = [p.shape for p in list(template_m.parameters())]\n",
    "\n",
    "num_conditions = 60 # we want more possible conditions than what we can reach\n",
    "size_task_embedding = 8 #to check if the best one\n",
    "\n",
    "hnet = HMLP(param_shapes, uncond_in_size=0,\n",
    "             cond_in_size=size_task_embedding,\n",
    "            layers=[13], \n",
    "            num_cond_embs=num_conditions).to(device)\n",
    "\n",
    "for param in hnet.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "w_test = hnet(cond_id = 0)\n",
    "\n",
    "LSTM_ = False\n",
    "\n",
    "model = RNN_Main_Model(num_features= num_features, hnet_output = w_test,  hidden_size = n_hidden_units,\n",
    "                            num_layers= num_layers,out_dims=num_dim_output,  \n",
    "                            dropout= dropout,  LSTM_ = LSTM_).to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "    \n",
    "#### Apply initialization to the hnet following the recommendations of hypnettorch\n",
    "hnet.apply_hyperfan_init()\n",
    "#### Training the hnet for the 1st task\n",
    "print('Task id: ', task_id)\n",
    "\n",
    "calc_reg = False \n",
    "\n",
    "if task_id >0:\n",
    "    calc_reg = True\n",
    "\n",
    "# Set up the optimizer with the specified learning rate\n",
    "optimizer = torch.optim.Adam(hnet.internal_params, lr=lr)\n",
    "\n",
    "# Set up a learning rate scheduler\n",
    "scheduler = lr_scheduler.StepLR(optimizer, \n",
    "                                step_size=lr_step_size, \n",
    "                                gamma=lr_gamma)\n",
    "\n",
    "train_losses_, val_losses_, best_w_ =train_current_task(\n",
    "        model, \n",
    "        hnet,\n",
    "        yy_train_base, \n",
    "        xx_train_base,\n",
    "        yy_val_base,\n",
    "        xx_val_base,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        calc_reg = calc_reg,\n",
    "        cond_id = task_id,\n",
    "        lr=lr,\n",
    "        lr_step_size=5,\n",
    "        lr_gamma= lr_gamma, #0.9\n",
    "        sequence_length_LSTM = seq_length_LSTM, #15\n",
    "        batch_size_train = batch_size_train, #15\n",
    "        batch_size_val = batch_size_train, #15\n",
    "        num_epochs=1000, \n",
    "        delta = 8,\n",
    "        beta = beta,             \n",
    "        regularizer=reg_hnet,\n",
    "        l1_ratio = l1_ratio_reg, #0.5\n",
    "        alpha = alpha_reg,    \n",
    "        early_stop = 5,\n",
    "        chunks = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m W_0 \u001b[38;5;241m=\u001b[39m hnet(cond_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m r2 \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_explained_variance_mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxx_val_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myy_val_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR2 for the task\u001b[39m\u001b[38;5;124m'\u001b[39m, task_id, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m, r2)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nerea\\OneDrive\\Documentos\\EPFL_MASTER\\PDM\\Project\\CL_UpperLimb_Control\\Notebooks\\..\\src\\helpers.py:621\u001b[0m, in \u001b[0;36mcalc_explained_variance_mnet\u001b[1;34m(x, y, weights, model)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the explained variance for a given dataset\"\"\"\u001b[39;00m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    619\u001b[0m     \u001b[38;5;66;03m# Process complete dataset as one batch.\u001b[39;00m\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;66;03m# Convert X_train and y_train to PyTorch tensors\u001b[39;00m\n\u001b[1;32m--> 621\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(x, device\u001b[38;5;241m=\u001b[39m\u001b[43mdevice\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    622\u001b[0m     targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    624\u001b[0m     model \u001b[38;5;241m=\u001b[39m RNN_Main_Model(num_features\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mnum_features, hnet_output \u001b[38;5;241m=\u001b[39m weights,  hidden_size \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[0;32m    625\u001b[0m                         num_layers\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mnum_layers, out_dims\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mout_features,  \n\u001b[0;32m    626\u001b[0m                         dropout\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdropout_value, LSTM_ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mLSTM_)\u001b[38;5;241m.\u001b[39mto(device)  \n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "W_0 = hnet(cond_id = 0)\n",
    "r2 = calc_explained_variance_mnet(xx_val_base, yy_val_base, W_0, model)\n",
    "print('R2 for the task', task_id, ' is ', r2)\n",
    "# Save the trained model\n",
    "save_model(hnet, task_id, \"HNET_Task_Recog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2- Loop over any new task\n",
    "#### Start only using one extra task and see if it works\n",
    "\n",
    "- 6- Load new data\n",
    "- 7- For each of the previous tasks test the trained TaskDetectorRNN and compute the R2 score. Save in a list all R2 scores.\n",
    "    - if max(r2_list) > thrs: get argmax of the list as the task_id\n",
    "        - use h_net with the task_id corresponding to the data.\n",
    "        - report results (R2,...)\n",
    "    - else:\n",
    "        - max_id += 1\n",
    "        - task_id = max_id\n",
    "        - train TaskDetectorRNN on new data and save it with its task_id\n",
    "        - check R2 > threshold --> else report ERROR.\n",
    "        - train h_net using this task_id.\n",
    "        - report results (R2,...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We first start trying only on one other task, in this case we perturb the dataset by removing neurons (replacing the columns by 0) up to 50% of the neurons in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'remove_neurons' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m task_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      5\u001b[0m max_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 6\u001b[0m sim_data \u001b[38;5;241m=\u001b[39m \u001b[43mremove_neurons\u001b[49m(data_matrix, \u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m      7\u001b[0m baseline_df_sim \u001b[38;5;241m=\u001b[39m baseline_df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      8\u001b[0m baseline_df_sim[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboth_rates\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sim_data\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'remove_neurons' is not defined"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# Loading data\n",
    "task_id = 0\n",
    "max_id = 0\n",
    "sim_data = remove_neurons(data_matrix, 50)\n",
    "baseline_df_sim = baseline_df.copy()\n",
    "baseline_df_sim['both_rates'] = sim_data.tolist()\n",
    "new_data = baseline_df_sim\n",
    "\n",
    "x_train_1, y_train_1, x_val_1, y_val_1,\\\n",
    "      x_test_1, y_test_1, info_train, info_val,\\\n",
    "          info_test, list_mins_base, \\\n",
    "            list_maxs_base= get_dataset(new_data, \n",
    "                                            fold, \n",
    "                                            target_variable= target_variable, \n",
    "                                            no_outliers = False, \n",
    "                                            force_data = True, \n",
    "                                            std = False)\n",
    "x_train_1, y_train_1 = x_train_1[:20,:,:], y_train_1[:20,:,:]\n",
    "path_recog_models = '../Models/Models_Task_Recognition'\n",
    "r2_list = []\n",
    "for i,m in enumerate(np.sort(os.listdir(path_recog_models))):\n",
    "    print(m)\n",
    "    model_i = torch.load(os.path.join(path_recog_models, m)).to(device)\n",
    "    model_i.eval()\n",
    "    dtype = torch.float32\n",
    "    _, _, _, r2_i,_ = eval_model(x_train_1, \n",
    "                                y_train_1, \n",
    "                                x_val_1, \n",
    "                                y_val_1,\n",
    "                                x_test_1, \n",
    "                                y_test_1,\n",
    "                                model_i, \n",
    "                                metric = 'r2')\n",
    "    r2_list.append(r2_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_r2 = max(r2_list)\n",
    "if max_r2 > thrs:\n",
    "    print('This data comes from a known task. ')\n",
    "    task_id = np.argmax(r2_list)\n",
    "    print('Task_id for this task is ', task_id)\n",
    "else:\n",
    "    print('This data comes from a different task !')\n",
    "    max_id += 1\n",
    "    task_id = max_id\n",
    "    print('Task_id for this task is ', task_id)\n",
    "    model_task_1 =  Causal_Simple_RNN(num_features=num_features, \n",
    "                hidden_units= hidden_units, \n",
    "                num_layers = num_layers, \n",
    "                out_dims = num_dim_output,\n",
    "                dropout = dropout).to(device)\n",
    "    dtype = torch.float32\n",
    "    train_losses, val_losses = \\\n",
    "    train_model(model_task_1, \n",
    "                x_train_1, \n",
    "                y_train_1, \n",
    "                x_val_1, \n",
    "                y_val_1,\n",
    "                lr=  0.00001,\n",
    "                lr_step_size=lr_step_size,\n",
    "                lr_gamma= lr_gamma,\n",
    "                sequence_length_LSTM=seq_length_LSTM,\n",
    "                batch_size_train = batch_size_train,\n",
    "                batch_size_val = batch_size_val,\n",
    "                num_epochs=1000, \n",
    "                delta = 8,                 \n",
    "                regularizer= Regularizer_RNN, \n",
    "                l1_ratio = l1_ratio_reg,\n",
    "                alpha = alpha_reg,     \n",
    "                early_stop = 5)\n",
    "\n",
    "    # Save the trained model\n",
    "    save_model(model_task_1, task_id, \"Models_Task_Recognition\")\n",
    "\n",
    "    # Evaluate the model and set a threshold\n",
    "    _, _, _, r2_i,_ = eval_model(x_train_1, \n",
    "                                y_train_1, \n",
    "                                x_val_1, \n",
    "                                y_val_1,\n",
    "                                x_test_1, \n",
    "                                y_test_1,\n",
    "                                model_task_1, \n",
    "                                metric = 'r2')\n",
    "\n",
    "    print('R2 for the task', task_id, ' is ', r2_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test now the hypernet on task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Current task id is ',task_id)\n",
    "\n",
    "if task_id >0:\n",
    "    calc_reg = True\n",
    "\n",
    "\n",
    "train_losses_, val_losses_, best_w_ =train_current_task(\n",
    "        model, \n",
    "        hnet,\n",
    "        y_train_1, \n",
    "        x_train_1, \n",
    "        y_val_1,\n",
    "        x_val_1, \n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        calc_reg = calc_reg,\n",
    "        cond_id = int(task_id),\n",
    "        lr=lr,\n",
    "        lr_step_size=5,\n",
    "        lr_gamma= lr_gamma, #0.9\n",
    "        sequence_length_LSTM = seq_length_LSTM, #15\n",
    "        batch_size_train = batch_size_train, #15\n",
    "        batch_size_val = batch_size_train, #15\n",
    "        num_epochs=1000, \n",
    "        delta = 8,\n",
    "        beta = beta,             \n",
    "        regularizer=reg_hnet,\n",
    "        l1_ratio = l1_ratio_reg, #0.5\n",
    "        alpha = alpha_reg,    \n",
    "        early_stop = 5,\n",
    "        chunks = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_1 = hnet(cond_id = 1)\n",
    "r2 = calc_explained_variance_mnet(x_val_1, y_val_1, W_1, model)\n",
    "print('R2 for the task', task_id, ' is ', r2)\n",
    "# Save the trained model\n",
    "save_model(hnet, task_id, \"HNET_Task_Recog\")\n",
    "\n",
    "# Test if it still performs well on the previous task\n",
    "W_0 = hnet(cond_id = 0)\n",
    "r2 = calc_explained_variance_mnet(xx_val_base, yy_val_base, W_0, model)\n",
    "print('R2 for the task 0 is ', r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try now to add a task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data2 = shuffle_neurons(data_matrix, 55)\n",
    "baseline_df_sim = baseline_df.copy()\n",
    "baseline_df_sim['both_rates'] = sim_data2.tolist()\n",
    "new_data2 = baseline_df_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_2, y_train_2, x_val_2, y_val_2,\\\n",
    "      x_test_2, y_test_2, info_train, info_val,\\\n",
    "          info_test, list_mins_base, \\\n",
    "            list_maxs_base= get_dataset(new_data2, \n",
    "                                            fold, \n",
    "                                            target_variable= target_variable, \n",
    "                                            no_outliers = False, \n",
    "                                            force_data = True, \n",
    "                                            std = False)\n",
    "\n",
    "path_recog_models = '../Models/Models_Task_Recognition'\n",
    "r2_list = []\n",
    "for i,m in enumerate(np.sort(os.listdir(path_recog_models))):\n",
    "    print(m)\n",
    "    model_i = torch.load(os.path.join(path_recog_models, m)).to(device)\n",
    "    model_i.eval()\n",
    "    dtype = torch.float32\n",
    "    _, _, _, r2_i,_ = eval_model(x_train_2, \n",
    "                                y_train_2, \n",
    "                                x_val_2, \n",
    "                                y_val_2,\n",
    "                                x_test_2,\n",
    "                                y_test_2,\n",
    "                                model_i, \n",
    "                                metric = 'r2')\n",
    "    r2_list.append(r2_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_r2 = max(r2_list)\n",
    "if max_r2 > thrs:\n",
    "    print('This data comes from a known task. ')\n",
    "    task_id = np.argmax(r2_list)\n",
    "    print('Task_id for this task is ', task_id)\n",
    "else:\n",
    "    print('This data comes from a different task !')\n",
    "    max_id += 1\n",
    "    task_id = max_id\n",
    "    print('Task_id for this task is ', task_id)\n",
    "    model_task_2 =  Causal_Simple_RNN(num_features=num_features, \n",
    "                hidden_units= hidden_units, \n",
    "                num_layers = num_layers, \n",
    "                out_dims = num_dim_output,\n",
    "                dropout = dropout).to(device)\n",
    "    dtype = torch.float32\n",
    "    train_losses, val_losses = \\\n",
    "    train_model(model_task_2, \n",
    "                x_train_2, \n",
    "                y_train_2, \n",
    "                x_val_2, \n",
    "                y_val_2,\n",
    "                lr=  0.00001,\n",
    "                lr_step_size=lr_step_size,\n",
    "                lr_gamma= lr_gamma,\n",
    "                sequence_length_LSTM=seq_length_LSTM,\n",
    "                batch_size_train = batch_size_train,\n",
    "                batch_size_val = batch_size_val,\n",
    "                num_epochs=1000, \n",
    "                delta = 8,                 \n",
    "                regularizer= Regularizer_RNN, \n",
    "                l1_ratio = l1_ratio_reg,\n",
    "                alpha = alpha_reg,     \n",
    "                early_stop = 5)\n",
    "\n",
    "    # Save the trained model\n",
    "    save_model(model_task_2, task_id, \"Models_Task_Recognition\")\n",
    "\n",
    "    # Evaluate the model and set a threshold\n",
    "    _, _, _, r2_i,_ = eval_model(x_train_2, \n",
    "                                y_train_2, \n",
    "                                x_val_2, \n",
    "                                y_val_2,\n",
    "                                x_test_2,\n",
    "                                y_test_2,\n",
    "                                model_task_2, \n",
    "                                metric = 'r2')\n",
    "\n",
    "    print('R2 for the task', task_id, ' is ', r2_i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train hypernet on task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(task_id)\n",
    "if task_id >0:\n",
    "    calc_reg = True\n",
    "\n",
    "# Set up the optimizer with the specified learning rate\n",
    "optimizer = torch.optim.Adam(hnet.internal_params, lr=lr)\n",
    "\n",
    "# Set up a learning rate scheduler\n",
    "scheduler = lr_scheduler.StepLR(optimizer, \n",
    "                                step_size=lr_step_size, \n",
    "                                gamma=lr_gamma)\n",
    "\n",
    "train_losses_, val_losses_, best_w_ =train_current_task(\n",
    "        model, \n",
    "        hnet,\n",
    "        y_train_2, \n",
    "        x_train_2, \n",
    "        y_val_2,\n",
    "        x_val_2,                         \n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        calc_reg = calc_reg,\n",
    "        cond_id = int(task_id),\n",
    "        lr=lr,\n",
    "        lr_step_size=5,\n",
    "        lr_gamma= lr_gamma, #0.9\n",
    "        sequence_length_LSTM = seq_length_LSTM, #15\n",
    "        batch_size_train = batch_size_train, #15\n",
    "        batch_size_val = batch_size_train, #15\n",
    "        num_epochs=1000, \n",
    "        delta = 8,\n",
    "        beta = beta,             \n",
    "        regularizer=reg_hnet,\n",
    "        l1_ratio = l1_ratio_reg, #0.5\n",
    "        alpha = alpha_reg,    \n",
    "        early_stop = 5,\n",
    "        chunks = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_2 = hnet(cond_id = 2)\n",
    "r2 = calc_explained_variance_mnet(x_val_2, y_val_2, W_2, model)\n",
    "print('R2 for the task', task_id, ' is ', r2)\n",
    "# Save the trained model\n",
    "save_model(hnet, task_id, \"HNET_Task_Recog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if it still performs well on the previous task\n",
    "W_0 = hnet(cond_id = 0)\n",
    "r2_0 = calc_explained_variance_mnet(xx_val_base, yy_val_base, W_0, model)\n",
    "print('R2 for the task 0 is ', r2_0)\n",
    "\n",
    "W_1 = hnet(cond_id = 1)\n",
    "r2_1 = calc_explained_variance_mnet(x_val_1, y_val_1, W_1, model)\n",
    "print('R2 for the task 0 is ', r2_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sinthlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
