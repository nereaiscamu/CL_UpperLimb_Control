{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nerea/anaconda3/envs/sinthlab/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.helpers import *\n",
    "from src.visualize import *\n",
    "from src.trainer import *\n",
    "from Models.models import *\n",
    "from Models.SimpleRNN_NC import SimpleRNN_NC\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.metrics import *\n",
    "from copy import deepcopy\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "from hypnettorch.hnets import HyperNetInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert between numpy arrays and tensors\n",
    "to_t = lambda array: torch.tensor(array, device='cpu', dtype=dtype)  #device\n",
    "to_t_eval =  lambda array: torch.tensor(array, device='cuda', dtype=dtype)  #device\n",
    "from_t = lambda tensor: tensor.to(\"cpu\").detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Chewie'\n",
    "date = '1007'\n",
    "fold = 4\n",
    "target_variable = 'vel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Helper functions for plotting (run this cell!)\n",
    "sns.set_context(\"notebook\")\n",
    "\n",
    "# initialize a color palette for plotting\n",
    "palette = sns.xkcd_palette([\"windows blue\",\n",
    "                            \"red\",\n",
    "                            \"medium green\",\n",
    "                            \"dusty purple\",\n",
    "                            \"orange\",\n",
    "                            \"amber\",\n",
    "                            \"clay\",\n",
    "                            \"pink\",\n",
    "                            \"greyish\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_t_eval =  lambda array: torch.tensor(array, device=device, dtype=dtype)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './Data/Processed_Data/Tidy_'+name+'_'+date+'.pkl'\n",
    "\n",
    "with open(data_path, 'rb') as file:\n",
    "    tidy_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df = tidy_df.loc[tidy_df['epoch'] == 'BL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_df =  tidy_df.loc[tidy_df['epoch'] == 'AD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to consider only the trials for which the monkey has already adapted to the perturbation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_keep = force_df.id.unique()[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline subset has a total of 170 trials, whereas the perturbation one contains 201 trials, we can for now try to remove the first 50 trials from the perturbation subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "force_df = force_df.loc[force_df.id.isin(ids_to_keep)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get train-val-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train trials 109\n",
      "Test trials  34\n",
      "Val trials 27\n",
      "We are testing the optimization method on fold  4\n"
     ]
    }
   ],
   "source": [
    "xx_train_base, yy_train_base, xx_val_base, yy_val_base,\\\n",
    "      xx_test_base, yy_test_base, info_train_base, info_val_base,\\\n",
    "          info_test_base, list_mins_base, \\\n",
    "            list_maxs_base= get_dataset(baseline_df, fold, target_variable= target_variable, no_outliers = False, force_data = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train trials 97\n",
      "Test trials  30\n",
      "Val trials 24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are testing the optimization method on fold  4\n"
     ]
    }
   ],
   "source": [
    "xx_train_force, yy_train_force, xx_val_force, yy_val_force,\\\n",
    "      xx_test_force, yy_test_force, info_train_force, info_val_force,\\\n",
    "          info_test_force,  list_mins_force, \\\n",
    "            list_maxs_force = get_dataset(force_df, fold, target_variable= target_variable, no_outliers = False, force_data = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train trials 211\n",
      "Test trials  66\n",
      "Val trials 53\n",
      "We are testing the optimization method on fold  4\n"
     ]
    }
   ],
   "source": [
    "xx_train_all, yy_train_all, xx_val_all, yy_val_all, \\\n",
    "    xx_test_all, yy_test_all, info_train_all, \\\n",
    "    info_val_all, info_test_all,  list_mins_all,\\\n",
    "          list_maxs_all = get_dataset(tidy_df,fold, target_variable= target_variable, no_outliers = False\n",
    "                                      , force_data = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify that we want our tensors on the GPU and in float32\n",
    "device = torch.device('cuda:0') #suposed to be cuda\n",
    "#device = torch.device('cpu') \n",
    "dtype = torch.float32\n",
    "path_to_models = './Models/Models_Force'\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "seed_value = 42\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)  # If using CUDA\n",
    "\n",
    "num_dim_output = yy_train_base.shape[2]\n",
    "num_features = xx_train_base.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_reset(m):\n",
    "    reset_parameters = getattr(m, \"reset_parameters\", None)\n",
    "    if callable(reset_parameters):\n",
    "        m.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state_dict(param_names, param_values):\n",
    "    s_d = {}\n",
    "    for n,v in zip(param_names, param_values):\n",
    "        s_d[n] = v\n",
    "    return s_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN_Main_Model(nn.Module):\n",
    "    def __init__(self, hnet_output, \n",
    "                 num_features = 124, \n",
    "                hidden_size= 3, \n",
    "                num_layers = 2, \n",
    "                out_dims = 6,\n",
    "                dropout = 0.5,\n",
    "                bias = True,\n",
    "                LSTM_ = False):\n",
    "        \n",
    "        super(RNN_Main_Model, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.hnet_output = hnet_output\n",
    "        self.bias = bias\n",
    "        self.out_features = out_dims\n",
    "        self.LSTM_ = LSTM_\n",
    "\n",
    "        self.dropout = nn.Dropout(p= dropout) #trial.suggest_float('dropout_1', 0.1, 0.9)\n",
    "\n",
    "        # Define recurrent layer\n",
    "        self.rnn = nn.RNN(self.num_features, self.hidden_size, self.num_layers, self.bias, batch_first = True, bidirectional = False)\n",
    "        names_p = [name for name, _ in self.rnn.named_parameters()]\n",
    "        self.hnet_output_dict = create_state_dict(names_p,hnet_output[2:] )\n",
    "\n",
    "        # Define recurrent layer (LSTM)\n",
    "        if self.LSTM_:\n",
    "            self.rnn = nn.LSTM(self.num_features, self.hidden_size, self.num_layers, self.bias, batch_first = True, bidirectional = False)\n",
    "            names_p = [name for name, _ in self.rnn.named_parameters()]\n",
    "            self.hnet_output_dict = create_state_dict(names_p,hnet_output[2:] )      \n",
    "\n",
    "        self.selu = nn.SELU()      \n",
    "\n",
    "    def forward(self, x, hx=None):\n",
    "        # Forward pass\n",
    "        if hx is None:\n",
    "            if self.LSTM_:\n",
    "                h0 = torch.randn(self.num_layers, x.size(0), self.hidden_size, device=x.device) * 0.1\n",
    "                c0 = torch.randn(self.num_layers, x.size(0), self.hidden_size, device=x.device) *0.1 # Initialize cell state\n",
    "                hx = (h0, c0)\n",
    "            else:\n",
    "                hx = torch.randn(self.num_layers, x.size(0), self.hidden_size, device=x.device) * 0.1\n",
    "        \n",
    "        # Perform RNN operation\n",
    "        x, _  = torch.func.functional_call(self.rnn, self.hnet_output_dict, (x, hx))\n",
    "        x = self.dropout(x)\n",
    "        x = self.selu(x) \n",
    "        output =  F.linear(x, self.hnet_output[0], bias=self.hnet_output[1])\n",
    "        \n",
    "        return output.squeeze() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a template model only to get automatically the model parameters shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7febf5f95d10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_dim_output = yy_train_base.shape[2]\n",
    "num_features = xx_train_base.shape[2]\n",
    "\n",
    "# Hyperparameters LSTM class (from force model without hnet)\n",
    "# Define hyperparameters\n",
    "\n",
    "#Hyperparameters objective and regularization\n",
    "alpha_reg = 1e-5\n",
    "l1_ratio_reg = 0.5\n",
    "\n",
    "lr = 0.00001\n",
    "loss_function = huber_loss\n",
    "delta = 8  # hyperparameter for huber loss\n",
    "\n",
    "# Hyperparameters LSTM class\n",
    "n_hidden_units = 300\n",
    "num_layers = 1\n",
    "input_size = 49\n",
    "dropout = 0.2\n",
    "\n",
    "#Other training hyperparameters\n",
    "\n",
    "lr_gamma= 1.37 #for scheduler\n",
    "lr_step_size = 10 #for scheduler\n",
    "\n",
    "seq_length_LSTM= 19\n",
    "batch_size_train= 25\n",
    "batch_size_val = 25\n",
    "\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_m = Causal_Simple_RNN(num_features=num_features, \n",
    "                    hidden_units= n_hidden_units, \n",
    "                    num_layers = num_layers, \n",
    "                    out_dims = num_dim_output, ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_shapes = [p.shape for p in list(template_m.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 300]),\n",
       " torch.Size([2]),\n",
       " torch.Size([300, 130]),\n",
       " torch.Size([300, 300]),\n",
       " torch.Size([300]),\n",
       " torch.Size([300])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created MLP Hypernet.\n",
      "Hypernetwork with 1822961 weights and 130202 outputs (compression ratio: 14.00).\n",
      "The network consists of 1822945 unconditional weights (1822945 internally maintained) and 16 conditional weights (16 internally maintained).\n"
     ]
    }
   ],
   "source": [
    "from hypnettorch.hnets import HMLP\n",
    "\n",
    "num_conditions = 2\n",
    "size_task_embedding = 8\n",
    "\n",
    "hnet = HMLP(param_shapes, uncond_in_size=0,\n",
    "             cond_in_size=size_task_embedding,\n",
    "            layers=[13], \n",
    "            num_cond_embs=num_conditions).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in hnet.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_test = hnet(cond_id = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_ = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_Main_Model(num_features= num_features, hnet_output = w_test,  hidden_size = n_hidden_units,\n",
    "                            num_layers= num_layers,out_dims=num_dim_output,  \n",
    "                            dropout= dropout,  LSTM_ = LSTM_).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_hnet(model):\n",
    "    # Initialize weights\n",
    "    for i, layer in enumerate(model.layer_weight_tensors[:-1]):\n",
    "        nn.init.kaiming_uniform_(layer, mode='fan_in', nonlinearity='relu')\n",
    "        if model.has_bias:\n",
    "            nn.init.zeros_(model.layer_bias_vectors[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_hnet(weights, alpha, l1_ratio):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implement an L1-L2 penalty on the norm of the model weights.\n",
    "\n",
    "    model: MLP\n",
    "    alpha: scaling parameter for the regularization.\n",
    "    l1_ratio: mixing parameter between L1 and L2 loss.\n",
    "\n",
    "    Returns:\n",
    "    reg: regularization term\n",
    "    \"\"\"\n",
    "    l1_loss = 0\n",
    "    l2_loss = 0\n",
    "\n",
    "    # Accumulate L1 and L2 losses for weight matrices in the model\n",
    "    \n",
    "\n",
    "    weights_ =  [i for i in weights if len(i.shape)==2]\n",
    "\n",
    "    for weight_tensor in weights_[:2]:\n",
    "        l1_loss += torch.sum(torch.abs(weight_tensor))\n",
    "        l2_loss += torch.sum(weight_tensor.pow(2))\n",
    "\n",
    "    reg = l1_ratio * l1_loss + (1 - l1_ratio) * l2_loss\n",
    "\n",
    "    reg = alpha * reg\n",
    "\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anirudh's Function\n",
    "\n",
    "def get_current_targets(task_id, hnet):\n",
    "    r\"\"\"For all :math:`j < \\text{task\\_id}`, compute the output of the\n",
    "    hypernetwork. This output will be detached from the graph before being added\n",
    "    to the return list of this function.\n",
    "\n",
    "    Note, if these targets don't change during training, it would be more memory\n",
    "    efficient to store the weights :math:`\\theta^*` of the hypernetwork (which\n",
    "    is a fixed amount of memory compared to the variable number of tasks).\n",
    "    Though, it is more computationally expensive to recompute\n",
    "    :math:`h(c_j, \\theta^*)` for all :math:`j < \\text{task\\_id}` everytime the\n",
    "    target is needed.\n",
    "\n",
    "    Note, this function sets the hypernet temporarily in eval mode. No gradients\n",
    "    are computed.\n",
    "\n",
    "    See argument ``targets`` of :func:`calc_fix_target_reg` for a use-case of\n",
    "    this function.\n",
    "\n",
    "    Args:\n",
    "        task_id (int): The ID of the current task.\n",
    "        hnet: An instance of the hypernetwork before learning a new task\n",
    "            (i.e., the hypernetwork has the weights :math:`\\theta^*` necessary\n",
    "            to compute the targets).\n",
    "\n",
    "    Returns:\n",
    "        An empty list, if ``task_id`` is ``0``. Otherwise, a list of\n",
    "        ``task_id-1`` targets. These targets can be passed to the function\n",
    "        :func:`calc_fix_target_reg` while training on the new task.\n",
    "    \"\"\"\n",
    "    # We temporarily switch to eval mode for target computation (e.g., to get\n",
    "    # rid of training stochasticities such as dropout).\n",
    "    hnet_mode = hnet.training\n",
    "    hnet.eval()\n",
    "\n",
    "    ret = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        W = hnet.forward(cond_id=list(range(task_id)), ret_format=\"sequential\")\n",
    "        ret = [[p.detach() for p in W_tid] for W_tid in W]\n",
    "\n",
    "    hnet.train(mode=hnet_mode)\n",
    "\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_fix_target_reg(\n",
    "    hnet,\n",
    "    task_id,\n",
    "    targets=None,\n",
    "    dTheta=None,\n",
    "    dTembs=None,\n",
    "    mnet=None,\n",
    "    prev_theta=None,\n",
    "    prev_task_embs=None,\n",
    "    batch_size=None,\n",
    "    reg_scaling=None,\n",
    "):\n",
    "    r\"\"\"This regularizer simply restricts the output-mapping for previous\n",
    "    task embeddings. I.e., for all :math:`j < \\text{task\\_id}` minimize:\n",
    "\n",
    "    .. math::\n",
    "        \\lVert \\text{target}_j - h(c_j, \\theta + \\Delta\\theta) \\rVert^2\n",
    "\n",
    "    where :math:`c_j` is the current task embedding for task :math:`j` (and we\n",
    "    assumed that ``dTheta`` was passed).\n",
    "\n",
    "    Args:\n",
    "        hnet: The hypernetwork whose output should be regularized; has to\n",
    "            implement the interface\n",
    "            :class:`hnets.hnet_interface.HyperNetInterface`.\n",
    "        task_id (int): The ID of the current task (the one that is used to\n",
    "            compute ``dTheta``).\n",
    "        targets (list): A list of outputs of the hypernetwork. Each list entry\n",
    "            must have the output shape as returned by the\n",
    "            :meth:`hnets.hnet_interface.HyperNetInterface.forward` method of the\n",
    "            ``hnet``. Note, this function doesn't detach targets. If desired,\n",
    "            that should be done before calling this function.\n",
    "\n",
    "            Also see :func:`get_current_targets`.\n",
    "        dTheta (list, optional): The current direction of weight change for the\n",
    "            internal (unconditional) weights of the hypernetwork evaluated on\n",
    "            the task-specific loss, i.e., the weight change that would be\n",
    "            applied to the unconditional parameters :math:`\\theta`. This\n",
    "            regularizer aims to modify this direction, such that the hypernet\n",
    "            output for embeddings of previous tasks remains unaffected.\n",
    "            Note, this function does not detach ``dTheta``. It is up to the\n",
    "            user to decide whether dTheta should be a constant vector or\n",
    "            might depend on parameters of the hypernet.\n",
    "\n",
    "            Also see :func:`utils.optim_step.calc_delta_theta`.\n",
    "        dTembs (list, optional): The current direction of weight change for the\n",
    "            task embeddings of all tasks that have been learned already.\n",
    "            See ``dTheta`` for details.\n",
    "        mnet: Instance of the main network. Has to be provided if\n",
    "            ``inds_of_out_heads`` are specified.\n",
    "        prev_theta (list, optional): If given, ``prev_task_embs`` but not\n",
    "            ``targets`` has to be specified. ``prev_theta`` is expected to be\n",
    "            the internal unconditional weights :math:`theta` prior to learning\n",
    "            the current task. Hence, it can be used to compute the targets on\n",
    "            the fly (which is more memory efficient (constant memory), but more\n",
    "            computationally demanding).\n",
    "            The computed targets will be detached from the computational graph.\n",
    "            Independent of the current hypernet mode, the targets are computed\n",
    "            in ``eval`` mode.\n",
    "        prev_task_embs (list, optional): If given, ``prev_theta`` but not\n",
    "            ``targets`` has to be specified. ``prev_task_embs`` are the task\n",
    "            embeddings (conditional parameters) of the hypernetwork.\n",
    "            See docstring of ``prev_theta`` for more details.\n",
    "        batch_size (int, optional): If specified, only a random subset of\n",
    "            previous tasks is regularized. If the given number is bigger than\n",
    "            the number of previous tasks, all previous tasks are regularized.\n",
    "\n",
    "            Note:\n",
    "                A ``batch_size`` smaller or equal to zero will be ignored\n",
    "                rather than throwing an error.\n",
    "        reg_scaling (list, optional): If specified, the regulariation terms for\n",
    "            the different tasks are scaled arcording to the entries of this\n",
    "            list.\n",
    "\n",
    "    Returns:\n",
    "        The value of the regularizer.\n",
    "    \"\"\"\n",
    "    assert isinstance(hnet, HyperNetInterface)\n",
    "    assert task_id > 0\n",
    "    # FIXME We currently assume the hypernet has all parameters internally.\n",
    "    # Alternatively, we could allow the parameters to be passed to us, that we\n",
    "    # will then pass to the forward method.\n",
    "    assert hnet.unconditional_params is not None and len(hnet.unconditional_params) > 0\n",
    "    assert targets is None or len(targets) == task_id\n",
    "    assert targets is None or (prev_theta is None and prev_task_embs is None)\n",
    "    assert prev_theta is None or prev_task_embs is not None\n",
    "    # assert prev_task_embs is None or len(prev_task_embs) >= task_id\n",
    "    assert dTembs is None or len(dTembs) >= task_id\n",
    "    assert reg_scaling is None or len(reg_scaling) >= task_id\n",
    "\n",
    "    # Number of tasks to be regularized.\n",
    "    num_regs = task_id\n",
    "    ids_to_reg = list(range(num_regs))\n",
    "    if batch_size is not None and batch_size > 0:\n",
    "        if num_regs > batch_size:\n",
    "            ids_to_reg = np.random.choice(\n",
    "                num_regs, size=batch_size, replace=False\n",
    "            ).tolist()\n",
    "            num_regs = batch_size\n",
    "\n",
    "    # FIXME Assuming all unconditional parameters are internal.\n",
    "    assert len(hnet.unconditional_params) == len(hnet.unconditional_param_shapes)\n",
    "\n",
    "    weights = dict()\n",
    "    uncond_params = hnet.unconditional_params\n",
    "    if dTheta is not None:\n",
    "        uncond_params = hnet.add_to_uncond_params(dTheta, params=uncond_params)\n",
    "    weights[\"uncond_weights\"] = uncond_params\n",
    "\n",
    "    if dTembs is not None:\n",
    "        # FIXME That's a very unintutive solution for the user.\n",
    "        assert (\n",
    "            hnet.conditional_params is not None\n",
    "            and len(hnet.conditional_params) == len(hnet.conditional_param_shapes)\n",
    "            and len(hnet.conditional_params) == len(dTembs)\n",
    "        )\n",
    "        weights[\"cond_weights\"] = hnet.add_to_uncond_params(\n",
    "            dTembs, params=hnet.conditional_params\n",
    "        )\n",
    "\n",
    "    if targets is None:\n",
    "        prev_weights = dict()\n",
    "        prev_weights[\"uncond_weights\"] = prev_theta\n",
    "        # FIXME We just assume that `prev_task_embs` are all conditional\n",
    "        # weights.\n",
    "        prev_weights[\"cond_weights\"] = prev_task_embs\n",
    "\n",
    "    reg = 0\n",
    "\n",
    "    for i in ids_to_reg:\n",
    "        weights_predicted = hnet.forward(cond_id=i, weights=weights)\n",
    "\n",
    "        if targets is not None:\n",
    "            target = targets[i]\n",
    "        else:\n",
    "            # Compute targets in eval mode!\n",
    "            hnet_mode = hnet.training\n",
    "            hnet.eval()\n",
    "\n",
    "            # Compute target on the fly using previous hnet.\n",
    "            with torch.no_grad():\n",
    "                target = hnet.forward(cond_id=i, weights=prev_weights)\n",
    "            target = [d.detach().clone() for d in target]\n",
    "\n",
    "            hnet.train(mode=hnet_mode)\n",
    "\n",
    "        # Regularize all weights of the main network.\n",
    "        W_target = torch.cat([w.view(-1) for w in target])\n",
    "        W_predicted = torch.cat([w.view(-1) for w in weights_predicted])\n",
    "\n",
    "        reg_i = (W_target - W_predicted).pow(2).sum()\n",
    "\n",
    "        if reg_scaling is not None:\n",
    "            reg += reg_scaling[i] * reg_i\n",
    "        else:\n",
    "            reg += reg_i\n",
    "\n",
    "    return reg / num_regs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_current_task(\n",
    "        model, \n",
    "        hnet,\n",
    "        y_train, \n",
    "        x_train,\n",
    "        y_val,\n",
    "        x_val,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        calc_reg = False,\n",
    "        cond_id = 0,\n",
    "        lr=0.0001,\n",
    "        lr_step_size=10,\n",
    "        lr_gamma=0.9,\n",
    "        sequence_length_LSTM=15,\n",
    "        batch_size_train = 25,\n",
    "        batch_size_val = 25,\n",
    "        num_epochs=1000, \n",
    "        delta = 8,      \n",
    "        beta=0,           \n",
    "        regularizer=None,\n",
    "        l1_ratio = 0.5,\n",
    "        alpha_A = 1e-5,  \n",
    "        alpha_B = 1e-5,        \n",
    "        early_stop = 10,\n",
    "        LSTM_ = LSTM_,\n",
    "        chunks = False):\n",
    "    \n",
    "    # Compute weights that result from hnet from all previous tasks\n",
    "    if calc_reg == True:\n",
    "        reg_targets = get_current_targets(cond_id, hnet)\n",
    "        prev_hnet_theta = None\n",
    "        prev_task_embs = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Keep track of the best model's parameters and loss\n",
    "    best_model_wts = deepcopy(model.state_dict())\n",
    "    best_loss = 1e8\n",
    "\n",
    "    # Enable anomaly detection for debugging\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    # Track the train and validation loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    # Counters for early stopping\n",
    "    not_increased = 0\n",
    "    end_train = 0\n",
    "    \n",
    "    # Reshape data for the LSTM\n",
    "    train_dataset = SequenceDataset(y_train, x_train, sequence_length=sequence_length_LSTM)\n",
    "    val_dataset = SequenceDataset(y_val, x_val, sequence_length=sequence_length_LSTM)\n",
    "    loader_train = data.DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "    loader_val = data.DataLoader(val_dataset, batch_size=batch_size_val, shuffle=True)\n",
    "\n",
    "    # Initialize h0 and c0 outside the model\n",
    "    if LSTM_ == True:\n",
    "\n",
    "        h0 = torch.randn(num_layers, batch_size_train, n_hidden_units, device=device) * 0.1\n",
    "        c0 = torch.randn(num_layers, batch_size_train, n_hidden_units, device=device) *0.1 # Initialize cell state\n",
    "        hx = (h0, c0) \n",
    "    else:\n",
    "        hx = torch.randn(num_layers, batch_size_train, n_hidden_units, device=device) * 0.1\n",
    "    # Loop through epochs\n",
    "    for epoch in np.arange(num_epochs):\n",
    "        for phase in ['train', 'val']:\n",
    "            # set model to train/validation as appropriate\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                loader = loader_train\n",
    "            else:\n",
    "                model.eval()\n",
    "                loader = loader_val\n",
    "\n",
    "            # Initialize variables to track loss and batch size\n",
    "            running_loss = 0\n",
    "            running_size = 0        \n",
    "\n",
    "            # Iterate over batches in the loader\n",
    "            for data in loader:\n",
    "\n",
    "                # Define data for this batch\n",
    "                x = data[0].to('cuda')\n",
    "                y = data[1].to('cuda')\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    with torch.set_grad_enabled(True):\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        # Forward pass through both models\n",
    "                        W = hnet(cond_id=0)\n",
    "                        model = RNN_Main_Model(\n",
    "                            num_features= num_features, \n",
    "                            hnet_output = W,  \n",
    "                            hidden_size = n_hidden_units,\n",
    "                            num_layers= num_layers, \n",
    "                            out_dims=num_dim_output,  \n",
    "                            dropout= dropout, \n",
    "                            LSTM_ = LSTM_).to(device)\n",
    "                                    \n",
    "                        y_pred = model(x, hx)\n",
    "                        \n",
    "                        # Compute loss from the current task\n",
    "                        loss_task = huber_loss(y_pred, y, delta = delta)\n",
    "                        \n",
    "                        # Add regularization from the previous tasks\n",
    "                        if calc_reg:\n",
    "                            loss_reg = calc_fix_target_reg(\n",
    "                                hnet,\n",
    "                                cond_id,\n",
    "                                targets=reg_targets,\n",
    "                                mnet=model,\n",
    "                                prev_theta=prev_hnet_theta,\n",
    "                                prev_task_embs=prev_task_embs,)\n",
    "\n",
    "                            loss_t = loss_task + beta * loss_reg \n",
    "                        else:\n",
    "                            loss_t = loss_task\n",
    "                        \n",
    "                        \n",
    "                    \n",
    "                        # Compute gradients and perform an optimization step\n",
    "                        loss_t.backward()\n",
    "                        optimizer.step()\n",
    "                else:\n",
    "                    # just compute the loss in validation phase\n",
    "                    # Compute FIRST loss.\n",
    "                    W = hnet(cond_id=0)\n",
    "                    model = RNN_Main_Model(num_features= num_features, hnet_output = W,  hidden_size = n_hidden_units,\n",
    "                        num_layers= num_layers, out_dims=num_dim_output,  \n",
    "                        dropout= dropout, LSTM_ = LSTM_).to(device)    \n",
    "                    y_pred = model(x, hx)\n",
    "                    loss = huber_loss(y_pred, y, delta = delta)\n",
    "\n",
    "                    loss_t = loss\n",
    "\n",
    "                # Ensure the loss is finite\n",
    "                assert torch.isfinite(loss_t)\n",
    "                running_loss += loss_t.item()\n",
    "                running_size += 1\n",
    "\n",
    "            # compute the train/validation loss and update the best\n",
    "            # model parameters if this is the lowest validation loss yet\n",
    "            running_loss /= running_size\n",
    "            if phase == \"train\":\n",
    "                train_losses.append(running_loss)\n",
    "            else:\n",
    "                val_losses.append(running_loss)\n",
    "                # Update best model parameters if validation loss improves\n",
    "                if running_loss < best_loss:\n",
    "                    best_loss = running_loss\n",
    "                    best_w = W\n",
    "                    best_model_wts = deepcopy(model.state_dict())\n",
    "                    not_increased = 0\n",
    "                else:\n",
    "                    # Perform early stopping if validation loss doesn't improve\n",
    "                    if epoch > 10:\n",
    "                        not_increased += 1\n",
    "                        # print('Not increased : {}/5'.format(not_increased))\n",
    "                        if not_increased == early_stop:\n",
    "                            print('Decrease LR')\n",
    "                            for g in optimizer.param_groups:\n",
    "                                g['lr'] = g['lr'] / 2\n",
    "                            not_increased = 0\n",
    "                            end_train += 1\n",
    "                        \n",
    "                        if end_train == 2:\n",
    "                            model.load_state_dict(best_model_wts)\n",
    "                            return np.array(train_losses), np.array(val_losses), best_w\n",
    "\n",
    "        # Update learning rate with the scheduler\n",
    "        scheduler.step()\n",
    "        print(\"Epoch {:03} Train {:.4f} Val {:.4f}\".format(epoch, train_losses[-1], val_losses[-1]))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return np.array(train_losses), np.array(val_losses), best_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "beta = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train trials 109\n",
      "Test trials  34\n",
      "Val trials 27\n",
      "We are testing the optimization method on fold  4\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'data' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 32\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Generate feature and target matrices\u001b[39;00m\n\u001b[1;32m     24\u001b[0m x_train, y_train, x_val, y_val, \\\n\u001b[1;32m     25\u001b[0m x_test, y_test, info_train, \\\n\u001b[1;32m     26\u001b[0m info_val, info_test,  list_mins,\\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m                                 no_outliers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m     30\u001b[0m                                 force_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 32\u001b[0m train_losses_, val_losses_, best_w_ \u001b[38;5;241m=\u001b[39m\u001b[43mtrain_current_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhnet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcalc_reg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcalc_reg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcond_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_step_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr_gamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr_gamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#0.9\u001b[39;49;00m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequence_length_LSTM\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseq_length_LSTM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#15\u001b[39;49;00m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#15\u001b[39;49;00m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size_val\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#15\u001b[39;49;00m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m             \u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreg_hnet_noweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ml1_ratio_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#0.5\u001b[39;49;00m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha_A\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malpha_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha_B\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malpha_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m train_losses[name] \u001b[38;5;241m=\u001b[39m train_losses_\n\u001b[1;32m     60\u001b[0m val_losses[name] \u001b[38;5;241m=\u001b[39m val_losses_\n",
      "Cell \u001b[0;32mIn[32], line 54\u001b[0m, in \u001b[0;36mtrain_current_task\u001b[0;34m(model, hnet, y_train, x_train, y_val, x_val, optimizer, scheduler, calc_reg, cond_id, lr, lr_step_size, lr_gamma, sequence_length_LSTM, batch_size_train, batch_size_val, num_epochs, delta, beta, regularizer, l1_ratio, alpha_A, alpha_B, early_stop, LSTM_, chunks)\u001b[0m\n\u001b[1;32m     52\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m SequenceDataset(y_train, x_train, sequence_length\u001b[38;5;241m=\u001b[39msequence_length_LSTM)\n\u001b[1;32m     53\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m SequenceDataset(y_val, x_val, sequence_length\u001b[38;5;241m=\u001b[39msequence_length_LSTM)\n\u001b[0;32m---> 54\u001b[0m loader_train \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mDataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size_train, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m loader_val \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mDataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size_val, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Initialize h0 and c0 outside the model\u001b[39;00m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'data' referenced before assignment"
     ]
    }
   ],
   "source": [
    "task_names = ['Baseline', 'Adaptation']\n",
    "task_data = [baseline_df, force_df]\n",
    "\n",
    "calc_reg = False\n",
    "task_id = 0\n",
    "\n",
    "train_losses  = {}\n",
    "val_losses = {}\n",
    "best_W = {}\n",
    "\n",
    "for name, dataset_ in zip(task_names, task_data):\n",
    "    if task_id >0:\n",
    "        calc_reg = True\n",
    "\n",
    "    # Set up the optimizer with the specified learning rate\n",
    "    optimizer = torch.optim.Adam(hnet.internal_params, lr=lr)\n",
    "\n",
    "    # Set up a learning rate scheduler\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, \n",
    "                                    step_size=lr_step_size, \n",
    "                                    gamma=lr_gamma)\n",
    "    \n",
    "    # Generate feature and target matrices\n",
    "    x_train, y_train, x_val, y_val, \\\n",
    "    x_test, y_test, info_train, \\\n",
    "    info_val, info_test,  list_mins,\\\n",
    "          list_maxs = get_dataset(dataset_,fold,\n",
    "                                    target_variable= target_variable,\n",
    "                                    no_outliers = False, \n",
    "                                    force_data = True)\n",
    "    \n",
    "    train_losses_, val_losses_, best_w_ =train_current_task(\n",
    "        model, \n",
    "        hnet,\n",
    "        y_train, \n",
    "        x_train,\n",
    "        y_val,\n",
    "        x_val,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        calc_reg = calc_reg,\n",
    "        cond_id = task_id,\n",
    "        lr=lr,\n",
    "        lr_step_size=5,\n",
    "        lr_gamma= lr_gamma, #0.9\n",
    "        sequence_length_LSTM = seq_length_LSTM, #15\n",
    "        batch_size_train = batch_size_train, #15\n",
    "        batch_size_val = batch_size_train, #15\n",
    "        num_epochs=1000, \n",
    "        delta = 8,\n",
    "        beta = beta,             \n",
    "        regularizer=reg_hnet_noweights,\n",
    "        l1_ratio = l1_ratio_reg, #0.5\n",
    "        alpha_A = alpha_reg, \n",
    "        alpha_B = alpha_reg,     \n",
    "        early_stop = 5,\n",
    "        chunks = False)\n",
    "    \n",
    "    train_losses[name] = train_losses_\n",
    "    val_losses[name] = val_losses_\n",
    "    best_W[name] = best_w_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# got stuck here, some problem with variable \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = ['Training', 'Validation', 'Test']\n",
    "\n",
    "data_base = [[xx_train_base, yy_train_base],\n",
    "             [xx_val_base, yy_val_base],\n",
    "             [xx_test_base, yy_test_base]]\n",
    "\n",
    "data_force = [[xx_train_force, yy_train_force],\n",
    "             [xx_val_force, yy_val_force],\n",
    "             [xx_test_force, yy_test_force]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance score for  Training  is :  0.9358438849449158\n",
      "Explained variance score for  Validation  is :  0.8675511181354523\n",
      "Explained variance score for  Test  is :  0.8655193746089935\n"
     ]
    }
   ],
   "source": [
    "for index, [x,y] in enumerate(data_base):\n",
    "    r2 = calc_explained_variance(x, y, W_base)\n",
    "    print('Explained variance score for ', subsets[index], ' is : ', r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance score for  Training  is :  0.9086236357688904\n",
      "Explained variance score for  Validation  is :  0.8310559093952179\n",
      "Explained variance score for  Test  is :  0.8234804272651672\n"
     ]
    }
   ],
   "source": [
    "for index, [x,y] in enumerate(data_force):\n",
    "    r2 = calc_explained_variance(x, y, W_force)\n",
    "    print('Explained variance score for ', subsets[index], ' is : ', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base_hnet = RNN_Main_Model(num_features= num_features, hnet_output = W_base,  hidden_size = n_hidden_units,\n",
    "                            num_layers= num_layers, out_dims=num_dim_output,  \n",
    "                            dropout= dropout, LSTM_ = LSTM_).to(device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_force_hnet = RNN_Main_Model(num_features= num_features, hnet_output = W_force,  hidden_size = n_hidden_units,\n",
    "                            num_layers= num_layers, out_dims=num_dim_output,  \n",
    "                            dropout= dropout, LSTM_ = LSTM_).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_base = 'RNN_hnet_'+name+'_'+date+'_Baseline'\n",
    "exp_force = 'RNN_hnet_'+name+'_'+date+'_Force'\n",
    "path_base = os.path.join(path_to_models,exp_base)\n",
    "path_force = os.path.join(path_to_models,exp_force)\n",
    "if not os.path.exists(path_base):\n",
    "    os.makedirs(path_base)\n",
    "if not os.path.exists(path_force):\n",
    "    os.makedirs(path_force)\n",
    "path_base_fold = os.path.join(path_base,'fold_{}.pth'.format(fold))\n",
    "path_force_fold = os.path.join(path_force,'fold_{}.pth'.format(fold))\n",
    "torch.save(model_base_hnet, path_base_fold)\n",
    "torch.save(model_force_hnet, path_force_fold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sinthlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
