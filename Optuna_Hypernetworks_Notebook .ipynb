{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nerea/anaconda3/envs/sinthlab/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "from src.helpers import *\n",
    "from src.visualize import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "\n",
    "import optuna\n",
    "\n",
    "from hypnettorch.hnets import HMLP\n",
    "from hypnettorch.mnets import SimpleRNN\n",
    "\n",
    "\n",
    "device = torch.device('cpu') #suposed to be cuda\n",
    "dtype = torch.float32\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"c:\\\\Users\\\\nerea\\\\OneDrive\\\\Documentos\\\\EPFL_MASTER\\\\PDM\\\\Project\\\\PyalData\")\n",
    "# to change for the actual path where PyalData has been cloned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './Data/Processed_Data/Tidy_Sansa_13_04.pkl'\n",
    "\n",
    "with open(data_path, 'rb') as file:\n",
    "    df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df = df.loc[df.type == 'BASELINE'].reset_index()\n",
    "stim_df = df.loc[df.type == 'TONIC'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test trials  4\n",
      "Val trials 3\n"
     ]
    }
   ],
   "source": [
    "X_train_b, y_train_b, X_val_b, y_val_b, X_test_b, y_test_b, info_train_b, info_val_b, info_test_b = train_test_split(baseline_df, train_variable = 'both_rates', \n",
    "                                                                                                   target_variable = 'target_pos', num_folds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test trials  8\n",
      "Val trials 7\n"
     ]
    }
   ],
   "source": [
    "X_train_s, y_train_s, X_val_s, y_val_s, X_test_s, y_test_s, info_train_s, info_val_s, info_test_s = train_test_split(stim_df, train_variable = 'both_rates', \n",
    "                                                                                                   target_variable = 'target_pos', num_folds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are testing the optimization method on fold  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f82ee1cf0f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test one of the folds first\n",
    "fold_num = 'fold0'\n",
    "fold = 0\n",
    "\n",
    "print('We are testing the optimization method on fold ', fold)\n",
    "\n",
    "def input_mats(x,y, seq_length = 75):\n",
    "    x = x[fold_num]\n",
    "    y = y[fold_num]\n",
    "    x = x.reshape(x.shape[0] // seq_length, seq_length, x.shape[1])  \n",
    "    y = y.reshape(y.shape[0] // seq_length, seq_length, y.shape[1])  \n",
    "    return x,y\n",
    "\n",
    "x_train_base, y_train_base = input_mats(X_train_b, y_train_b)\n",
    "x_train_stim, y_train_stim = input_mats(X_train_s, y_train_s)\n",
    "\n",
    "x_val_base, y_val_base = input_mats(X_val_b, y_val_b)\n",
    "x_val_stim, y_val_stim = input_mats(X_val_s, y_val_s)\n",
    "\n",
    "x_test_base, y_test_base = input_mats(X_test_b, y_test_b)\n",
    "x_test_stim, y_test_stim = input_mats(X_test_s, y_test_s)\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f82ee1cf0f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify that we want our tensors on the GPU and in float32\n",
    "device = torch.device('cuda:0') #suposed to be cuda\n",
    "dtype = torch.float32\n",
    "path_to_models = './Models'\n",
    "\n",
    "num_dim_output = y_train_stim.shape[2]\n",
    "num_features = x_train_stim.shape[2]\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for training\n",
    "num_epochs = 50\n",
    "batch_size_train = 25\n",
    "batch_size_val = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "\n",
    "    def __init__(self, y, X, seq_length):\n",
    "        \"\"\"\n",
    "        Initializes the SequenceDataset.\n",
    "        \n",
    "        Args:\n",
    "            y (torch.Tensor): The target labels for each sequence.\n",
    "            X (torch.Tensor): The input sequences.\n",
    "            sequence_length (int): The desired length of each sequence.\n",
    "        \"\"\"\n",
    "        self.sequence_length = seq_length\n",
    "        self.y = torch.tensor(y)\n",
    "        self.X = torch.tensor(X)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return self.X.shape[0] * self.X.shape[1]\n",
    "\n",
    "    def __getitem__(self, i): \n",
    "        \"\"\"\n",
    "        Gets the i-th sample from the dataset.\n",
    "        \n",
    "        Args:\n",
    "            i (int): Index of the desired sample.\n",
    "        \n",
    "        Returns:\n",
    "            xx (torch.Tensor): Input sequence of length sequence_length.\n",
    "            yy (torch.Tensor): Corresponding target sequence.\n",
    "        \"\"\"\n",
    "        trial_index = i // self.X.shape[1]\n",
    "        point_index = i % self.X.shape[1]\n",
    "        \n",
    "        if point_index > self.sequence_length - 1:\n",
    "            point_start = point_index - self.sequence_length\n",
    "            xx = self.X[trial_index, point_start:point_index, :]\n",
    "            yy = self.y[trial_index, point_start+1:point_index+1, :]\n",
    "        else:\n",
    "            padding_x = self.X[trial_index, 0:1, :].repeat(self.sequence_length - point_index, 1)\n",
    "            padding_y = self.y[trial_index, 0:1, :].repeat(self.sequence_length - point_index - 1, 1)\n",
    "            xx = self.X[trial_index, 0:point_index, :]\n",
    "            xx = torch.cat((padding_x, xx), dim=0)\n",
    "            yy = self.y[trial_index, 0:point_index + 1, :]\n",
    "            yy = torch.cat((padding_y, yy), dim=0)\n",
    "            \n",
    "        return xx, yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(X, y, delta):\n",
    "    residual = torch.abs(X - y)\n",
    "    condition = residual < delta\n",
    "    loss = torch.where(condition, 0.5 * residual**2, delta * residual - 0.5 * delta**2)\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_hnet_noweights(weights, alpha, l1_ratio):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implement an L1-L2 penalty on the norm of the model weights.\n",
    "\n",
    "    model: MLP\n",
    "    alpha: scaling parameter for the regularization.\n",
    "    l1_ratio: mixing parameter between L1 and L2 loss.\n",
    "\n",
    "    Returns:\n",
    "    reg: regularization term\n",
    "    \"\"\"\n",
    "    l1_loss = 0\n",
    "    l2_loss = 0\n",
    "\n",
    "    # Accumulate L1 and L2 losses for weight matrices in the model\n",
    "    for weight_tensor in weights[1:2]:\n",
    "        l1_loss += torch.sum(torch.abs(weight_tensor))\n",
    "        l2_loss += torch.sum(weight_tensor.pow(2))\n",
    "\n",
    "    reg = l1_ratio * l1_loss + (1 - l1_ratio) * l2_loss\n",
    "\n",
    "    reg = alpha * reg\n",
    "\n",
    "    return reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the LSTM model\n",
    "def train_model_optuna(trial):\n",
    "    \n",
    "    ###########################################################\n",
    "    ### Definition of inputs\n",
    "    ###########################################################\n",
    "    X_base = x_train_base\n",
    "    Y_base = y_train_base\n",
    "    X_val_base = x_val_base\n",
    "    Y_val_base = y_val_base\n",
    "\n",
    "    X_stim = x_train_stim\n",
    "    Y_stim = y_train_stim\n",
    "    X_val_stim = x_val_stim\n",
    "    Y_val_stim = y_val_stim\n",
    "\n",
    "\n",
    "    ###########################################################\n",
    "    ### Definition of hyperparameters and model for main network\n",
    "    ###########################################################\n",
    "    hidden_units = trial.suggest_int('hidden_units', 10, 50)\n",
    "    input_rec = trial.suggest_int('input_rec', 30, 60)\n",
    "    seq_length_LSTM = trial.suggest_int('seq_length_LSTM', 10, 20)\n",
    "\n",
    "\n",
    "    model = SimpleRNN(n_in=num_features, rnn_layers=(hidden_units,), \n",
    "                    fc_layers_pre=(input_rec,), fc_layers=(num_dim_output,),\n",
    "                    use_lstm = True, no_weights = True).to(device)\n",
    "    \n",
    "\n",
    "    ###########################################################\n",
    "    ### Definition of hyperparameters and model for hypernetwork\n",
    "    ###########################################################\n",
    "    num_conditions = 2 # fixed, as we only have baseline and stimulation\n",
    "    size_task_embedding = trial.suggest_int('size_task_embedding', 4, 50)\n",
    "    num_units_hnet = trial.suggest_int('num_units_hnet', 10,50)\n",
    "    num_layers_hnet = trial.suggest_int('num_layers_hnet', 1,4)\n",
    "    layers = [num_units_hnet for i in range(num_layers_hnet)]\n",
    "    # Model\n",
    "    hnet = HMLP(model.param_shapes, uncond_in_size=0,\n",
    "             cond_in_size=size_task_embedding,\n",
    "            layers=layers, \n",
    "            num_cond_embs=num_conditions).to(device)\n",
    "    \n",
    "    hnet.apply_hyperfan_init(mnet=model)\n",
    "\n",
    "    ###########################################################\n",
    "    ### Fixed training parameters\n",
    "    ###########################################################\n",
    "    num_epochs= 1000\n",
    "    early_stop = 5\n",
    "    \n",
    "\n",
    "    ###########################################################\n",
    "    ### Defining training hyperparameters to optimize\n",
    "    ###########################################################\n",
    "\n",
    "    # Set up the optimizer with the specified learning rate\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log = True)\n",
    "    # Set up the optimizer with the specified learning rate\n",
    "    optimizer = torch.optim.Adam(hnet.internal_params, lr=lr)\n",
    "\n",
    "    # Set up a learning rate scheduler\n",
    "    lr_step_size = 10 # trial.suggest_int('lr_step_size', 5, 15)\n",
    "    lr_gamma = 0.9 # trial.suggest_float('lr_gamma', 1, 1.5)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, \n",
    "                                    step_size=lr_step_size, \n",
    "                                    gamma=lr_gamma)\n",
    "    # hyperparameter for huber loss\n",
    "    delta = trial.suggest_int('huber_delta', 5, 10) #was 8 before\n",
    "\n",
    "    # hyperparameter for regularizer\n",
    "    alpha = trial.suggest_float('alpha_reg', 1e-7, 1e-3, log = True) # was 1e-5 before\n",
    "    l1_ratio =trial.suggest_float('l1_ratio_reg', 0.2, 0.8) #was 0.5 before\n",
    "\n",
    "    # Reshaping data and creating batchs\n",
    "    batch_size_train = trial.suggest_int('batch_size_train', 20, 50)\n",
    "    batch_size_val = trial.suggest_int('batch_size_val',20, 50)\n",
    "\n",
    "    train_dataset_baseline = SequenceDataset(\n",
    "    y_train_base,    x_train_base,    seq_length=seq_length_LSTM)\n",
    "    train_dataset_stim = SequenceDataset(\n",
    "    y_train_stim,    x_train_stim,    seq_length=seq_length_LSTM)\n",
    "    val_dataset_baseline = SequenceDataset(\n",
    "    y_val_base,    x_val_base,    seq_length=seq_length_LSTM)\n",
    "    val_dataset_stim = SequenceDataset(\n",
    "    y_val_stim,    x_val_stim,    seq_length=seq_length_LSTM)\n",
    "\n",
    "    loader_train_b = data.DataLoader(train_dataset_baseline, batch_size=batch_size_train, shuffle=True)\n",
    "    loader_train_s = data.DataLoader(train_dataset_stim, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "    loader_val_b = data.DataLoader(val_dataset_baseline, batch_size=batch_size_val, shuffle=True)\n",
    "    loader_val_s = data.DataLoader(val_dataset_stim, batch_size=batch_size_val, shuffle=True)\n",
    "\n",
    "\n",
    "    #####################################################\n",
    "    ########## START TRAINING \n",
    "    #####################################################\n",
    "\n",
    "    # Keep track of the best model's parameters and loss\n",
    "    best_model_wts = deepcopy(model.state_dict())\n",
    "    best_loss = 1e8\n",
    "\n",
    "    # Enable anomaly detection for debugging\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    # Track the train and validation loss\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    # Counters for early stopping\n",
    "    not_increased = 0\n",
    "    end_train = 0\n",
    "    \n",
    "    regularizer = True\n",
    "\n",
    "    # Loop through epochs\n",
    "    for epoch in np.arange(num_epochs):\n",
    "        for phase in ['train', 'val']:\n",
    "            # set model to train/validation as appropriate\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "                loaders = zip(loader_train_b, loader_train_s)\n",
    "            else:\n",
    "                model.eval()\n",
    "                loaders = zip(loader_val_b, loader_val_s)\n",
    "\n",
    "            # Initialize variables to track loss and batch size\n",
    "            running_loss = 0\n",
    "            running_size = 0        \n",
    "\n",
    "            # Iterate over batches in the loader\n",
    "            for data_b, data_s in loaders:\n",
    "\n",
    "                # Define data for this batch\n",
    "                x_b = data_b[0].to('cuda')\n",
    "                y_b = data_b[1].to('cuda')\n",
    "                x_s = data_s[0].to('cuda')\n",
    "                y_s = data_s[1].to('cuda')\n",
    "\n",
    "                \n",
    "                if phase == \"train\":\n",
    "                    with torch.set_grad_enabled(True):\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        # Compute BASELINE loss.\n",
    "                        W_base = hnet(cond_id=0)\n",
    "                        base_P = model.forward(x_b, weights=W_base)\n",
    "                        base_P = torch.squeeze(base_P) # torch.sigmoid(base_P))\n",
    "                        loss_base = huber_loss(base_P, y_b, delta = delta)\n",
    "                        \n",
    "                        \n",
    "                        # Compute STIMULATION loss.\n",
    "                        W_stim = hnet(cond_id=1)\n",
    "                        stim_P = model.forward(x_s, weights=W_stim)\n",
    "                        stim_P = torch.squeeze(stim_P) #torch.sigmoid(stim_P))\n",
    "                        loss_stim = huber_loss(stim_P, y_s, delta = delta)\n",
    "\n",
    "                        # Combine loss for 2 tasks\n",
    "                        loss_t = loss_base + loss_stim    #only for printing\n",
    "\n",
    "                        # Add regularization to the loss in the training phase\n",
    "                        if regularizer is not None:\n",
    "                            loss_stim_reg = loss_stim + reg_hnet_noweights(W_stim, l1_ratio, alpha)\n",
    "                            loss_base_reg = loss_base + reg_hnet_noweights(W_base, l1_ratio, alpha)\n",
    "                            # Combine loss for 2 tasks\n",
    "                            loss_t_r = loss_base_reg + loss_stim_reg\n",
    "\n",
    "                        else:               \n",
    "                            loss_t_r = loss_t \n",
    "\n",
    "                        # Compute gradients and perform an optimization step\n",
    "                        loss_t_r.backward()\n",
    "                        optimizer.step()\n",
    "                else:\n",
    "                    # just compute the loss in validation phase\n",
    "                    W_base = hnet(cond_id=0)\n",
    "                    base_P = model.forward(x_b, weights=W_base)\n",
    "                    base_P = torch.squeeze(base_P) #torch.sigmoid(base_P))\n",
    "                    loss_base = huber_loss(base_P, y_b, delta = delta)\n",
    "\n",
    "                    W_stim = hnet(cond_id=1)\n",
    "                    stim_P = model.forward(x_s, weights=W_stim)\n",
    "                    stim_P = torch.squeeze(stim_P) #torch.sigmoid(stim_P))\n",
    "                    loss_stim = huber_loss(stim_P, y_s, delta = delta)\n",
    "\n",
    "                    loss_t = loss_base + loss_stim\n",
    "\n",
    "                # Ensure the loss is finite\n",
    "                assert torch.isfinite(loss_t)\n",
    "                assert torch.isfinite(loss_t_r)\n",
    "                running_loss += loss_t.item()\n",
    "                running_size += 1\n",
    "\n",
    "            # compute the train/validation loss and update the best\n",
    "            # model parameters if this is the lowest validation loss yet\n",
    "            running_loss /= running_size\n",
    "            if phase == \"train\":\n",
    "                train_losses.append(running_loss)\n",
    "            else:\n",
    "                val_losses.append(running_loss)\n",
    "                \n",
    "                # Update best model parameters if validation loss improves\n",
    "                if running_loss < best_loss:\n",
    "                    best_loss = running_loss\n",
    "                    best_model_wts = deepcopy(model.state_dict())\n",
    "                    not_increased = 0\n",
    "                else:\n",
    "                    # Perform early stopping if validation loss doesn't improve\n",
    "                    if epoch > 10:\n",
    "                        not_increased += 1\n",
    "                        # print('Not increased : {}/5'.format(not_increased))\n",
    "                        if not_increased == early_stop:\n",
    "                            print('Decrease LR')\n",
    "                            for g in optimizer.param_groups:\n",
    "                                g['lr'] = g['lr'] / 2\n",
    "                            not_increased = 0\n",
    "                            end_train += 1\n",
    "                        \n",
    "                        if end_train == 2:\n",
    "                            model.load_state_dict(best_model_wts)\n",
    "                            return best_loss\n",
    "                        \n",
    "\n",
    "        # Update learning rate with the scheduler\n",
    "        scheduler.step()\n",
    "        #print(\"Epoch {:03} Train {:.4f} Val {:.4f}\".format(epoch, train_losses[-1], val_losses[-1]))\n",
    "\n",
    "\n",
    "        trial.report(val_losses[-1], epoch)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    \n",
    "\n",
    "    return best_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 16:49:57,077] A new study created in memory with name: no-name-e83fc5d1-77d2-4094-bc5a-4993a98882e8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a simple RNN with 10428 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 357529 weights and 10428 outputs (compression ratio: 34.29).\n",
      "The network consists of 357489 unconditional weights (357489 internally maintained) and 40 conditional weights (40 internally maintained).\n",
      "Epoch 000 Train 4.7249 Val 3.7838\n",
      "Epoch 001 Train 3.1366 Val 2.4792\n",
      "Epoch 002 Train 1.8752 Val 1.4785\n",
      "Epoch 003 Train 1.0148 Val 0.8099\n",
      "Epoch 004 Train 0.5244 Val 0.4099\n",
      "Epoch 005 Train 0.2646 Val 0.2443\n",
      "Epoch 006 Train 0.1701 Val 0.1808\n",
      "Epoch 007 Train 0.1355 Val 0.1501\n",
      "Epoch 008 Train 0.1131 Val 0.1369\n",
      "Epoch 009 Train 0.1040 Val 0.1244\n",
      "Epoch 010 Train 0.0988 Val 0.1249\n",
      "Epoch 011 Train 0.0921 Val 0.1155\n",
      "Epoch 012 Train 0.0884 Val 0.1111\n",
      "Epoch 013 Train 0.0832 Val 0.1082\n",
      "Epoch 014 Train 0.0789 Val 0.1050\n",
      "Epoch 015 Train 0.0771 Val 0.1033\n",
      "Epoch 016 Train 0.0730 Val 0.0978\n",
      "Epoch 017 Train 0.0732 Val 0.1011\n",
      "Epoch 018 Train 0.0692 Val 0.1015\n",
      "Epoch 019 Train 0.0670 Val 0.0959\n",
      "Epoch 020 Train 0.0656 Val 0.0880\n",
      "Epoch 021 Train 0.0625 Val 0.0894\n",
      "Epoch 022 Train 0.0600 Val 0.0903\n",
      "Epoch 023 Train 0.0606 Val 0.0877\n",
      "Epoch 024 Train 0.0586 Val 0.0877\n",
      "Epoch 025 Train 0.0563 Val 0.0813\n",
      "Epoch 026 Train 0.0532 Val 0.0828\n",
      "Epoch 027 Train 0.0524 Val 0.0802\n",
      "Epoch 028 Train 0.0513 Val 0.0791\n",
      "Epoch 029 Train 0.0498 Val 0.0767\n",
      "Epoch 030 Train 0.0502 Val 0.0784\n",
      "Epoch 031 Train 0.0486 Val 0.0788\n",
      "Epoch 032 Train 0.0470 Val 0.0745\n",
      "Epoch 033 Train 0.0472 Val 0.0742\n",
      "Epoch 034 Train 0.0458 Val 0.0785\n",
      "Epoch 035 Train 0.0443 Val 0.0792\n",
      "Epoch 036 Train 0.0435 Val 0.0746\n",
      "Epoch 037 Train 0.0420 Val 0.0737\n",
      "Epoch 038 Train 0.0418 Val 0.0694\n",
      "Epoch 039 Train 0.0413 Val 0.0729\n",
      "Epoch 040 Train 0.0409 Val 0.0698\n",
      "Epoch 041 Train 0.0392 Val 0.0711\n",
      "Epoch 042 Train 0.0382 Val 0.0667\n",
      "Epoch 043 Train 0.0384 Val 0.0690\n",
      "Epoch 044 Train 0.0371 Val 0.0655\n",
      "Epoch 045 Train 0.0365 Val 0.0664\n",
      "Epoch 046 Train 0.0362 Val 0.0647\n",
      "Epoch 047 Train 0.0363 Val 0.0668\n",
      "Epoch 048 Train 0.0352 Val 0.0685\n",
      "Epoch 049 Train 0.0349 Val 0.0648\n",
      "Epoch 050 Train 0.0342 Val 0.0649\n",
      "Decrease LR\n",
      "Epoch 051 Train 0.0338 Val 0.0660\n",
      "Epoch 052 Train 0.0342 Val 0.0677\n",
      "Epoch 053 Train 0.0330 Val 0.0622\n",
      "Epoch 054 Train 0.0326 Val 0.0617\n",
      "Epoch 055 Train 0.0328 Val 0.0642\n",
      "Epoch 056 Train 0.0325 Val 0.0674\n",
      "Epoch 057 Train 0.0322 Val 0.0614\n",
      "Epoch 058 Train 0.0324 Val 0.0641\n",
      "Epoch 059 Train 0.0322 Val 0.0637\n",
      "Epoch 060 Train 0.0321 Val 0.0609\n",
      "Epoch 061 Train 0.0312 Val 0.0629\n",
      "Epoch 062 Train 0.0312 Val 0.0635\n",
      "Epoch 063 Train 0.0307 Val 0.0647\n",
      "Epoch 064 Train 0.0314 Val 0.0626\n",
      "Epoch 065 Train 0.0302 Val 0.0599\n",
      "Epoch 066 Train 0.0305 Val 0.0600\n",
      "Epoch 067 Train 0.0308 Val 0.0638\n",
      "Epoch 068 Train 0.0304 Val 0.0603\n",
      "Epoch 069 Train 0.0295 Val 0.0589\n",
      "Epoch 070 Train 0.0300 Val 0.0602\n",
      "Epoch 071 Train 0.0298 Val 0.0613\n",
      "Epoch 072 Train 0.0297 Val 0.0600\n",
      "Epoch 073 Train 0.0298 Val 0.0589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 17:15:29,646] Trial 0 finished with value: 0.05891955678222606 and parameters: {'hidden_units': 19, 'input_rec': 43, 'seq_length_LSTM': 11, 'size_task_embedding': 20, 'num_units_hnet': 33, 'num_layers_hnet': 3, 'lr': 2.0059438162661392e-05, 'huber_delta': 9, 'alpha_reg': 0.00011252632910055825, 'l1_ratio_reg': 0.3651107054885052, 'batch_size_train': 32, 'batch_size_val': 36}. Best is trial 0 with value: 0.05891955678222606.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrease LR\n",
      "Creating a simple RNN with 8669 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 418560 weights and 8669 outputs (compression ratio: 48.28).\n",
      "The network consists of 418462 unconditional weights (418462 internally maintained) and 98 conditional weights (98 internally maintained).\n",
      "Epoch 000 Train 1.5703 Val 1.4964\n",
      "Epoch 001 Train 1.1810 Val 1.1705\n",
      "Epoch 002 Train 0.8924 Val 0.9223\n",
      "Epoch 003 Train 0.6403 Val 0.6737\n",
      "Epoch 004 Train 0.4489 Val 0.5003\n",
      "Epoch 005 Train 0.3220 Val 0.3741\n",
      "Epoch 006 Train 0.2440 Val 0.2894\n",
      "Epoch 007 Train 0.1925 Val 0.2560\n",
      "Epoch 008 Train 0.1551 Val 0.2171\n",
      "Epoch 009 Train 0.1329 Val 0.2003\n",
      "Epoch 010 Train 0.1153 Val 0.1839\n",
      "Epoch 011 Train 0.1028 Val 0.1708\n",
      "Epoch 012 Train 0.0914 Val 0.1611\n",
      "Epoch 013 Train 0.0830 Val 0.1510\n",
      "Epoch 014 Train 0.0764 Val 0.1391\n",
      "Epoch 015 Train 0.0711 Val 0.1302\n",
      "Epoch 016 Train 0.0649 Val 0.1194\n",
      "Epoch 017 Train 0.0614 Val 0.1257\n",
      "Epoch 018 Train 0.0583 Val 0.1151\n",
      "Epoch 019 Train 0.0542 Val 0.1199\n",
      "Epoch 020 Train 0.0514 Val 0.1106\n",
      "Epoch 021 Train 0.0489 Val 0.1115\n",
      "Epoch 022 Train 0.0485 Val 0.1040\n",
      "Epoch 023 Train 0.0448 Val 0.1000\n",
      "Epoch 024 Train 0.0435 Val 0.0925\n",
      "Epoch 025 Train 0.0423 Val 0.0981\n",
      "Epoch 026 Train 0.0399 Val 0.0934\n",
      "Epoch 027 Train 0.0388 Val 0.0948\n",
      "Epoch 028 Train 0.0388 Val 0.0919\n",
      "Epoch 029 Train 0.0372 Val 0.0903\n",
      "Epoch 030 Train 0.0364 Val 0.0893\n",
      "Epoch 031 Train 0.0361 Val 0.0920\n",
      "Epoch 032 Train 0.0335 Val 0.0913\n",
      "Epoch 033 Train 0.0337 Val 0.0892\n",
      "Epoch 034 Train 0.0325 Val 0.0878\n",
      "Epoch 035 Train 0.0319 Val 0.0895\n",
      "Epoch 036 Train 0.0318 Val 0.0893\n",
      "Epoch 037 Train 0.0305 Val 0.0870\n",
      "Epoch 038 Train 0.0302 Val 0.0866\n",
      "Epoch 039 Train 0.0291 Val 0.0845\n",
      "Epoch 040 Train 0.0286 Val 0.0833\n",
      "Epoch 041 Train 0.0277 Val 0.0830\n",
      "Epoch 042 Train 0.0276 Val 0.0845\n",
      "Epoch 043 Train 0.0273 Val 0.0831\n",
      "Epoch 044 Train 0.0270 Val 0.0759\n",
      "Epoch 045 Train 0.0272 Val 0.0768\n",
      "Epoch 046 Train 0.0267 Val 0.0782\n",
      "Epoch 047 Train 0.0261 Val 0.0743\n",
      "Epoch 048 Train 0.0253 Val 0.0758\n",
      "Epoch 049 Train 0.0252 Val 0.0753\n",
      "Epoch 050 Train 0.0247 Val 0.0776\n",
      "Epoch 051 Train 0.0247 Val 0.0780\n",
      "Decrease LR\n",
      "Epoch 052 Train 0.0242 Val 0.0753\n",
      "Epoch 053 Train 0.0239 Val 0.0773\n",
      "Epoch 054 Train 0.0238 Val 0.0768\n",
      "Epoch 055 Train 0.0242 Val 0.0789\n",
      "Epoch 056 Train 0.0235 Val 0.0773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 17:31:01,764] Trial 1 finished with value: 0.07427431724078508 and parameters: {'hidden_units': 14, 'input_rec': 42, 'seq_length_LSTM': 14, 'size_task_embedding': 49, 'num_units_hnet': 47, 'num_layers_hnet': 1, 'lr': 1.3070208278080213e-05, 'huber_delta': 9, 'alpha_reg': 0.0002070864599485419, 'l1_ratio_reg': 0.3388216255543003, 'batch_size_train': 38, 'batch_size_val': 23}. Best is trial 0 with value: 0.05891955678222606.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrease LR\n",
      "Creating a simple RNN with 19299 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 815462 weights and 19299 outputs (compression ratio: 42.25).\n",
      "The network consists of 815396 unconditional weights (815396 internally maintained) and 66 conditional weights (66 internally maintained).\n",
      "Epoch 000 Train 0.1082 Val 0.0679\n",
      "Epoch 001 Train 0.0516 Val 0.0661\n",
      "Epoch 002 Train 0.0485 Val 0.0696\n",
      "Epoch 003 Train 0.0457 Val 0.0608\n",
      "Epoch 004 Train 0.0375 Val 0.0593\n",
      "Epoch 005 Train 0.0300 Val 0.0593\n",
      "Epoch 006 Train 0.0253 Val 0.0579\n",
      "Epoch 007 Train 0.0214 Val 0.0542\n",
      "Epoch 008 Train 0.0198 Val 0.0502\n",
      "Epoch 009 Train 0.0181 Val 0.0523\n",
      "Epoch 010 Train 0.0169 Val 0.0503\n",
      "Epoch 011 Train 0.0158 Val 0.0484\n",
      "Epoch 012 Train 0.0150 Val 0.0474\n",
      "Epoch 013 Train 0.0143 Val 0.0498\n",
      "Epoch 014 Train 0.0134 Val 0.0479\n",
      "Epoch 015 Train 0.0127 Val 0.0505\n",
      "Epoch 016 Train 0.0123 Val 0.0470\n",
      "Epoch 017 Train 0.0122 Val 0.0470\n",
      "Epoch 018 Train 0.0115 Val 0.0482\n",
      "Epoch 019 Train 0.0102 Val 0.0467\n",
      "Epoch 020 Train 0.0102 Val 0.0488\n",
      "Epoch 021 Train 0.0100 Val 0.0486\n",
      "Epoch 022 Train 0.0094 Val 0.0493\n",
      "Epoch 023 Train 0.0092 Val 0.0504\n",
      "Epoch 024 Train 0.0088 Val 0.0467\n",
      "Epoch 025 Train 0.0087 Val 0.0490\n",
      "Epoch 026 Train 0.0081 Val 0.0460\n",
      "Epoch 027 Train 0.0081 Val 0.0475\n",
      "Epoch 028 Train 0.0075 Val 0.0473\n",
      "Epoch 029 Train 0.0073 Val 0.0484\n",
      "Epoch 030 Train 0.0067 Val 0.0475\n",
      "Decrease LR\n",
      "Epoch 031 Train 0.0066 Val 0.0484\n",
      "Epoch 032 Train 0.0065 Val 0.0495\n",
      "Epoch 033 Train 0.0064 Val 0.0515\n",
      "Epoch 034 Train 0.0064 Val 0.0484\n",
      "Epoch 035 Train 0.0060 Val 0.0495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 17:40:46,464] Trial 2 finished with value: 0.04602542646080345 and parameters: {'hidden_units': 32, 'input_rec': 58, 'seq_length_LSTM': 13, 'size_task_embedding': 33, 'num_units_hnet': 41, 'num_layers_hnet': 3, 'lr': 0.00044443365152317953, 'huber_delta': 8, 'alpha_reg': 1.512076567610703e-07, 'l1_ratio_reg': 0.7524167352917817, 'batch_size_train': 34, 'batch_size_val': 27}. Best is trial 2 with value: 0.04602542646080345.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrease LR\n",
      "Creating a simple RNN with 17876 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 521588 weights and 17876 outputs (compression ratio: 29.18).\n",
      "The network consists of 521540 unconditional weights (521540 internally maintained) and 48 conditional weights (48 internally maintained).\n",
      "Epoch 000 Train 0.1564 Val 0.0943\n",
      "Epoch 001 Train 0.0653 Val 0.0779\n",
      "Epoch 002 Train 0.0396 Val 0.0571\n",
      "Epoch 003 Train 0.0305 Val 0.0491\n",
      "Epoch 004 Train 0.0257 Val 0.0479\n",
      "Epoch 005 Train 0.0226 Val 0.0455\n",
      "Epoch 006 Train 0.0211 Val 0.0451\n",
      "Epoch 007 Train 0.0183 Val 0.0432\n",
      "Epoch 008 Train 0.0166 Val 0.0414\n",
      "Epoch 009 Train 0.0141 Val 0.0425\n",
      "Epoch 010 Train 0.0123 Val 0.0436\n",
      "Epoch 011 Train 0.0109 Val 0.0447\n",
      "Epoch 012 Train 0.0095 Val 0.0473\n",
      "Epoch 013 Train 0.0078 Val 0.0467\n",
      "Epoch 014 Train 0.0067 Val 0.0454\n",
      "Decrease LR\n",
      "Epoch 015 Train 0.0057 Val 0.0438\n",
      "Epoch 016 Train 0.0051 Val 0.0462\n",
      "Epoch 017 Train 0.0046 Val 0.0449\n",
      "Epoch 018 Train 0.0044 Val 0.0464\n",
      "Epoch 019 Train 0.0038 Val 0.0447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 17:46:15,647] Trial 3 finished with value: 0.04140114768215861 and parameters: {'hidden_units': 39, 'input_rec': 40, 'seq_length_LSTM': 15, 'size_task_embedding': 24, 'num_units_hnet': 28, 'num_layers_hnet': 4, 'lr': 0.0016783133238250521, 'huber_delta': 6, 'alpha_reg': 0.0002577339781036089, 'l1_ratio_reg': 0.672162300160937, 'batch_size_train': 43, 'batch_size_val': 45}. Best is trial 3 with value: 0.04140114768215861.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrease LR\n",
      "Creating a simple RNN with 20416 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 822302 weights and 20416 outputs (compression ratio: 40.28).\n",
      "The network consists of 822256 unconditional weights (822256 internally maintained) and 46 conditional weights (46 internally maintained).\n",
      "Epoch 000 Train 0.2725 Val 0.0684\n",
      "Epoch 001 Train 0.0407 Val 0.0593\n",
      "Epoch 002 Train 0.0308 Val 0.0621\n",
      "Epoch 003 Train 0.0281 Val 0.0540\n",
      "Epoch 004 Train 0.0264 Val 0.0476\n",
      "Epoch 005 Train 0.0254 Val 0.0472\n",
      "Epoch 006 Train 0.0246 Val 0.0475\n",
      "Epoch 007 Train 0.0233 Val 0.0479\n",
      "Epoch 008 Train 0.0230 Val 0.0471\n",
      "Epoch 009 Train 0.0227 Val 0.0453\n",
      "Epoch 010 Train 0.0218 Val 0.0446\n",
      "Epoch 011 Train 0.0214 Val 0.0444\n",
      "Epoch 012 Train 0.0205 Val 0.0465\n",
      "Epoch 013 Train 0.0199 Val 0.0448\n",
      "Epoch 014 Train 0.0186 Val 0.0421\n",
      "Epoch 015 Train 0.0188 Val 0.0438\n",
      "Epoch 016 Train 0.0179 Val 0.0432\n",
      "Epoch 017 Train 0.0164 Val 0.0434\n",
      "Epoch 018 Train 0.0162 Val 0.0432\n",
      "Decrease LR\n",
      "Epoch 019 Train 0.0160 Val 0.0452\n",
      "Epoch 020 Train 0.0152 Val 0.0443\n",
      "Epoch 021 Train 0.0155 Val 0.0425\n",
      "Epoch 022 Train 0.0146 Val 0.0442\n",
      "Epoch 023 Train 0.0142 Val 0.0449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 17:52:50,085] Trial 4 finished with value: 0.0421086738023414 and parameters: {'hidden_units': 47, 'input_rec': 35, 'seq_length_LSTM': 14, 'size_task_embedding': 23, 'num_units_hnet': 39, 'num_layers_hnet': 4, 'lr': 0.00019204824128562557, 'huber_delta': 7, 'alpha_reg': 0.00015148875775767496, 'l1_ratio_reg': 0.4685122952188539, 'batch_size_train': 47, 'batch_size_val': 44}. Best is trial 3 with value: 0.04140114768215861.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrease LR\n",
      "Creating a simple RNN with 17398 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 453212 weights and 17398 outputs (compression ratio: 26.05).\n",
      "The network consists of 453198 unconditional weights (453198 internally maintained) and 14 conditional weights (14 internally maintained).\n",
      "Epoch 000 Train 0.4158 Val 0.1497\n",
      "Epoch 001 Train 0.0851 Val 0.0752\n",
      "Epoch 002 Train 0.0487 Val 0.0636\n",
      "Epoch 003 Train 0.0401 Val 0.0574\n",
      "Epoch 004 Train 0.0347 Val 0.0532\n",
      "Epoch 005 Train 0.0299 Val 0.0483\n",
      "Epoch 006 Train 0.0261 Val 0.0482\n",
      "Epoch 007 Train 0.0232 Val 0.0496\n",
      "Epoch 008 Train 0.0210 Val 0.0489\n",
      "Epoch 009 Train 0.0189 Val 0.0494\n",
      "Epoch 010 Train 0.0171 Val 0.0462\n",
      "Epoch 011 Train 0.0160 Val 0.0475\n",
      "Epoch 012 Train 0.0145 Val 0.0470\n",
      "Epoch 013 Train 0.0133 Val 0.0481\n",
      "Epoch 014 Train 0.0129 Val 0.0460\n",
      "Epoch 015 Train 0.0116 Val 0.0480\n",
      "Epoch 016 Train 0.0109 Val 0.0472\n",
      "Epoch 017 Train 0.0104 Val 0.0485\n",
      "Epoch 018 Train 0.0101 Val 0.0500\n",
      "Decrease LR\n",
      "Epoch 019 Train 0.0091 Val 0.0476\n",
      "Epoch 020 Train 0.0089 Val 0.0465\n",
      "Epoch 021 Train 0.0086 Val 0.0467\n",
      "Epoch 022 Train 0.0084 Val 0.0487\n",
      "Epoch 023 Train 0.0081 Val 0.0448\n",
      "Epoch 024 Train 0.0078 Val 0.0464\n",
      "Epoch 025 Train 0.0074 Val 0.0480\n",
      "Epoch 026 Train 0.0073 Val 0.0478\n",
      "Epoch 027 Train 0.0073 Val 0.0468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 18:00:32,884] Trial 5 finished with value: 0.04475161809498376 and parameters: {'hidden_units': 41, 'input_rec': 35, 'seq_length_LSTM': 10, 'size_task_embedding': 7, 'num_units_hnet': 25, 'num_layers_hnet': 2, 'lr': 7.968432978600385e-05, 'huber_delta': 10, 'alpha_reg': 1.6392655864135365e-06, 'l1_ratio_reg': 0.5641664026718061, 'batch_size_train': 30, 'batch_size_val': 41}. Best is trial 3 with value: 0.04140114768215861.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrease LR\n",
      "Creating a simple RNN with 13995 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 660213 weights and 13995 outputs (compression ratio: 47.17).\n",
      "The network consists of 660203 unconditional weights (660203 internally maintained) and 10 conditional weights (10 internally maintained).\n",
      "Epoch 000 Train 0.5031 Val 0.4603\n",
      "Epoch 001 Train 0.3186 Val 0.2983\n",
      "Epoch 002 Train 0.1888 Val 0.1954\n",
      "Epoch 003 Train 0.1178 Val 0.1445\n",
      "Epoch 004 Train 0.0832 Val 0.1120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 18:02:05,871] Trial 6 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 Train 0.0664 Val 0.0997\n",
      "Creating a simple RNN with 21852 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 858312 weights and 21852 outputs (compression ratio: 39.28).\n",
      "The network consists of 858232 unconditional weights (858232 internally maintained) and 80 conditional weights (80 internally maintained).\n",
      "Epoch 000 Train 0.6765 Val 0.4705\n",
      "Epoch 001 Train 0.2547 Val 0.2262\n",
      "Epoch 002 Train 0.1280 Val 0.1368\n",
      "Epoch 003 Train 0.0859 Val 0.1042\n",
      "Epoch 004 Train 0.0672 Val 0.0882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 18:03:47,136] Trial 7 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 Train 0.0571 Val 0.0776\n",
      "Creating a simple RNN with 10325 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 527977 weights and 10325 outputs (compression ratio: 51.14).\n",
      "The network consists of 527925 unconditional weights (527925 internally maintained) and 52 conditional weights (52 internally maintained).\n",
      "Epoch 000 Train 0.2581 Val 0.0700\n",
      "Epoch 001 Train 0.0370 Val 0.0544\n",
      "Epoch 002 Train 0.0212 Val 0.0489\n",
      "Epoch 003 Train 0.0155 Val 0.0467\n",
      "Epoch 004 Train 0.0125 Val 0.0482\n",
      "Epoch 005 Train 0.0098 Val 0.0499\n",
      "Epoch 006 Train 0.0077 Val 0.0541\n",
      "Epoch 007 Train 0.0062 Val 0.0531\n",
      "Epoch 008 Train 0.0051 Val 0.0581\n",
      "Epoch 009 Train 0.0039 Val 0.0528\n",
      "Epoch 010 Train 0.0032 Val 0.0520\n",
      "Epoch 011 Train 0.0026 Val 0.0508\n",
      "Epoch 012 Train 0.0022 Val 0.0535\n",
      "Epoch 013 Train 0.0018 Val 0.0532\n",
      "Epoch 014 Train 0.0015 Val 0.0480\n",
      "Decrease LR\n",
      "Epoch 015 Train 0.0014 Val 0.0560\n",
      "Epoch 016 Train 0.0012 Val 0.0527\n",
      "Epoch 017 Train 0.0011 Val 0.0518\n",
      "Epoch 018 Train 0.0010 Val 0.0526\n",
      "Epoch 019 Train 0.0009 Val 0.0583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 18:09:19,232] Trial 8 finished with value: 0.04668999018347644 and parameters: {'hidden_units': 14, 'input_rec': 51, 'seq_length_LSTM': 16, 'size_task_embedding': 26, 'num_units_hnet': 50, 'num_layers_hnet': 1, 'lr': 0.006587622056505793, 'huber_delta': 10, 'alpha_reg': 0.0001304253219122565, 'l1_ratio_reg': 0.7850528070730365, 'batch_size_train': 30, 'batch_size_val': 32}. Best is trial 3 with value: 0.04140114768215861.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrease LR\n",
      "Creating a simple RNN with 23260 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 513031 weights and 23260 outputs (compression ratio: 22.06).\n",
      "The network consists of 512959 unconditional weights (512959 internally maintained) and 72 conditional weights (72 internally maintained).\n",
      "Epoch 000 Train 0.8527 Val 0.6735\n",
      "Epoch 001 Train 0.4987 Val 0.4050\n",
      "Epoch 002 Train 0.2615 Val 0.2267\n",
      "Epoch 003 Train 0.1261 Val 0.1345\n",
      "Epoch 004 Train 0.0830 Val 0.1085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 18:10:53,741] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 Train 0.0680 Val 0.0970\n",
      "Creating a simple RNN with 12252 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 135292 weights and 12252 outputs (compression ratio: 11.04).\n",
      "The network consists of 135262 unconditional weights (135262 internally maintained) and 30 conditional weights (30 internally maintained).\n",
      "Epoch 000 Train 0.2510 Val 0.1163\n",
      "Epoch 001 Train 0.0687 Val 0.0759\n",
      "Epoch 002 Train 0.0437 Val 0.0527\n",
      "Epoch 003 Train 0.0304 Val 0.0460\n",
      "Epoch 004 Train 0.0252 Val 0.0428\n",
      "Epoch 005 Train 0.0224 Val 0.0390\n",
      "Epoch 006 Train 0.0210 Val 0.0390\n",
      "Epoch 007 Train 0.0186 Val 0.0384\n",
      "Epoch 008 Train 0.0169 Val 0.0373\n",
      "Epoch 009 Train 0.0144 Val 0.0387\n",
      "Epoch 010 Train 0.0124 Val 0.0370\n",
      "Epoch 011 Train 0.0107 Val 0.0373\n",
      "Epoch 012 Train 0.0096 Val 0.0399\n",
      "Epoch 013 Train 0.0084 Val 0.0391\n",
      "Epoch 014 Train 0.0077 Val 0.0397\n",
      "Decrease LR\n",
      "Epoch 015 Train 0.0068 Val 0.0377\n",
      "Epoch 016 Train 0.0064 Val 0.0406\n",
      "Epoch 017 Train 0.0060 Val 0.0393\n",
      "Epoch 018 Train 0.0057 Val 0.0405\n",
      "Epoch 019 Train 0.0055 Val 0.0401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 18:16:25,500] Trial 10 finished with value: 0.036975778469882564 and parameters: {'hidden_units': 31, 'input_rec': 32, 'seq_length_LSTM': 19, 'size_task_embedding': 15, 'num_units_hnet': 10, 'num_layers_hnet': 4, 'lr': 0.002719136472254405, 'huber_delta': 5, 'alpha_reg': 9.367466469591627e-06, 'l1_ratio_reg': 0.6380706893491844, 'batch_size_train': 46, 'batch_size_val': 50}. Best is trial 10 with value: 0.036975778469882564.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrease LR\n",
      "Creating a simple RNN with 12131 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 133949 weights and 12131 outputs (compression ratio: 11.04).\n",
      "The network consists of 133921 unconditional weights (133921 internally maintained) and 28 conditional weights (28 internally maintained).\n",
      "Epoch 000 Train 0.2910 Val 0.1099\n",
      "Epoch 001 Train 0.0769 Val 0.0841\n",
      "Epoch 002 Train 0.0444 Val 0.0513\n",
      "Epoch 003 Train 0.0278 Val 0.0428\n",
      "Epoch 004 Train 0.0242 Val 0.0436\n",
      "Epoch 005 Train 0.0214 Val 0.0427\n",
      "Epoch 006 Train 0.0188 Val 0.0390\n",
      "Epoch 007 Train 0.0162 Val 0.0394\n",
      "Epoch 008 Train 0.0150 Val 0.0383\n",
      "Epoch 009 Train 0.0129 Val 0.0381\n",
      "Epoch 010 Train 0.0111 Val 0.0367\n",
      "Epoch 011 Train 0.0098 Val 0.0385\n",
      "Epoch 012 Train 0.0083 Val 0.0383\n",
      "Epoch 013 Train 0.0076 Val 0.0408\n",
      "Epoch 014 Train 0.0069 Val 0.0396\n",
      "Decrease LR\n",
      "Epoch 015 Train 0.0063 Val 0.0405\n",
      "Epoch 016 Train 0.0054 Val 0.0408\n",
      "Epoch 017 Train 0.0052 Val 0.0405\n",
      "Epoch 018 Train 0.0048 Val 0.0414\n",
      "Epoch 019 Train 0.0047 Val 0.0426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 18:21:50,122] Trial 11 finished with value: 0.03666015840803216 and parameters: {'hidden_units': 32, 'input_rec': 30, 'seq_length_LSTM': 20, 'size_task_embedding': 14, 'num_units_hnet': 10, 'num_layers_hnet': 4, 'lr': 0.0034028840097651237, 'huber_delta': 5, 'alpha_reg': 8.38820392002778e-06, 'l1_ratio_reg': 0.6400974750754305, 'batch_size_train': 49, 'batch_size_val': 49}. Best is trial 11 with value: 0.03666015840803216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrease LR\n",
      "Creating a simple RNN with 11621 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 128217 weights and 11621 outputs (compression ratio: 11.03).\n",
      "The network consists of 128191 unconditional weights (128191 internally maintained) and 26 conditional weights (26 internally maintained).\n",
      "Epoch 000 Train 0.2038 Val 0.0693\n",
      "Epoch 001 Train 0.0365 Val 0.0485\n",
      "Epoch 002 Train 0.0252 Val 0.0399\n",
      "Epoch 003 Train 0.0198 Val 0.0380\n",
      "Epoch 004 Train 0.0173 Val 0.0372\n",
      "Epoch 005 Train 0.0145 Val 0.0381\n",
      "Epoch 006 Train 0.0120 Val 0.0412\n",
      "Epoch 007 Train 0.0096 Val 0.0381\n",
      "Epoch 008 Train 0.0077 Val 0.0391\n",
      "Epoch 009 Train 0.0063 Val 0.0386\n",
      "Epoch 010 Train 0.0054 Val 0.0378\n",
      "Epoch 011 Train 0.0047 Val 0.0423\n",
      "Epoch 012 Train 0.0041 Val 0.0389\n",
      "Epoch 013 Train 0.0035 Val 0.0367\n",
      "Epoch 014 Train 0.0031 Val 0.0398\n",
      "Epoch 015 Train 0.0028 Val 0.0411\n",
      "Epoch 016 Train 0.0023 Val 0.0410\n",
      "Epoch 017 Train 0.0020 Val 0.0400\n",
      "Decrease LR\n",
      "Epoch 018 Train 0.0018 Val 0.0412\n",
      "Epoch 019 Train 0.0016 Val 0.0415\n",
      "Epoch 020 Train 0.0015 Val 0.0420\n",
      "Epoch 021 Train 0.0014 Val 0.0419\n",
      "Epoch 022 Train 0.0013 Val 0.0422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 18:28:02,541] Trial 12 finished with value: 0.03667137352906793 and parameters: {'hidden_units': 30, 'input_rec': 31, 'seq_length_LSTM': 20, 'size_task_embedding': 13, 'num_units_hnet': 10, 'num_layers_hnet': 3, 'lr': 0.009818191294744729, 'huber_delta': 5, 'alpha_reg': 5.5237116916880655e-06, 'l1_ratio_reg': 0.6154040867846227, 'batch_size_train': 49, 'batch_size_val': 50}. Best is trial 11 with value: 0.03666015840803216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrease LR\n",
      "Creating a simple RNN with 9953 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 119880 weights and 9953 outputs (compression ratio: 12.04).\n",
      "The network consists of 119854 unconditional weights (119854 internally maintained) and 26 conditional weights (26 internally maintained).\n",
      "Epoch 000 Train 0.2767 Val 0.0789\n",
      "Epoch 001 Train 0.0357 Val 0.0450\n",
      "Epoch 002 Train 0.0232 Val 0.0367\n",
      "Epoch 003 Train 0.0190 Val 0.0390\n",
      "Epoch 004 Train 0.0155 Val 0.0371\n",
      "Epoch 005 Train 0.0120 Val 0.0341\n",
      "Epoch 006 Train 0.0091 Val 0.0367\n",
      "Epoch 007 Train 0.0073 Val 0.0388\n",
      "Epoch 008 Train 0.0061 Val 0.0363\n",
      "Epoch 009 Train 0.0053 Val 0.0399\n",
      "Epoch 010 Train 0.0045 Val 0.0376\n",
      "Epoch 011 Train 0.0038 Val 0.0361\n",
      "Epoch 012 Train 0.0034 Val 0.0375\n",
      "Epoch 013 Train 0.0030 Val 0.0372\n",
      "Epoch 014 Train 0.0026 Val 0.0374\n",
      "Decrease LR\n",
      "Epoch 015 Train 0.0024 Val 0.0391\n",
      "Epoch 016 Train 0.0021 Val 0.0379\n",
      "Epoch 017 Train 0.0019 Val 0.0393\n",
      "Epoch 018 Train 0.0018 Val 0.0397\n",
      "Epoch 019 Train 0.0018 Val 0.0420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 18:33:31,391] Trial 13 finished with value: 0.034108375129166556 and parameters: {'hidden_units': 26, 'input_rec': 30, 'seq_length_LSTM': 20, 'size_task_embedding': 13, 'num_units_hnet': 11, 'num_layers_hnet': 3, 'lr': 0.008724683475679063, 'huber_delta': 6, 'alpha_reg': 3.3324906702023635e-06, 'l1_ratio_reg': 0.2132256325697216, 'batch_size_train': 50, 'batch_size_val': 50}. Best is trial 13 with value: 0.034108375129166556.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrease LR\n",
      "Creating a simple RNN with 10859 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 196338 weights and 10859 outputs (compression ratio: 18.08).\n",
      "The network consists of 196312 unconditional weights (196312 internally maintained) and 26 conditional weights (26 internally maintained).\n",
      "Epoch 000 Train 0.2274 Val 0.0781\n",
      "Epoch 001 Train 0.0551 Val 0.0642\n",
      "Epoch 002 Train 0.0399 Val 0.0565\n",
      "Epoch 003 Train 0.0303 Val 0.0551\n",
      "Epoch 004 Train 0.0265 Val 0.0478\n",
      "Epoch 005 Train 0.0224 Val 0.0469\n",
      "Epoch 006 Train 0.0208 Val 0.0486\n",
      "Epoch 007 Train 0.0192 Val 0.0433\n",
      "Epoch 008 Train 0.0181 Val 0.0434\n",
      "Epoch 009 Train 0.0160 Val 0.0448\n",
      "Epoch 010 Train 0.0146 Val 0.0450\n",
      "Epoch 011 Train 0.0138 Val 0.0480\n",
      "Epoch 012 Train 0.0129 Val 0.0452\n",
      "Epoch 013 Train 0.0119 Val 0.0447\n",
      "Epoch 014 Train 0.0111 Val 0.0453\n",
      "Decrease LR\n",
      "Epoch 015 Train 0.0102 Val 0.0497\n",
      "Epoch 016 Train 0.0091 Val 0.0470\n",
      "Epoch 017 Train 0.0087 Val 0.0461\n",
      "Epoch 018 Train 0.0083 Val 0.0459\n",
      "Epoch 019 Train 0.0082 Val 0.0479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 18:38:59,245] Trial 14 finished with value: 0.04325725001848413 and parameters: {'hidden_units': 24, 'input_rec': 37, 'seq_length_LSTM': 18, 'size_task_embedding': 13, 'num_units_hnet': 17, 'num_layers_hnet': 3, 'lr': 0.0016853815481690482, 'huber_delta': 6, 'alpha_reg': 1.0576826856557215e-06, 'l1_ratio_reg': 0.23229372046381225, 'batch_size_train': 50, 'batch_size_val': 46}. Best is trial 13 with value: 0.034108375129166556.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrease LR\n",
      "Creating a simple RNN with 8972 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 153662 weights and 8972 outputs (compression ratio: 17.13).\n",
      "The network consists of 153628 unconditional weights (153628 internally maintained) and 34 conditional weights (34 internally maintained).\n",
      "Epoch 000 Train 0.3896 Val 0.0934\n",
      "Epoch 001 Train 0.0731 Val 0.0797\n",
      "Epoch 002 Train 0.0514 Val 0.0652\n",
      "Epoch 003 Train 0.0345 Val 0.0487\n",
      "Epoch 004 Train 0.0265 Val 0.0454\n",
      "Epoch 005 Train 0.0224 Val 0.0431\n",
      "Epoch 006 Train 0.0201 Val 0.0427\n",
      "Epoch 007 Train 0.0182 Val 0.0413\n",
      "Epoch 008 Train 0.0163 Val 0.0417\n",
      "Epoch 009 Train 0.0137 Val 0.0396\n",
      "Epoch 010 Train 0.0127 Val 0.0442\n",
      "Epoch 011 Train 0.0108 Val 0.0432\n",
      "Epoch 012 Train 0.0094 Val 0.0432\n",
      "Epoch 013 Train 0.0083 Val 0.0456\n",
      "Epoch 014 Train 0.0074 Val 0.0450\n",
      "Decrease LR\n",
      "Epoch 015 Train 0.0067 Val 0.0428\n",
      "Epoch 016 Train 0.0062 Val 0.0462\n",
      "Epoch 017 Train 0.0058 Val 0.0447\n",
      "Epoch 018 Train 0.0056 Val 0.0410\n",
      "Epoch 019 Train 0.0053 Val 0.0462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 18:45:14,501] Trial 15 finished with value: 0.039649954645896386 and parameters: {'hidden_units': 23, 'input_rec': 30, 'seq_length_LSTM': 17, 'size_task_embedding': 17, 'num_units_hnet': 16, 'num_layers_hnet': 4, 'lr': 0.004385989310185562, 'huber_delta': 6, 'alpha_reg': 3.260626923075163e-05, 'l1_ratio_reg': 0.2487731050598352, 'batch_size_train': 43, 'batch_size_val': 40}. Best is trial 13 with value: 0.034108375129166556.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrease LR\n",
      "Creating a simple RNN with 15033 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 241176 weights and 15033 outputs (compression ratio: 16.04).\n",
      "The network consists of 241158 unconditional weights (241158 internally maintained) and 18 conditional weights (18 internally maintained).\n",
      "Epoch 000 Train 0.3781 Val 0.1677\n",
      "Epoch 001 Train 0.1707 Val 0.1190\n",
      "Epoch 002 Train 0.0805 Val 0.0728\n",
      "Epoch 003 Train 0.0436 Val 0.0563\n",
      "Epoch 004 Train 0.0359 Val 0.0550\n",
      "Epoch 005 Train 0.0253 Val 0.0461\n",
      "Epoch 006 Train 0.0184 Val 0.0437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 18:47:38,162] Trial 16 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007 Train 0.0155 Val 0.0456\n",
      "Creating a simple RNN with 14727 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 355877 weights and 14727 outputs (compression ratio: 24.16).\n",
      "The network consists of 355817 unconditional weights (355817 internally maintained) and 60 conditional weights (60 internally maintained).\n",
      "Epoch 000 Train 0.2057 Val 0.1352\n",
      "Epoch 001 Train 0.0912 Val 0.0652\n",
      "Epoch 002 Train 0.0395 Val 0.0516\n",
      "Epoch 003 Train 0.0305 Val 0.0491\n",
      "Epoch 004 Train 0.0273 Val 0.0500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 18:49:29,146] Trial 17 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 Train 0.0234 Val 0.0498\n",
      "Creating a simple RNN with 6057 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 85325 weights and 6057 outputs (compression ratio: 14.09).\n",
      "The network consists of 85305 unconditional weights (85305 internally maintained) and 20 conditional weights (20 internally maintained).\n",
      "Epoch 000 Train 0.2550 Val 0.1026\n",
      "Epoch 001 Train 0.0757 Val 0.0898\n",
      "Epoch 002 Train 0.0503 Val 0.0612\n",
      "Epoch 003 Train 0.0297 Val 0.0462\n",
      "Epoch 004 Train 0.0210 Val 0.0432\n",
      "Epoch 005 Train 0.0182 Val 0.0412\n",
      "Epoch 006 Train 0.0166 Val 0.0427\n",
      "Epoch 007 Train 0.0142 Val 0.0423\n",
      "Epoch 008 Train 0.0125 Val 0.0433\n",
      "Epoch 009 Train 0.0107 Val 0.0430\n",
      "Epoch 010 Train 0.0099 Val 0.0429\n",
      "Epoch 011 Train 0.0084 Val 0.0406\n",
      "Epoch 012 Train 0.0077 Val 0.0405\n",
      "Epoch 013 Train 0.0072 Val 0.0421\n",
      "Epoch 014 Train 0.0061 Val 0.0420\n",
      "Epoch 015 Train 0.0058 Val 0.0414\n",
      "Epoch 016 Train 0.0054 Val 0.0425\n",
      "Decrease LR\n",
      "Epoch 017 Train 0.0045 Val 0.0426\n",
      "Epoch 018 Train 0.0044 Val 0.0448\n",
      "Epoch 019 Train 0.0041 Val 0.0445\n",
      "Epoch 020 Train 0.0040 Val 0.0442\n",
      "Epoch 021 Train 0.0038 Val 0.0439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 18:56:17,809] Trial 18 finished with value: 0.04049005195319517 and parameters: {'hidden_units': 10, 'input_rec': 33, 'seq_length_LSTM': 18, 'size_task_embedding': 10, 'num_units_hnet': 13, 'num_layers_hnet': 3, 'lr': 0.003883978241006223, 'huber_delta': 5, 'alpha_reg': 3.6720893304546627e-06, 'l1_ratio_reg': 0.31789612859573635, 'batch_size_train': 45, 'batch_size_val': 38}. Best is trial 13 with value: 0.034108375129166556.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrease LR\n",
      "Creating a simple RNN with 13743 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 289131 weights and 13743 outputs (compression ratio: 21.04).\n",
      "The network consists of 289123 unconditional weights (289123 internally maintained) and 8 conditional weights (8 internally maintained).\n",
      "Epoch 000 Train 0.2599 Val 0.0755\n",
      "Epoch 001 Train 0.0329 Val 0.0607\n",
      "Epoch 002 Train 0.0223 Val 0.0495\n",
      "Epoch 003 Train 0.0183 Val 0.0450\n",
      "Epoch 004 Train 0.0149 Val 0.0470\n",
      "Epoch 005 Train 0.0114 Val 0.0476\n",
      "Epoch 006 Train 0.0092 Val 0.0477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 18:58:24,241] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007 Train 0.0073 Val 0.0481\n",
      "Creating a simple RNN with 14308 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 461424 weights and 14308 outputs (compression ratio: 32.25).\n",
      "The network consists of 461390 unconditional weights (461390 internally maintained) and 34 conditional weights (34 internally maintained).\n",
      "Epoch 000 Train 0.1118 Val 0.0681\n",
      "Epoch 001 Train 0.0446 Val 0.0622\n",
      "Epoch 002 Train 0.0402 Val 0.0610\n",
      "Epoch 003 Train 0.0380 Val 0.0536\n",
      "Epoch 004 Train 0.0365 Val 0.0572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 19:00:00,124] Trial 20 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 Train 0.0371 Val 0.0557\n",
      "Creating a simple RNN with 11367 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 136835 weights and 11367 outputs (compression ratio: 12.04).\n",
      "The network consists of 136811 unconditional weights (136811 internally maintained) and 24 conditional weights (24 internally maintained).\n",
      "Epoch 000 Train 0.0735 Val 0.0716\n",
      "Epoch 001 Train 0.0245 Val 0.0425\n",
      "Epoch 002 Train 0.0150 Val 0.0440\n",
      "Epoch 003 Train 0.0100 Val 0.0454\n",
      "Epoch 004 Train 0.0074 Val 0.0459\n",
      "Epoch 005 Train 0.0053 Val 0.0470\n",
      "Epoch 006 Train 0.0038 Val 0.0466\n",
      "Epoch 007 Train 0.0029 Val 0.0459\n",
      "Epoch 008 Train 0.0022 Val 0.0468\n",
      "Epoch 009 Train 0.0018 Val 0.0469\n",
      "Epoch 010 Train 0.0014 Val 0.0469\n",
      "Epoch 011 Train 0.0012 Val 0.0465\n",
      "Epoch 012 Train 0.0010 Val 0.0462\n",
      "Epoch 013 Train 0.0008 Val 0.0467\n",
      "Epoch 014 Train 0.0007 Val 0.0458\n",
      "Decrease LR\n",
      "Epoch 015 Train 0.0006 Val 0.0477\n",
      "Epoch 016 Train 0.0005 Val 0.0459\n",
      "Epoch 017 Train 0.0005 Val 0.0463\n",
      "Epoch 018 Train 0.0005 Val 0.0475\n",
      "Epoch 019 Train 0.0004 Val 0.0476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 19:05:34,105] Trial 21 finished with value: 0.042454063622191326 and parameters: {'hidden_units': 28, 'input_rec': 33, 'seq_length_LSTM': 20, 'size_task_embedding': 12, 'num_units_hnet': 11, 'num_layers_hnet': 3, 'lr': 0.008683080337045464, 'huber_delta': 5, 'alpha_reg': 5.759214744836728e-06, 'l1_ratio_reg': 0.6110069779040423, 'batch_size_train': 48, 'batch_size_val': 50}. Best is trial 13 with value: 0.034108375129166556.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrease LR\n",
      "Creating a simple RNN with 7776 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 148826 weights and 7776 outputs (compression ratio: 19.14).\n",
      "The network consists of 148788 unconditional weights (148788 internally maintained) and 38 conditional weights (38 internally maintained).\n",
      "Epoch 000 Train 0.2247 Val 0.1512\n",
      "Epoch 001 Train 0.1075 Val 0.0955\n",
      "Epoch 002 Train 0.0657 Val 0.0810\n",
      "Epoch 003 Train 0.0458 Val 0.0587\n",
      "Epoch 004 Train 0.0282 Val 0.0447\n",
      "Epoch 005 Train 0.0224 Val 0.0435\n",
      "Epoch 006 Train 0.0192 Val 0.0424\n",
      "Epoch 007 Train 0.0173 Val 0.0428\n",
      "Epoch 008 Train 0.0150 Val 0.0448\n",
      "Epoch 009 Train 0.0127 Val 0.0429\n",
      "Epoch 010 Train 0.0110 Val 0.0442\n",
      "Epoch 011 Train 0.0098 Val 0.0446\n",
      "Epoch 012 Train 0.0085 Val 0.0423\n",
      "Epoch 013 Train 0.0080 Val 0.0444\n",
      "Epoch 014 Train 0.0074 Val 0.0444\n",
      "Epoch 015 Train 0.0064 Val 0.0432\n",
      "Epoch 016 Train 0.0059 Val 0.0442\n",
      "Decrease LR\n",
      "Epoch 017 Train 0.0055 Val 0.0439\n",
      "Epoch 018 Train 0.0052 Val 0.0439\n",
      "Epoch 019 Train 0.0048 Val 0.0437\n",
      "Epoch 020 Train 0.0046 Val 0.0442\n",
      "Epoch 021 Train 0.0043 Val 0.0450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 19:11:36,774] Trial 22 finished with value: 0.04230191191993042 and parameters: {'hidden_units': 19, 'input_rec': 30, 'seq_length_LSTM': 19, 'size_task_embedding': 19, 'num_units_hnet': 18, 'num_layers_hnet': 3, 'lr': 0.0027463124412829966, 'huber_delta': 5, 'alpha_reg': 3.2508301610897744e-06, 'l1_ratio_reg': 0.7158810459493113, 'batch_size_train': 50, 'batch_size_val': 48}. Best is trial 13 with value: 0.034108375129166556.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrease LR\n",
      "Creating a simple RNN with 14505 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 159941 weights and 14505 outputs (compression ratio: 11.03).\n",
      "The network consists of 159915 unconditional weights (159915 internally maintained) and 26 conditional weights (26 internally maintained).\n",
      "Epoch 000 Train 0.1381 Val 0.0922\n",
      "Epoch 001 Train 0.0397 Val 0.0487\n",
      "Epoch 002 Train 0.0237 Val 0.0392\n",
      "Epoch 003 Train 0.0185 Val 0.0434\n",
      "Epoch 004 Train 0.0157 Val 0.0442\n",
      "Epoch 005 Train 0.0132 Val 0.0408\n",
      "Epoch 006 Train 0.0103 Val 0.0409\n",
      "Epoch 007 Train 0.0083 Val 0.0451\n",
      "Epoch 008 Train 0.0069 Val 0.0430\n",
      "Epoch 009 Train 0.0058 Val 0.0457\n",
      "Epoch 010 Train 0.0050 Val 0.0444\n",
      "Epoch 011 Train 0.0041 Val 0.0452\n",
      "Epoch 012 Train 0.0035 Val 0.0455\n",
      "Epoch 013 Train 0.0028 Val 0.0487\n",
      "Epoch 014 Train 0.0022 Val 0.0489\n",
      "Decrease LR\n",
      "Epoch 015 Train 0.0019 Val 0.0506\n",
      "Epoch 016 Train 0.0017 Val 0.0479\n",
      "Epoch 017 Train 0.0015 Val 0.0496\n",
      "Epoch 018 Train 0.0014 Val 0.0461\n",
      "Epoch 019 Train 0.0013 Val 0.0491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 19:17:13,221] Trial 23 finished with value: 0.03915894933652075 and parameters: {'hidden_units': 34, 'input_rec': 36, 'seq_length_LSTM': 18, 'size_task_embedding': 13, 'num_units_hnet': 10, 'num_layers_hnet': 3, 'lr': 0.005367265275910398, 'huber_delta': 5, 'alpha_reg': 1.413231533156293e-05, 'l1_ratio_reg': 0.6300206446272003, 'batch_size_train': 47, 'batch_size_val': 50}. Best is trial 13 with value: 0.034108375129166556.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrease LR\n",
      "Creating a simple RNN with 19572 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 294140 weights and 19572 outputs (compression ratio: 15.03).\n",
      "The network consists of 294098 unconditional weights (294098 internally maintained) and 42 conditional weights (42 internally maintained).\n",
      "Epoch 000 Train 0.1617 Val 0.0875\n",
      "Epoch 001 Train 0.0439 Val 0.0638\n",
      "Epoch 002 Train 0.0326 Val 0.0500\n",
      "Epoch 003 Train 0.0261 Val 0.0476\n",
      "Epoch 004 Train 0.0218 Val 0.0460\n",
      "Epoch 005 Train 0.0185 Val 0.0431\n",
      "Epoch 006 Train 0.0146 Val 0.0423\n",
      "Epoch 007 Train 0.0122 Val 0.0417\n",
      "Epoch 008 Train 0.0106 Val 0.0384\n",
      "Epoch 009 Train 0.0092 Val 0.0403\n",
      "Epoch 010 Train 0.0081 Val 0.0408\n",
      "Epoch 011 Train 0.0072 Val 0.0431\n",
      "Epoch 012 Train 0.0064 Val 0.0431\n",
      "Epoch 013 Train 0.0056 Val 0.0436\n",
      "Epoch 014 Train 0.0052 Val 0.0432\n",
      "Decrease LR\n",
      "Epoch 015 Train 0.0046 Val 0.0448\n",
      "Epoch 016 Train 0.0041 Val 0.0441\n",
      "Epoch 017 Train 0.0039 Val 0.0451\n",
      "Epoch 018 Train 0.0038 Val 0.0417\n",
      "Epoch 019 Train 0.0036 Val 0.0423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 19:23:41,584] Trial 24 finished with value: 0.03840133901558546 and parameters: {'hidden_units': 43, 'input_rec': 39, 'seq_length_LSTM': 20, 'size_task_embedding': 21, 'num_units_hnet': 14, 'num_layers_hnet': 2, 'lr': 0.0020802033413381583, 'huber_delta': 6, 'alpha_reg': 6.108678213676071e-06, 'l1_ratio_reg': 0.5311994873480823, 'batch_size_train': 44, 'batch_size_val': 43}. Best is trial 13 with value: 0.034108375129166556.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrease LR\n",
      "Creating a simple RNN with 9206 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 120128 weights and 9206 outputs (compression ratio: 13.05).\n",
      "The network consists of 120110 unconditional weights (120110 internally maintained) and 18 conditional weights (18 internally maintained).\n",
      "Epoch 000 Train 0.2188 Val 0.0691\n",
      "Epoch 001 Train 0.0339 Val 0.0470\n",
      "Epoch 002 Train 0.0249 Val 0.0400\n",
      "Epoch 003 Train 0.0183 Val 0.0397\n",
      "Epoch 004 Train 0.0151 Val 0.0428\n",
      "Epoch 005 Train 0.0119 Val 0.0451\n",
      "Epoch 006 Train 0.0092 Val 0.0447\n",
      "Epoch 007 Train 0.0074 Val 0.0452\n",
      "Epoch 008 Train 0.0059 Val 0.0439\n",
      "Epoch 009 Train 0.0049 Val 0.0409\n",
      "Epoch 010 Train 0.0041 Val 0.0402\n",
      "Epoch 011 Train 0.0035 Val 0.0410\n",
      "Epoch 012 Train 0.0031 Val 0.0411\n",
      "Epoch 013 Train 0.0026 Val 0.0408\n",
      "Epoch 014 Train 0.0022 Val 0.0405\n",
      "Decrease LR\n",
      "Epoch 015 Train 0.0020 Val 0.0444\n",
      "Epoch 016 Train 0.0017 Val 0.0438\n",
      "Epoch 017 Train 0.0016 Val 0.0413\n",
      "Epoch 018 Train 0.0015 Val 0.0465\n",
      "Epoch 019 Train 0.0014 Val 0.0440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 19:30:01,486] Trial 25 finished with value: 0.039733627646335604 and parameters: {'hidden_units': 21, 'input_rec': 34, 'seq_length_LSTM': 19, 'size_task_embedding': 9, 'num_units_hnet': 12, 'num_layers_hnet': 3, 'lr': 0.009799203426985943, 'huber_delta': 5, 'alpha_reg': 5.969195846445162e-07, 'l1_ratio_reg': 0.59553062947283, 'batch_size_train': 48, 'batch_size_val': 48}. Best is trial 13 with value: 0.034108375129166556.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrease LR\n",
      "Creating a simple RNN with 10768 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 216813 weights and 10768 outputs (compression ratio: 20.13).\n",
      "The network consists of 216785 unconditional weights (216785 internally maintained) and 28 conditional weights (28 internally maintained).\n",
      "Epoch 000 Train 0.3080 Val 0.0709\n",
      "Epoch 001 Train 0.0377 Val 0.0568\n",
      "Epoch 002 Train 0.0316 Val 0.0591\n",
      "Epoch 003 Train 0.0264 Val 0.0560\n",
      "Epoch 004 Train 0.0206 Val 0.0518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 19:31:49,159] Trial 26 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 Train 0.0186 Val 0.0479\n",
      "Creating a simple RNN with 14015 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 380619 weights and 14015 outputs (compression ratio: 27.16).\n",
      "The network consists of 380563 unconditional weights (380563 internally maintained) and 56 conditional weights (56 internally maintained).\n",
      "Epoch 000 Train 0.1711 Val 0.1044\n",
      "Epoch 001 Train 0.0658 Val 0.0742\n",
      "Epoch 002 Train 0.0361 Val 0.0484\n",
      "Epoch 003 Train 0.0245 Val 0.0405\n",
      "Epoch 004 Train 0.0216 Val 0.0390\n",
      "Epoch 005 Train 0.0190 Val 0.0399\n",
      "Epoch 006 Train 0.0167 Val 0.0398\n",
      "Epoch 007 Train 0.0148 Val 0.0397\n",
      "Epoch 008 Train 0.0135 Val 0.0398\n",
      "Epoch 009 Train 0.0119 Val 0.0373\n",
      "Epoch 010 Train 0.0108 Val 0.0391\n",
      "Epoch 011 Train 0.0100 Val 0.0379\n",
      "Epoch 012 Train 0.0088 Val 0.0382\n",
      "Epoch 013 Train 0.0079 Val 0.0378\n",
      "Epoch 014 Train 0.0071 Val 0.0387\n",
      "Decrease LR\n",
      "Epoch 015 Train 0.0064 Val 0.0379\n",
      "Epoch 016 Train 0.0059 Val 0.0396\n",
      "Epoch 017 Train 0.0054 Val 0.0385\n",
      "Epoch 018 Train 0.0051 Val 0.0389\n",
      "Epoch 019 Train 0.0048 Val 0.0394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 19:38:06,683] Trial 27 finished with value: 0.037298525279522356 and parameters: {'hidden_units': 36, 'input_rec': 31, 'seq_length_LSTM': 20, 'size_task_embedding': 28, 'num_units_hnet': 26, 'num_layers_hnet': 3, 'lr': 0.0038407773987667528, 'huber_delta': 7, 'alpha_reg': 2.562054749667704e-06, 'l1_ratio_reg': 0.7293587415623108, 'batch_size_train': 50, 'batch_size_val': 50}. Best is trial 13 with value: 0.034108375129166556.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrease LR\n",
      "Creating a simple RNN with 13260 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 212704 weights and 13260 outputs (compression ratio: 16.04).\n",
      "The network consists of 212670 unconditional weights (212670 internally maintained) and 34 conditional weights (34 internally maintained).\n",
      "Epoch 000 Train 0.1582 Val 0.0839\n",
      "Epoch 001 Train 0.0383 Val 0.0497\n",
      "Epoch 002 Train 0.0240 Val 0.0449\n",
      "Epoch 003 Train 0.0179 Val 0.0445\n",
      "Epoch 004 Train 0.0143 Val 0.0428\n",
      "Epoch 005 Train 0.0108 Val 0.0476\n",
      "Epoch 006 Train 0.0083 Val 0.0441\n",
      "Epoch 007 Train 0.0063 Val 0.0429\n",
      "Epoch 008 Train 0.0051 Val 0.0436\n",
      "Epoch 009 Train 0.0042 Val 0.0404\n",
      "Epoch 010 Train 0.0035 Val 0.0421\n",
      "Epoch 011 Train 0.0030 Val 0.0431\n",
      "Epoch 012 Train 0.0026 Val 0.0387\n",
      "Epoch 013 Train 0.0022 Val 0.0416\n",
      "Epoch 014 Train 0.0019 Val 0.0412\n",
      "Epoch 015 Train 0.0016 Val 0.0425\n",
      "Epoch 016 Train 0.0015 Val 0.0420\n",
      "Decrease LR\n",
      "Epoch 017 Train 0.0013 Val 0.0430\n",
      "Epoch 018 Train 0.0012 Val 0.0388\n",
      "Epoch 019 Train 0.0011 Val 0.0415\n",
      "Epoch 020 Train 0.0010 Val 0.0415\n",
      "Epoch 021 Train 0.0010 Val 0.0417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 19:45:03,282] Trial 28 finished with value: 0.03867565437342744 and parameters: {'hidden_units': 31, 'input_rec': 36, 'seq_length_LSTM': 18, 'size_task_embedding': 17, 'num_units_hnet': 15, 'num_layers_hnet': 2, 'lr': 0.005297718408174701, 'huber_delta': 5, 'alpha_reg': 2.176486136462753e-05, 'l1_ratio_reg': 0.20571143159122676, 'batch_size_train': 41, 'batch_size_val': 43}. Best is trial 13 with value: 0.034108375129166556.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decrease LR\n",
      "Creating a simple RNN with 9578 weights.\n",
      "Created MLP Hypernet.\n",
      "Hypernetwork with 221808 weights and 9578 outputs (compression ratio: 23.16).\n",
      "The network consists of 221768 unconditional weights (221768 internally maintained) and 40 conditional weights (40 internally maintained).\n",
      "Epoch 000 Train 0.0903 Val 0.0756\n",
      "Epoch 001 Train 0.0337 Val 0.0550\n",
      "Epoch 002 Train 0.0216 Val 0.0481\n",
      "Epoch 003 Train 0.0179 Val 0.0454\n",
      "Epoch 004 Train 0.0157 Val 0.0464\n",
      "Epoch 005 Train 0.0140 Val 0.0448\n",
      "Epoch 006 Train 0.0131 Val 0.0436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-22 19:47:31,841] Trial 29 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007 Train 0.0121 Val 0.0439\n",
      "lr: 0.21014374889128268\n",
      "huber_delta: 0.13013276935793192\n",
      "batch_size_val: 0.12130770741551816\n",
      "num_units_hnet: 0.10569991913357972\n",
      "size_task_embedding: 0.10488262950363018\n",
      "batch_size_train: 0.07627180230148475\n",
      "seq_length_LSTM: 0.0706055119627895\n",
      "input_rec: 0.043272813606277515\n",
      "l1_ratio_reg: 0.042129665231524185\n",
      "alpha_reg: 0.03388434194171077\n",
      "num_layers_hnet: 0.031113185708871262\n",
      "hidden_units: 0.03055590494539925\n",
      "Study statistics: \n",
      "Number of finished trials:  30\n",
      "Number of pruned trials:  9\n",
      "Number of complete trials:  21\n",
      "Best trial: \n",
      "Loss: 0.034108375129166556\n",
      "Best hyperparameters: {'hidden_units': 26, 'input_rec': 30, 'seq_length_LSTM': 20, 'size_task_embedding': 13, 'num_units_hnet': 11, 'num_layers_hnet': 3, 'lr': 0.008724683475679063, 'huber_delta': 6, 'alpha_reg': 3.3324906702023635e-06, 'l1_ratio_reg': 0.2132256325697216, 'batch_size_train': 50, 'batch_size_val': 50}\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "# Set pruning options\n",
    "study.pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=5)\n",
    "\n",
    "study.optimize(train_model_optuna, n_trials=30)\n",
    "\n",
    "importance_scores = optuna.importance.get_param_importances(study)\n",
    "\n",
    "# Print importance scores\n",
    "for param, score in importance_scores.items():\n",
    "    print(f\"{param}: {score}\")\n",
    "\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print('Study statistics: ')\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print('Best trial: ')\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"Loss: {}\".format(trial.value))\n",
    "print(\"Best hyperparameters: {}\".format(trial.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "# Retrieve the best trial\n",
    "best_trial = study.best_trial\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = best_trial.params\n",
    "\n",
    "# Save the best parameters\n",
    "with open('stim_0411_best_params.json', 'w') as f:\n",
    "    json.dump(best_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/sinthlab/lib/python3.8/site-packages/optuna/visualization/_plotly_imports.py:7\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m try_import() \u001b[38;5;28;01mas\u001b[39;00m _imports:\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m plotly_version\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moptuna\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_optimization_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/sinthlab/lib/python3.8/site-packages/optuna/visualization/_optimization_history.py:222\u001b[0m, in \u001b[0;36mplot_optimization_history\u001b[0;34m(study, target, target_name, error_bar)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_optimization_history\u001b[39m(\n\u001b[1;32m    173\u001b[0m     study: Study \u001b[38;5;241m|\u001b[39m Sequence[Study],\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     error_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    178\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgo.Figure\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Plot optimization history of all trials in a study.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m        A :class:`plotly.graph_objects.Figure` object.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     \u001b[43m_imports\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m     info_list \u001b[38;5;241m=\u001b[39m _get_optimization_history_info_list(study, target, target_name, error_bar)\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_optimization_history_plot(info_list, target_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/sinthlab/lib/python3.8/site-packages/optuna/_imports.py:89\u001b[0m, in \u001b[0;36m_DeferredImportExceptionContextManager.check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deferred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     exc_value, message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deferred\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc_value\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'."
     ]
    }
   ],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/sinthlab/lib/python3.8/site-packages/optuna/visualization/_plotly_imports.py:7\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m try_import() \u001b[38;5;28;01mas\u001b[39;00m _imports:\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m plotly_version\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Plotting the accuracies for each hyperparameter for each trial.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43moptuna\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/sinthlab/lib/python3.8/site-packages/optuna/visualization/_slice.py:194\u001b[0m, in \u001b[0;36mplot_slice\u001b[0;34m(study, params, target, target_name)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_slice\u001b[39m(\n\u001b[1;32m    144\u001b[0m     study: Study,\n\u001b[1;32m    145\u001b[0m     params: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m     target_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObjective Value\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    149\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgo.Figure\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    150\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Plot the parameter relationship as slice plot in a study.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m    Note that, if a parameter contains missing values, a trial with missing values is not plotted.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03m        A :class:`plotly.graph_objects.Figure` object.\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m     \u001b[43m_imports\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_slice_plot(_get_slice_plot_info(study, params, target, target_name))\n",
      "File \u001b[0;32m~/anaconda3/envs/sinthlab/lib/python3.8/site-packages/optuna/_imports.py:89\u001b[0m, in \u001b[0;36m_DeferredImportExceptionContextManager.check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deferred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     exc_value, message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deferred\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc_value\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'."
     ]
    }
   ],
   "source": [
    "#Plotting the accuracies for each hyperparameter for each trial.\n",
    "    \n",
    "optuna.visualization.plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/sinthlab/lib/python3.8/site-packages/optuna/visualization/_plotly_imports.py:7\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m try_import() \u001b[38;5;28;01mas\u001b[39;00m _imports:\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m plotly_version\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plotting the optimization history of the study.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43moptuna\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_optimization_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#Plotting the accuracies for each hyperparameter for each trial.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m optuna\u001b[38;5;241m.\u001b[39mvisualization\u001b[38;5;241m.\u001b[39mplot_slice(study)\n",
      "File \u001b[0;32m~/anaconda3/envs/sinthlab/lib/python3.8/site-packages/optuna/visualization/_optimization_history.py:222\u001b[0m, in \u001b[0;36mplot_optimization_history\u001b[0;34m(study, target, target_name, error_bar)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_optimization_history\u001b[39m(\n\u001b[1;32m    173\u001b[0m     study: Study \u001b[38;5;241m|\u001b[39m Sequence[Study],\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     error_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    178\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgo.Figure\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Plot optimization history of all trials in a study.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m        A :class:`plotly.graph_objects.Figure` object.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     \u001b[43m_imports\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m     info_list \u001b[38;5;241m=\u001b[39m _get_optimization_history_info_list(study, target, target_name, error_bar)\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_optimization_history_plot(info_list, target_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/sinthlab/lib/python3.8/site-packages/optuna/_imports.py:89\u001b[0m, in \u001b[0;36m_DeferredImportExceptionContextManager.check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deferred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     exc_value, message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deferred\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc_value\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'."
     ]
    }
   ],
   "source": [
    "# Plotting the optimization history of the study.\n",
    "\n",
    "optuna.visualization.plot_optimization_history(study)\n",
    "\n",
    "#Plotting the accuracies for each hyperparameter for each trial.\n",
    "    \n",
    "optuna.visualization.plot_slice(study)\n",
    "\n",
    "# Plotting the accuracy surface for the hyperparameters involved in the random forest model.\n",
    "\n",
    "optuna.visualization.plot_contour(study, params=[\"seq_length_LSTM\", \"n_hidden_units\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sinthlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
